{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84aa7305-b32c-4ac2-9c58-96851c6f8ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "from transformers import PLBartTokenizer \n",
    "\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-multi_task-python\", language_codes=\"multi\", src_lang=\"python\", tgt_lang =\"python\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e61519-8eed-4f63-ae6c-f5ce5cc648c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "from transformers import PLBartForConditionalGeneration\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    Seq2SeqSequenceClassifierOutput,\n",
    ")\n",
    "\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right, and wrap the last non pad token (the <LID> token) Note that MBart does not\n",
    "    have a single `decoder_start_token_id` in contrast to other Bart-like models.\n",
    "    \"\"\"\n",
    "    prev_output_tokens = input_ids.clone()\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n",
    "\n",
    "    index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "    decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n",
    "    prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n",
    "    prev_output_tokens[:, 0] = decoder_start_tokens\n",
    "\n",
    "    return prev_output_tokens\n",
    "\n",
    "class InRepPlusGAN(torch.nn.Module):\n",
    "    def __init__(self, style_dim):\n",
    "        super(InRepPlusGAN, self).__init__()\n",
    "        self.model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-multi_task-python\", )\n",
    "        self.encoder = self.model.get_encoder()\n",
    "        self.decoder = self.model.get_decoder()\n",
    "        self.config = self.model.config\n",
    "        self.modifier = torch.nn.Linear(self.config.d_model + style_dim, self.config.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        style_encoding: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.LongTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id)\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # different to other models, PLBart automatically creates decoder_input_ids from\n",
    "        # input_ids if no decoder_input_ids are provided\n",
    "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            decoder_input_ids = shift_tokens_right(input_ids, self.config.pad_token_id)\n",
    "        \n",
    "        # encoder E, with no grad\n",
    "        if encoder_outputs is None:\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = self.encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    head_mask=head_mask,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "        \n",
    "        # need an additional tunable encoder M\n",
    "        \n",
    "        batch_size = encoder_outputs[0].shape[0]\n",
    "        seq_len = encoder_outputs[0].shape[1]\n",
    "        \n",
    "        style_encoding = style_encoding.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "#         for _ in range(1, seq_len):\n",
    "#             style_encoding = torch.cat((style_encoding, style_encoding.unsqueeze(1)), dim=1)\n",
    "            \n",
    "#         print(encoder_outputs[0].shape, style_encoding.shape)\n",
    "        combined_encoding = torch.cat((encoder_outputs[0], style_encoding), dim=-1)\n",
    "        modifier_outputs = []\n",
    "        for i in range(seq_len):\n",
    "            modifier_output = self.modifier(combined_encoding[:, i, :])\n",
    "            modifier_outputs += [modifier_output.unsqueeze(1)]\n",
    "        modifier_outputs = torch.cat(modifier_outputs, dim=1)\n",
    "        \n",
    "        # decoder G, with no grad\n",
    "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
    "        with torch.no_grad():\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                attention_mask=decoder_attention_mask,\n",
    "                encoder_hidden_states=modifier_outputs,\n",
    "                encoder_attention_mask=attention_mask,\n",
    "                head_mask=decoder_head_mask,\n",
    "                cross_attn_head_mask=cross_attn_head_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=decoder_inputs_embeds,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "        outputs = None\n",
    "        if not return_dict:\n",
    "            outputs = decoder_outputs + encoder_outputs\n",
    "        else:\n",
    "            outputs = Seq2SeqModelOutput(\n",
    "                last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "                past_key_values=decoder_outputs.past_key_values,\n",
    "                decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "                decoder_attentions=decoder_outputs.attentions,\n",
    "                cross_attentions=decoder_outputs.cross_attentions,\n",
    "                encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "                encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "                encoder_attentions=encoder_outputs.attentions,\n",
    "            )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            lm_logits = self.model.lm_head(outputs[0]) + self.model.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "        \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        ), modifier_outputs\n",
    "    def get_encoding(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        style_encoding: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.LongTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        # encoder E, with no grad\n",
    "        if encoder_outputs is None:\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = self.encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    head_mask=head_mask,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "        return encoder_outputs\n",
    "    # def forward(self, **inputs):\n",
    "    #     outputs = self.model(**inputs)\n",
    "    #     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8951dfce-1ebd-4ec0-95b1-3b7f2579128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can start with 1 layer\n",
    "# use embedding layers\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, style_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.style_dim = style_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, output_size, 1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(output_size, style_dim)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        # self.l2 = torch.nn.Linear(self.config.d_model + style_dim, self.config.d_model)\n",
    "        # self.l3 = torch.nn.Linear(self.config.d_model + style_dim, self.config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        # RNN Layer\n",
    "        init_hidden = torch.zeros(1, batch_size, self.output_size)\n",
    "        output, hidden = self.rnn(embedded_x, init_hidden)\n",
    "\n",
    "        # Linear Layer\n",
    "        hidden = hidden.squeeze(0)\n",
    "        output = self.linear(hidden)\n",
    "        logits = self.softmax(output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7156958a-636d-4a9c-8d12-d27ab8fa9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk('datasets/plbart_train.hf')\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print ((train_dataset[0][\"input_ids\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc6e25c-bf45-4345-972e-c8311ad454f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c20b7c-3703-474f-8362-31dd394496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 2\n",
    "def get_data_loader(split=\"train\"):\n",
    "    # dataset_map = {\n",
    "    #     \"train\": train_set,\n",
    "    #     \"dev\": dev_set,\n",
    "    #     \"test\": test_set,\n",
    "    # }\n",
    "\n",
    "    # tokenized_set = prepare_slot_dataset(\n",
    "    #     dataset_map[split], pretrained_tokenizer, split\n",
    "    # )\n",
    "    data_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, collate_fn=default_data_collator\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b36a4f1-aa94-42be-9652-e06a6f39249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a3beb9-f476-437e-9ed2-35fe297c3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Vocabulary Class\n",
    "    Store the index mapping for the tokens and recognize the unknown token and then return it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens, base_map={}, max_size=None, least_freq=0):\n",
    "        self.token2idx = base_map\n",
    "        # count the word/token/tags frequency\n",
    "        self.freq = Counter(\n",
    "            [token for sequence in tokens for token in sequence]\n",
    "        )\n",
    "\n",
    "        vocab_size = 0\n",
    "        # store the token start from higher frequency\n",
    "        for word, count in sorted(\n",
    "            self.freq.items(), key=lambda item: item[1], reverse=True\n",
    "        ):\n",
    "            if count < least_freq:\n",
    "                break\n",
    "            # if vocab size is larger than max size, stop inserting words into vocab\n",
    "            if max_size is not None and vocab_size > max_size:\n",
    "                break\n",
    "            self.insert(word)\n",
    "            vocab_size += 1\n",
    "\n",
    "        self.idx2token = reverse_map(self.token2idx)\n",
    "\n",
    "    def insert(self, token):\n",
    "        if token in self.token2idx.keys():\n",
    "            return\n",
    "        self.token2idx[token] = len(self.token2idx)\n",
    "\n",
    "    def lookup_index(self, word):\n",
    "        if word not in self.token2idx.keys():\n",
    "            word = UNK_TOKEN\n",
    "        return self.token2idx[word]\n",
    "\n",
    "    def lookup_token(self, idx):\n",
    "        return self.idx2token[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token2idx)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.token2idx)\n",
    "\n",
    "\n",
    "def reverse_map(_map):\n",
    "    reversed_map = {}\n",
    "    for key, val in _map.items():\n",
    "        reversed_map[val] = key\n",
    "    return reversed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc96803f-3883-4163-8f87-62e1f585fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = train_dataset[\"labels\"].detach().numpy()\n",
    "cluster_labels_no_outliers = cluster_labels[cluster_labels != -1]\n",
    "cluster_vocab = Vocab([cluster_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f391110-1abb-40f6-81a3-612a9452b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_DIM = len(cluster_vocab) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b6a359c-2efd-4f51-a6b8-74d8436581f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = InRepPlusGAN(style_dim=STYLE_DIM)\n",
    "discriminator = Discriminator(vocab_size=generator.config.vocab_size, embedding_dim=512, output_size=128, style_dim=STYLE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73fcb976-f6eb-4350-bfa7-e0fe462fb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "# Setup Adam optimizers for both G and D\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters())\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057f37d4-631d-458a-8384-41a99b2c2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tensor_to_one_hot(label_tensor):\n",
    "    one_hot_tensor_list = []\n",
    "    for idx in range(label_tensor.shape[0]):\n",
    "        cluster_idx = label_tensor[idx].item()\n",
    "        one_hot_tensor = cluster_to_one_hot_tensor(cluster_idx, STYLE_DIM)\n",
    "        one_hot_tensor_list.append(one_hot_tensor.unsqueeze(0))\n",
    "        \n",
    "    return torch.cat(one_hot_tensor_list, dim=0)\n",
    "\n",
    "def cluster_to_one_hot_tensor(cluster_idx, style_dim):\n",
    "    style_tensor = torch.zeros(style_dim)\n",
    "    if cluster_idx < 0:\n",
    "        return style_tensor\n",
    "    style_tensor[cluster_idx] = 1\n",
    "    return style_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f30001-0f70-4dcc-8e5c-59217ea8debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(input_batch, discriminator, generator, criterion, discriminator_optimizer, generator_optimizer):\n",
    "    # update D network\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    # All-real training\n",
    "    # Format real batch\n",
    "    real_data = input_batch[\"input_ids\"]\n",
    "    real_style = label_tensor_to_one_hot(input_batch[\"labels\"])\n",
    "    \n",
    "    # Forward pass real batch through D\n",
    "    output = discriminator(real_data)\n",
    "\n",
    "    # Calculate loss on all-real batch\n",
    "    discriminator_real_loss = criterion(output, real_style)\n",
    "    \n",
    "    # Calculate gradients for D in backward pass\n",
    "    discriminator_real_loss.backward()\n",
    "\n",
    "    # All-fake training\n",
    "    \n",
    "    # sampling the target styles for a whole patch\n",
    "    sampled_style_indexes = random.sample(list(cluster_labels_no_outliers), BATCH_SIZE)\n",
    "    style_encoding = label_tensor_to_one_hot(torch.Tensor(sampled_style_indexes).long())\n",
    "    \n",
    "    # Forward pass to generate the styled output\n",
    "    generator_output, modifier_output = generator(\n",
    "        input_ids=input_batch[\"input_ids\"], \n",
    "        attention_mask=input_batch[\"attention_mask\"], \n",
    "        style_encoding=style_encoding\n",
    "    )\n",
    "    generated_logits = generator_output.logits\n",
    "    \n",
    "    # use Gumbel Softmax to decode the output\n",
    "    generated_tokens = torch.nn.functional.gumbel_softmax(generated_logits, hard=True, dim=-1)\n",
    "    \n",
    "    # produce the fake data\n",
    "    fake_data = generated_tokens.argmax(-1)\n",
    "    # print(tokenizer.batch_decode(fake_data))\n",
    "\n",
    "    # Classify all fake batch with D\n",
    "    output = discriminator(fake_data)\n",
    "\n",
    "    # Calculate D's loss on the all-fake batch\n",
    "    discriminator_fake_loss = criterion(output, style_encoding)\n",
    "\n",
    "    # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "    discriminator_fake_loss.backward()\n",
    "\n",
    "    # Compute error of D as sum over the fake and the real batches\n",
    "    discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n",
    "\n",
    "    # Update D\n",
    "    discriminator_optimizer.step()\n",
    "    \n",
    "    # update M network\n",
    "\n",
    "    generator.zero_grad()\n",
    "\n",
    "    # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "    output = discriminator(fake_data)\n",
    "    \n",
    "    # Calculate G's loss based on this output\n",
    "    generator_class_loss = criterion(output, style_encoding)\n",
    "    \n",
    "    # Calculate gradients for G\n",
    "    generator_class_loss.backward()\n",
    "    \n",
    "    # TODO: add the modifier loss\n",
    "    generator_loss = generator_class_loss\n",
    "    \n",
    "    # Update G\n",
    "    generator_optimizer.step()\n",
    "    \n",
    "    return generator_loss, discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1416cc-bf7a-4f6a-9873-30ddf321e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b77632f85d045a5997f23c0272b5620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2651, grad_fn=<AddBackward0>)\n",
      "tensor(0.2059, grad_fn=<AddBackward0>)\n",
      "tensor(0.2007, grad_fn=<AddBackward0>)\n",
      "tensor(0.2644, grad_fn=<AddBackward0>)\n",
      "tensor(0.1914, grad_fn=<AddBackward0>)\n",
      "tensor(0.2068, grad_fn=<AddBackward0>)\n",
      "tensor(0.3454, grad_fn=<AddBackward0>)\n",
      "tensor(0.2614, grad_fn=<AddBackward0>)\n",
      "tensor(0.2736, grad_fn=<AddBackward0>)\n",
      "tensor(0.1976, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f494a8f874fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msampled_style_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels_no_outliers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mstyle_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_tensor_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_style_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         generator_output, modifier_output = generator(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9f7e9002a13a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, style_encoding, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/transformers/models/plbart/modeling_plbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     )\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                     layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    806\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/transformers/models/plbart/modeling_plbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[1;32m    315\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         hidden_states, attn_weights, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py_3_8/lib/python3.8/site-packages/transformers/models/plbart/modeling_plbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_g = 0\n",
    "    epoch_loss_d = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        loss_g, loss_d = training_step(batch, discriminator, generator, criterion, discriminator_optimizer, generator_optimizer)\n",
    "        epoch_loss_g += loss_g\n",
    "        epoch_loss_d += loss_d\n",
    "    print(epoch_loss_g, epoch_loss_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "703538f6-d990-436e-beb7-637719ffd4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1185, -0.1352,  0.2604,  ..., -0.0646,  0.0714,  0.2654],\n",
       "         [-0.2654, -0.1587, -0.0490,  ..., -0.2254, -0.0637,  0.2084],\n",
       "         [-0.3837, -0.0131, -0.3011,  ..., -1.5033,  0.2889,  0.5032],\n",
       "         ...,\n",
       "         [-1.4976,  0.1250,  0.2543,  ..., -0.4279,  1.3188,  0.1330],\n",
       "         [-0.9512,  0.1536, -0.5133,  ...,  0.3540,  1.4173,  0.7646],\n",
       "         [-0.2426,  0.1208,  0.1431,  ...,  0.4847,  1.1334,  0.8726]],\n",
       "\n",
       "        [[ 0.6149, -0.4667,  0.7789,  ...,  0.1539,  0.7153,  1.2592],\n",
       "         [-0.6662, -0.4611,  0.2164,  ..., -0.1547,  0.0227,  0.2276],\n",
       "         [-1.0176, -1.1486, -0.1792,  ...,  0.7986,  0.5448,  0.8010],\n",
       "         ...,\n",
       "         [-1.0494, -0.4101,  0.7645,  ...,  0.2734, -0.1472, -0.2740],\n",
       "         [ 0.5117,  0.8126,  0.2128,  ...,  0.5699,  0.3619, -0.1144],\n",
       "         [ 0.0317, -0.0154, -0.0085,  ..., -0.0026, -0.0344, -0.0209]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.get_encoding(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c89c9f-903d-48bd-9921-47256e8a9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fd7a4bc-1463-42b7-853c-7db5c18e793a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656725576"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56100347-c023-48f7-83aa-4e5edf3591ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656725553.8966103"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c9d88-9e76-4216-8665-0debb62d56b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
