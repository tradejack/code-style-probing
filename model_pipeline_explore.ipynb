{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84aa7305-b32c-4ac2-9c58-96851c6f8ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec90874e2874088acd21cbadef386b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/963k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdc13b7aa3f4f248fa51bf2632b479c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "from transformers import PLBartTokenizer \n",
    "\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-multi_task-python\", src_lang=\"python\", tgt_lang=\"python\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b6e61519-8eed-4f63-ae6c-f5ce5cc648c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "from transformers import PLBartForConditionalGeneration\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    Seq2SeqSequenceClassifierOutput,\n",
    ")\n",
    "\n",
    "style_dim = 10\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right, and wrap the last non pad token (the <LID> token) Note that MBart does not\n",
    "    have a single `decoder_start_token_id` in contrast to other Bart-like models.\n",
    "    \"\"\"\n",
    "    prev_output_tokens = input_ids.clone()\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n",
    "\n",
    "    index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "    decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n",
    "    prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n",
    "    prev_output_tokens[:, 0] = decoder_start_tokens\n",
    "\n",
    "    return prev_output_tokens\n",
    "\n",
    "class InRepPlusGAN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InRepPlusGAN, self).__init__()\n",
    "        self.model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-multi_task-python\")\n",
    "        self.encoder = self.model.get_encoder()\n",
    "        self.decoder = self.model.get_decoder()\n",
    "        self.config = self.model.config\n",
    "        self.modifier = torch.nn.Linear(self.config.d_model + style_dim, self.config.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        style_encoding: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.LongTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id)\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # different to other models, PLBart automatically creates decoder_input_ids from\n",
    "        # input_ids if no decoder_input_ids are provided\n",
    "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            decoder_input_ids = shift_tokens_right(input_ids, self.config.pad_token_id)\n",
    "        \n",
    "        # encoder E, with no grad\n",
    "        if encoder_outputs is None:\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = self.encoder(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    head_mask=head_mask,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "        \n",
    "        # need an additional tunable encoder M => \n",
    "        combined_encoding = torch.cat((encoder_outputs[0], style_encoding), dim=2)\n",
    "        seq_len = combined_encoding.shape[1]\n",
    "        modifier_outputs = []\n",
    "        for i in range(seq_len):\n",
    "            modifier_output = self.modifier(combined_encoding[:, i, :])\n",
    "            modifier_outputs += [modifier_output.unsqueeze(1)]\n",
    "        modifier_outputs = torch.cat(modifier_outputs, dim=1)\n",
    "        \n",
    "        # decoder G, with no grad\n",
    "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
    "        with torch.no_grad():\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                attention_mask=decoder_attention_mask,\n",
    "                encoder_hidden_states=modifier_outputs,\n",
    "                encoder_attention_mask=attention_mask,\n",
    "                head_mask=decoder_head_mask,\n",
    "                cross_attn_head_mask=cross_attn_head_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=decoder_inputs_embeds,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "        outputs = None\n",
    "        if not return_dict:\n",
    "            outputs = decoder_outputs + encoder_outputs\n",
    "        else:\n",
    "            outputs = Seq2SeqModelOutput(\n",
    "                last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "                past_key_values=decoder_outputs.past_key_values,\n",
    "                decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "                decoder_attentions=decoder_outputs.attentions,\n",
    "                cross_attentions=decoder_outputs.cross_attentions,\n",
    "                encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "                encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "                encoder_attentions=encoder_outputs.attentions,\n",
    "            )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            lm_logits = self.model.lm_head(outputs[0]) + self.model.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "        \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )\n",
    "    # def forward(self, **inputs):\n",
    "    #     outputs = self.model(**inputs)\n",
    "    #     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b8419a8e-2f7d-4ab9-a2ab-81704abca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"def hello_world():\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c52a5129-fe14-473a-aa44-641a2950428d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b50097bb-c466-4080-9449-eeb00de952ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0.]]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_dim = 2\n",
    "style_tensor = torch.zeros(style_dim)\n",
    "style_tensor[0] = 1\n",
    "style_tensor = style_tensor.unsqueeze(0)\n",
    "style_tensor, style_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b90142eb-447f-4419-b7a7-b1e3b954bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_ids.shape[0]\n",
    "seq_len = inputs.input_ids.shape[1]\n",
    "\n",
    "style_encoding = style_tensor.unsqueeze(1)\n",
    "for _ in range(1, seq_len):\n",
    "    style_encoding = torch.cat((style_encoding, style_tensor.unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "eb39891e-f658-4efd-a727-ca6ac2685c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 2])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8f28610a-35a3-422f-a07b-01f261920e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "model = InRepPlusGAN()\n",
    "output = model(**inputs, style_encoding=style_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ad8b34f2-b718-429d-998f-a831023fd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_logits = model.model.lm_head(outputs[0]) + model.model.final_logits_bias\n",
    "logits = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "36cdef6b-d80f-4e33-aca2-48e6c6519502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def get (world ( #en_XX']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(logits.argmax(-1))\n",
    "# ['def world_world ( #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8951dfce-1ebd-4ec0-95b1-3b7f2579128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can start with 1 layer\n",
    "# use embedding layers\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, style_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.style_dim = style_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, output_size, 1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(output_size, style_dim)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        # self.l2 = torch.nn.Linear(self.config.d_model + style_dim, self.config.d_model)\n",
    "        # self.l3 = torch.nn.Linear(self.config.d_model + style_dim, self.config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        # RNN Layer\n",
    "        init_hidden = torch.zeros(1, batch_size, self.output_size)\n",
    "        output, hidden = self.rnn(embedded_x, init_hidden)\n",
    "\n",
    "        # Linear Layer\n",
    "        hidden = hidden.squeeze(1)\n",
    "        output = self.linear(hidden)\n",
    "        logits = self.softmax(output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c9bdfd1f-e769-424c-8591-ccec7c254ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(vocab_size=model.config.vocab_size, embedding_dim=512, output_size=128, style_dim=style_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3d62af82-6705-424e-83e1-fc58e1e02240",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = discriminator(inputs.input_ids).argmax(-1)\n",
    "gold = style_tensor.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f065c1dd-e961-4e58-8fe6-def238bb090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([0]))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b1c33309-1d04-4620-a6e8-072c37f09c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (embedding): Embedding(50008, 512)\n",
       "  (rnn): RNN(512, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "80dd7a10-9710-4e40-b706-9994b88243fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  134,  4498, 33456, 11393,  4071,     2, 50002]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "dd088b4f-e363-4169-8b01-5dd642c76fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_style_tensor = torch.zeros(style_dim)\n",
    "real_style_tensor[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2e9d4bb0-517d-4da3-9adb-6eab6f2ac55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_label_tensor = torch.zeros(style_dim).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e4f013fa-1c40-4be6-a3b9-1cbefc37f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "# Setup Adam optimizers for both G and D\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters())\n",
    "generator_optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "d9642256-1696-4867-ad35-ee78f1255175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc974460a34a4078a37de454400af4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def export (tryѧ #!!!']\n",
      "['def matmul_ Depth ( #en_XX']\n",
      "['絀 parse (full_ #PAY']\n",
      "['def __sp (ᾶ systemId']\n",
      "['defἏ_format ( fallen_XX']\n",
      "['EncoderCycle_c_ꘫen_XX']\n",
      "['defSH (spec discover #en_XX']\n",
      "['def threw (resource_ Functionpython']\n",
      "['def s (fileecessary printRaster']\n",
      "['department on (context ( #en_XX']\n",
      "['def get ( virt ( storepython']\n",
      "['def do (spec꧴Convert\\u0cd0']\n",
      "['def getFont.</file ( aryen_XX']\n",
      "['def refresh ( iterating ( ifen_XX']\n",
      "['def _ޯ si (嬾S']\n",
      "['defẖ (tr (HttpResponse Page']\n",
      "['def init_iterable ( #⡣']\n",
      "['defAllQuery ( PAGE ( #℞']\n",
      "['berry do_state ( superlocals']\n",
      "['def get_params ( fpython']\n",
      "['def el (稱 ( )en_XX']\n",
      "['declar convert_bin_ ifen_XX']\n",
      "['the export_class (stateen_XX']\n",
      "['GERPrincipal ( msbuild ( wsdlpython']\n",
      "['護 _ (କ (entry霍']\n",
      "['def make (table_AND peek']\n",
      "['defჷ ( parentNode ( catalog.']\n",
      "['def _ݧall_ (en_XX']\n",
      "['def render_stream ( #en_XX']\n",
      "[\"def'?吪array ( Listpython\"]\n",
      "['def.1 (tor2 assertୂ']\n",
      "['def cs studentbus ( #method']\n",
      "['def method_cart ( ifen_XX']\n",
      "['def make (end (Dstpython']\n",
      "['def detailed_ dangerous (dumps𐑎']\n",
      "['fragmentseffect_ getNum (forwardsen_XX']\n",
      "['def phase_type ( PsiFieldpython']\n",
      "['AUTHORIZ get_ elb ( charseten_XX']\n",
      "['def truncate (message (ibleen_XX']\n",
      "['defForAll_body ( foren_XX']\n",
      "['霜橢 ( \"./ (forNameen_XX']\n",
      "['rt make (멘 (`en_XX']\n",
      "['def dump (fields ( #颂']\n",
      "['def set (svc antoin蝎']\n",
      "['䱴 prepare_押 ( Ween_XX']\n",
      "['def pubkey_world ( # C']\n",
      "['def Regex (length ( assert outcome']\n",
      "['def stage (module ( #en_XX']\n",
      "['onDestroy補sun宠_ localen_XX']\n",
      "['def build (field ( @python']\n",
      "['def get_listscalCmp缃']\n",
      "['defUI_side ( symlink Our']\n",
      "['defrí (of ( std plt']\n",
      "['def get_branch ( \"\"\" getHe']\n",
      "['def getlist ( \"< ( #python']\n",
      "['def get_rinclude 0.00 Do']\n",
      "['def localEquivalentfiles ( #en_XX']\n",
      "['def get ( consuming ( #걀']\n",
      "['def process (n ( Entryen_XX']\n",
      "['def file (lace (77en_XX']\n",
      "['crashesInputs_followingã outsubnet']\n",
      "['def _ (file ( Invoen_XX']\n",
      "['⳹ get_module ( #`']\n",
      "['def create (fn ( ifen_XX']\n",
      "['def get (javax (᥅en_XX']\n",
      "['def export (Coord ()en_XX']\n",
      "['defCookie_㴵 ( city UNIQUE']\n",
      "['def get (file_ # param']\n",
      "['def add (table ( mListener According']\n",
      "['def get (helper ( pd D']\n",
      "['defpriv (side ( #en_XX']\n",
      "['def render (area_ #python']\n",
      "['fs make (style ( #S']\n",
      "['def _ (n ( triggered0\"']\n",
      "['def edge (Object ( componentḵ']\n",
      "['def process (ꗡ (Filenameen_XX']\n",
      "['def process ( few ( {en_XX']\n",
      "['def transformttp halfs {Sleep']\n",
      "['. overriding calleemodule ( if ex']\n",
      "['getPolicy resolve_list (ľen_XX']\n",
      "['def承_tCaching #en_XX']\n",
      "['def 22:_ategoryfields f</s>']\n",
      "['def rest (汉 ( #Iteration']\n",
      "['𐓰Ҿ_google (FSsyn']\n",
      "['def load ( getY ( Uripython']\n",
      "['def load (糣_Lgenre']\n",
      "['def schema (f ( \"\"\"᭐']\n",
      "['defrackwall脴 ( #en_XX']\n",
      "['defboot (=<_ globalpython']\n",
      "['Avg rstmon linalg ( #en_XX']\n",
      "['def computed_䭙 ( # find']\n",
      "['def gen (standardٳ \"\"\"</s>']\n",
      "['def get dj everytime ( on🌠']\n",
      "['def list_button ( # Persist']\n",
      "['switching alter (block ( #Ỗ']\n",
      "['def ref (Reser ( lOutput']\n",
      "['def generateR Cython ( # method']\n",
      "['def export (proto ( #en_XX']\n",
      "['def✉_Section ( #en_XX']\n",
      "['defvetic (op ({en_XX']\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "num_epochs = 100\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # for data in enumerate(dataloader, 0):\n",
    "    ############################\n",
    "    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    ###########################\n",
    "    ## Train with all-real batch\n",
    "    discriminator.zero_grad()\n",
    "    # Format batch\n",
    "    real_data = inputs.input_ids\n",
    "    \n",
    "    # Forward pass real batch through D\n",
    "    output = discriminator(real_data)\n",
    "    \n",
    "    # Calculate loss on all-real batch\n",
    "    discriminator_real_loss = criterion(output, real_style_tensor.unsqueeze(0))\n",
    "    \n",
    "    # Calculate gradients for D in backward pass\n",
    "    discriminator_real_loss.backward()\n",
    "    # D_x = output.mean().item()\n",
    "    \n",
    "    ## Train with all-fake batch\n",
    "    \n",
    "    generator_output = model(**inputs, style_encoding=style_encoding)\n",
    "    generated_logits = generator_output.logits\n",
    "    generated_tokens = torch.nn.functional.gumbel_softmax(generated_logits, hard=True, dim=-1)\n",
    "    fake_data = generated_tokens.argmax(-1)\n",
    "    print(tokenizer.batch_decode(fake_data))\n",
    "    # Classify all fake batch with D\n",
    "    output = discriminator(fake_data)\n",
    "    \n",
    "    # Calculate D's loss on the all-fake batch\n",
    "    discriminator_fake_loss = criterion(output, fake_label_tensor)\n",
    "    \n",
    "    # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "    discriminator_fake_loss.backward()\n",
    "    \n",
    "    # D_G_z1 = output.mean().item()\n",
    "    \n",
    "    # Compute error of D as sum over the fake and the real batches\n",
    "    discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n",
    "    \n",
    "    # Update D\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    ############################\n",
    "    # (2) Update G network: maximize log(D(G(z)))\n",
    "    ###########################\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "    output = discriminator(fake_data)\n",
    "    # Calculate G's loss based on this output\n",
    "    generator_class_loss = criterion(output, style_tensor)\n",
    "    # Calculate gradients for G\n",
    "    generator_class_loss.backward()\n",
    "    # D_G_z2 = output.mean().item()\n",
    "    # Update G\n",
    "    generator_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef80f5e-3704-41d4-b67a-b58d1dcacf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = \n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6bd503-7080-4c3e-922b-7be06b8839cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
