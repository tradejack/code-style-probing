{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for training pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset\n",
    "import pickle\n",
    "from utils.helper import read_py150k_code, read_file_to_string\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_prefix = \"/data/users/team2_capstone/code-style-probing/\"\n",
    "\n",
    "target_features = [\n",
    "     'snake_case_var_ratio',\n",
    "     'snake_case_class_ratio',\n",
    "     'snake_case_method_ratio',\n",
    "     'upper_camel_case_var_ratio',\n",
    "     'upper_camel_case_class_ratio',\n",
    "     'upper_camel_case_method_ratio',\n",
    "     'lower_camel_case_var_ratio',\n",
    "     'lower_camel_case_class_ratio',\n",
    "     'lower_camel_case_method_ratio',\n",
    "     'func_decorators_avg',\n",
    "     'class_decorators_avg',\n",
    "     'class_parents_avg',\n",
    "     'comprehensions_avg',\n",
    "     'generators_avg',\n",
    "     'lambda_avg',\n",
    "     'comment_density',\n",
    "     'ds_density',\n",
    "]\n",
    "PY150K_DIR = fname_prefix + \"data/py150\"\n",
    "PY150K_CODE_DIR = fname_prefix + \"data/py150/py150_files\"\n",
    "PY150K_TRAIN_AST = fname_prefix + \"data/py150/python100k_train.json\"\n",
    "PY150K_EVAL_AST = fname_prefix + \"data/py150/python50k_eval.json\"\n",
    "PY150K_TRAIN_CODE = fname_prefix + \"data/py150/py150_files/python100k_train.txt\"\n",
    "PY150K_EVAL_CODE = fname_prefix + \"data/py150/py150_files/python50k_eval.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "py150k_df = pd.read_csv(fname_prefix + \"data/py150k_metric_20220527.csv\")\n",
    "bigquery_df = pd.read_csv(fname_prefix + \"data/bigquery_metric_20220526.csv\")\n",
    "bq_content_df = pd.read_csv(fname_prefix + f'data/BigQuery/files/cubert_metadata000000000000') \n",
    "\n",
    "combined_df = pd.concat([py150k_df, bigquery_df], axis = 0) \n",
    "combined_df['file'] = [ x + y  for x, y in zip(combined_df['repository'], combined_df['filepath'])]\n",
    "bq_content_df['file'] = [ x + y  for x, y in zip(bq_content_df['repository'], bq_content_df['filepath'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname_prefix + \"data/combined_dataset/clusters/feature_set_1/full_feature_clusterer.pickle\", \"rb\") as file:\n",
    "    cluster_pred = pickle.load(file)\n",
    "    labels = cluster_pred.labels_\n",
    "    cluster_num = len(np.unique(labels))\n",
    "combined_df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get code\n",
    "\n",
    "\n",
    "code_filenames = read_py150k_code(PY150K_TRAIN_CODE)\n",
    "#print ((code_filenames[1]))\n",
    "py150_path = []\n",
    "py150_code = []\n",
    "for i in range(0, len(code_filenames)):\n",
    "    #print (i)\n",
    "    try:\n",
    "        py150_code.append( read_file_to_string( #regex codefilenames drop data/py150/py150_files/data/ \n",
    "            f\"{PY150K_CODE_DIR}/{code_filenames[i]}\"\n",
    "        ))\n",
    "        py150_path.append(f\"{code_filenames[i]}\")\n",
    "    except: # currently appending empty string for empy files\n",
    "        print (f\"{PY150K_CODE_DIR}/{code_filenames[i]}\")\n",
    "        py150_code.append( \"File Error\"\n",
    "        )\n",
    "        py150_path.append(f\"{code_filenames[i]}\")\n",
    "ex_files = list(combined_df['file'])\n",
    "filtered_bq = bq_content_df[bq_content_df['file'].isin(ex_files)]\n",
    "bigquery_code = list(filtered_bq['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_total_len</th>\n",
       "      <th>comment_avg_len</th>\n",
       "      <th>comment_density</th>\n",
       "      <th>id_total</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>id_total_var</th>\n",
       "      <th>lower_case_var</th>\n",
       "      <th>snake_case_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>repository</th>\n",
       "      <th>filepath</th>\n",
       "      <th>forks</th>\n",
       "      <th>issue_events</th>\n",
       "      <th>stars</th>\n",
       "      <th>parse_error</th>\n",
       "      <th>func_async_count</th>\n",
       "      <th>file</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/00/wikihouse/urls.py</td>\n",
       "      <td>19</td>\n",
       "      <td>#!/usr/bin/env python\\n# -*- coding: utf-8 -*-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>363.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>51.833333</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>381.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.341207</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/0rpc/zerorpc-python/zerorpc/events.py</td>\n",
       "      <td>-1</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n# Open Source Initiat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/0xadada/dockdj/app/manage.py</td>\n",
       "      <td>19</td>\n",
       "      <td>#!/usr/bin/env python\\n\"\"\"Django's command lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/1stvamp/hippybot/setup.py</td>\n",
       "      <td>16</td>\n",
       "      <td>\"\"\"Installer for hippybot\\n\"\"\"\\n\\nimport os\\nc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/2buntu/2buntu-blog/manage.py</td>\n",
       "      <td>19</td>\n",
       "      <td>#!/usr/bin/env python\\nimport os\\nimport sys\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115045</th>\n",
       "      <td>1292.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>7829.0</td>\n",
       "      <td>38.950249</td>\n",
       "      <td>0.155573</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>0.056695</td>\n",
       "      <td>...</td>\n",
       "      <td>oscarbranson/latools</td>\n",
       "      <td>latools/helpers/plot.py</td>\n",
       "      <td>11.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oscarbranson/latoolslatools/helpers/plot.py</td>\n",
       "      <td>-1</td>\n",
       "      <td>\"\"\"\\nPlotting functions.\\n\\n(c) Oscar Branson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115046</th>\n",
       "      <td>194.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>37.235294</td>\n",
       "      <td>0.087629</td>\n",
       "      <td>63.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>...</td>\n",
       "      <td>erjac77/ansible-module-f5bigip</td>\n",
       "      <td>library/f5bigip_ltm_monitor_snmp_dca.py</td>\n",
       "      <td>5.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>erjac77/ansible-module-f5bigiplibrary/f5bigip_...</td>\n",
       "      <td>-1</td>\n",
       "      <td>#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115047</th>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>111.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>...</td>\n",
       "      <td>python-hyper/hyper-h2</td>\n",
       "      <td>test/test_config.py</td>\n",
       "      <td>139.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-hyper/hyper-h2test/test_config.py</td>\n",
       "      <td>-1</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\ntest_config\\n~~~...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115048</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>...</td>\n",
       "      <td>techbureau/zaifbot</td>\n",
       "      <td>tests/trade/test_trade.py</td>\n",
       "      <td>14.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>techbureau/zaifbottests/trade/test_trade.py</td>\n",
       "      <td>-1</td>\n",
       "      <td>import unittest\\nfrom zaifbot.trade.trade impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115049</th>\n",
       "      <td>2910.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>4932.0</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>0.024742</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>2995.0</td>\n",
       "      <td>3009.0</td>\n",
       "      <td>2992.0</td>\n",
       "      <td>0.026589</td>\n",
       "      <td>...</td>\n",
       "      <td>bingopodcast/bingos</td>\n",
       "      <td>bingo_emulator/singapore/game.py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bingopodcast/bingosbingo_emulator/singapore/ga...</td>\n",
       "      <td>-1</td>\n",
       "      <td>#!/usr/bin/python\\n\\nimport logging\\nlogging.b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215050 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        line_count  comment_count  comment_total_len  comment_avg_len  \\\n",
       "0            116.0            2.0               44.0        22.000000   \n",
       "1            363.0           24.0             1244.0        51.833333   \n",
       "2             13.0            1.0               21.0        21.000000   \n",
       "3             34.0            0.0                0.0         0.000000   \n",
       "4             11.0            1.0               21.0        21.000000   \n",
       "...            ...            ...                ...              ...   \n",
       "115045      1292.0          201.0             7829.0        38.950249   \n",
       "115046       194.0           17.0              633.0        37.235294   \n",
       "115047       145.0            1.0               23.0        23.000000   \n",
       "115048        48.0            0.0                NaN         0.000000   \n",
       "115049      2910.0           72.0             4932.0        68.500000   \n",
       "\n",
       "        comment_density  id_total  lower_case  id_total_var  lower_case_var  \\\n",
       "0              0.017241       1.0         1.0           1.0             1.0   \n",
       "1              0.066116     381.0       214.0         333.0           195.0   \n",
       "2              0.076923       3.0         3.0           3.0             3.0   \n",
       "3              0.000000      26.0        18.0          26.0            18.0   \n",
       "4              0.090909       3.0         3.0           3.0             3.0   \n",
       "...                 ...       ...         ...           ...             ...   \n",
       "115045         0.155573    1658.0      1533.0        1646.0          1527.0   \n",
       "115046         0.087629      63.0        42.0          57.0            41.0   \n",
       "115047         0.006897     111.0        76.0          99.0            76.0   \n",
       "115048         0.000000      53.0        44.0          47.0            44.0   \n",
       "115049         0.024742    3084.0      2995.0        3009.0          2992.0   \n",
       "\n",
       "        snake_case_ratio  ...                      repository  \\\n",
       "0               0.000000  ...                             NaN   \n",
       "1               0.341207  ...                             NaN   \n",
       "2               0.000000  ...                             NaN   \n",
       "3               0.307692  ...                             NaN   \n",
       "4               0.000000  ...                             NaN   \n",
       "...                  ...  ...                             ...   \n",
       "115045          0.056695  ...            oscarbranson/latools   \n",
       "115046          0.126984  ...  erjac77/ansible-module-f5bigip   \n",
       "115047          0.270270  ...           python-hyper/hyper-h2   \n",
       "115048          0.094340  ...              techbureau/zaifbot   \n",
       "115049          0.026589  ...             bingopodcast/bingos   \n",
       "\n",
       "                                       filepath  forks  issue_events  stars  \\\n",
       "0                                           NaN    NaN           NaN    NaN   \n",
       "1                                           NaN    NaN           NaN    NaN   \n",
       "2                                           NaN    NaN           NaN    NaN   \n",
       "3                                           NaN    NaN           NaN    NaN   \n",
       "4                                           NaN    NaN           NaN    NaN   \n",
       "...                                         ...    ...           ...    ...   \n",
       "115045                  latools/helpers/plot.py   11.0          77.0    9.0   \n",
       "115046  library/f5bigip_ltm_monitor_snmp_dca.py    5.0          72.0    6.0   \n",
       "115047                      test/test_config.py  139.0         376.0  754.0   \n",
       "115048                tests/trade/test_trade.py   14.0         198.0   42.0   \n",
       "115049         bingo_emulator/singapore/game.py    1.0         511.0    3.0   \n",
       "\n",
       "        parse_error  func_async_count  \\\n",
       "0               NaN               NaN   \n",
       "1               NaN               NaN   \n",
       "2               NaN               NaN   \n",
       "3               NaN               NaN   \n",
       "4               NaN               NaN   \n",
       "...             ...               ...   \n",
       "115045          1.0               NaN   \n",
       "115046          1.0               NaN   \n",
       "115047          1.0               NaN   \n",
       "115048          1.0               NaN   \n",
       "115049          1.0               NaN   \n",
       "\n",
       "                                                     file  labels  \\\n",
       "0                              /data/00/wikihouse/urls.py      19   \n",
       "1             /data/0rpc/zerorpc-python/zerorpc/events.py      -1   \n",
       "2                      /data/0xadada/dockdj/app/manage.py      19   \n",
       "3                         /data/1stvamp/hippybot/setup.py      16   \n",
       "4                      /data/2buntu/2buntu-blog/manage.py      19   \n",
       "...                                                   ...     ...   \n",
       "115045        oscarbranson/latoolslatools/helpers/plot.py      -1   \n",
       "115046  erjac77/ansible-module-f5bigiplibrary/f5bigip_...      -1   \n",
       "115047           python-hyper/hyper-h2test/test_config.py      -1   \n",
       "115048        techbureau/zaifbottests/trade/test_trade.py      -1   \n",
       "115049  bingopodcast/bingosbingo_emulator/singapore/ga...      -1   \n",
       "\n",
       "                                                  content  \n",
       "0       #!/usr/bin/env python\\n# -*- coding: utf-8 -*-...  \n",
       "1       # -*- coding: utf-8 -*-\\n# Open Source Initiat...  \n",
       "2       #!/usr/bin/env python\\n\"\"\"Django's command lin...  \n",
       "3       \"\"\"Installer for hippybot\\n\"\"\"\\n\\nimport os\\nc...  \n",
       "4       #!/usr/bin/env python\\nimport os\\nimport sys\\n...  \n",
       "...                                                   ...  \n",
       "115045  \"\"\"\\nPlotting functions.\\n\\n(c) Oscar Branson ...  \n",
       "115046  #!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\...  \n",
       "115047  # -*- coding: utf-8 -*-\\n\"\"\"\\ntest_config\\n~~~...  \n",
       "115048  import unittest\\nfrom zaifbot.trade.trade impo...  \n",
       "115049  #!/usr/bin/python\\n\\nimport logging\\nlogging.b...  \n",
       "\n",
       "[215050 rows x 94 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hugging face dataset\n",
    "\n",
    "print (len(filtered_bq))\n",
    "combined_code = py150_code + bigquery_code\n",
    "combined_df['file'] =py150_path + list(combined_df['file'][100000:])\n",
    "combined_df['content'] = combined_code\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df to dataset\n",
    "dataset = Dataset.from_pandas(combined_df).train_test_split(test_size=0.2)\n",
    "combined_df.to_csv(fname_prefix + 'data/labeled_code/combined_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/vocab.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\1e2aacf615bc83f25a9d748eccb762b335eee01a29ab7a8db9b8e86cc851d489.9a48c5abf25554713c6513ab01066e53569b9a2da0d6189715951cf7c6288805\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/merges.txt from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\7eaa9b856402f05e8fdd452951872ecd3c2692ea9abb86b7ab62b07e3bc5f7de.7179059568f1a130b0a79e4bac71f38545207cab0ec45ce82ca09afadb2649a3\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/added_tokens.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\a3e93db547e41cdd21f01826d07c5679e111b02d8e969c607611c30a6acbe191.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/special_tokens_map.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\5941df5e4315c5ab63b7b2ac791fb0bf0f209744a055c06b43b5274849137cdd.b9905d0575bde443a20834122b6e2d48e853b2e36444ce98ddeb43c38097eb3f\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/tokenizer_config.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\c99468017f7cb1b243c80a5640fd483688c5ec58bcd18b64efa5b82d8df7bc24.f1b0f4acf5601ca7b482b9f000524cffdc0c3950f7d8c45c32380bc213334af2\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\e57de2ba12d2b1d3cae7ce5921704890ac50789e8eb95100ff4c64dc98559729.c65001d1986897f4ae6d41d2c49f0e1621d3518cab63e0ffa3005e5deb5aae40\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/config.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\68f40642f8534e3482166065ce817305c07e1f4b8ea96013fe62ab865088bddb.8929a51af95d04be1b1d966435fd1ad4a48aca2f2953bc4e4a354b1f1dfd0b55\n",
      "Model config PLBartConfig {\n",
      "  \"_name_or_path\": \"uclanlp/plbart-python-en_XX\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"PLBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"plbart\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50005\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, PLBartTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"python\" )\n",
    "#return_tensors ='pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/vocab.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\1e2aacf615bc83f25a9d748eccb762b335eee01a29ab7a8db9b8e86cc851d489.9a48c5abf25554713c6513ab01066e53569b9a2da0d6189715951cf7c6288805\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/merges.txt from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\7eaa9b856402f05e8fdd452951872ecd3c2692ea9abb86b7ab62b07e3bc5f7de.7179059568f1a130b0a79e4bac71f38545207cab0ec45ce82ca09afadb2649a3\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/added_tokens.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\a3e93db547e41cdd21f01826d07c5679e111b02d8e969c607611c30a6acbe191.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/special_tokens_map.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\5941df5e4315c5ab63b7b2ac791fb0bf0f209744a055c06b43b5274849137cdd.b9905d0575bde443a20834122b6e2d48e853b2e36444ce98ddeb43c38097eb3f\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/tokenizer_config.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\c99468017f7cb1b243c80a5640fd483688c5ec58bcd18b64efa5b82d8df7bc24.f1b0f4acf5601ca7b482b9f000524cffdc0c3950f7d8c45c32380bc213334af2\n",
      "100%|██████████| 173/173 [17:40<00:00,  6.13s/ba]\n",
      "100%|██████████| 44/44 [04:14<00:00,  5.78s/ba]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "def tokenization(example):\n",
    "    #print (tokenizer(example[\"content\"], padding='max_length', truncation=True).keys())\n",
    "    return tokenizer(example[\"content\"], padding='max_length', truncation=True)\n",
    "train_dataset = dataset[\"train\"].map(tokenization, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_dataset.format['type']\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(tokenization, batched=True)\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.format['type']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(20), 'input_ids': tensor([    1,     7, 14848,    30,  7718,    17,    28,   203,   203,  8395,\n",
      "          203,   565, 24314, 10211,  8392,  1491,   203,   203,   565,   411,\n",
      "           84,    34,  1986, 24314, 10211,  8392,  1491, 19808,  1846,   358,\n",
      "        16592,   598, 24314, 10211,  7084,  1450,  4529,  8392,  1491,  8513,\n",
      "           18,  4554,   848,   999,   326,  8392,  1491,   358, 18472,   340,\n",
      "         2975,   715,  7120,  5295,  4123,   487,  6635,  1047,  1998,  5550,\n",
      "        17347,    84,  4438,    84,    34,  9434,  1846,  1221,  8392,  1491,\n",
      "         4097,  8220,   326, 24314, 10211,  8392,  1491,  7323,  1846,  1297,\n",
      "          527,   326,  1446,  1239,  6063,    30, 25892,   264,   473,  5618,\n",
      "        23480,  5618,    31,  2557,    17,  8412,    10,  4521, 23480,  4521,\n",
      "           31,  2412,   358,  3433,  2239,  3285, 17347,    84,    34,   225,\n",
      "          468,  8054,    30,   512,  9172,   203,   203,   565,  3502,  2557,\n",
      "          857,  1177,    30,   331,    22,   203,   565, 13329,    30,   462,\n",
      "          305,   381, 12124,    36, 17838, 10211,    18,   832,   203,   565,\n",
      "        11025,   635,    30,  2333,  2207,  6662,    18,   832,    19, 23258,\n",
      "           17,  2425,    19, 23258,    17,   710,  4507,    18,  6845,   203,\n",
      "         8395,   203,   203,   203,  2080,  1001, 14343,   972,  1930,  4967,\n",
      "           67,  5666,   203,   203,  5666,  2836,  3813,   203,   203,  5666,\n",
      "        10794, 10211,    67,  2425,    67,  2625,   203,  2080, 10794, 10211,\n",
      "           67,  2425,    67,  2625,    18,  7665,    18,  3278,    67,  4631,\n",
      "         1930, 24792,   225,   468,  8054,    30,   512,  9172,   203,  2080,\n",
      "        10794, 10211,    67,  2425,    67,  2625,    18,  8792,  1930, 10873,\n",
      "          203,   203,   203,  1106,  7766, 27441,    12,  4873,  3813,    18,\n",
      "         4709,  2449,  4672,   203,   565,  3536, 27441,  2836,  1842,  7168,\n",
      "           87,  8395,   203,   203,   565,  1652, 24292,    12,  2890,  4672,\n",
      "          203,  3639,  1342,   203,   203,   565,  1652,   268,  2091,  4164,\n",
      "           12,  2890,  4672,   203,  3639,  1342,   203,   203,   565,  1652,\n",
      "         1842, 27441,    12,  2890,  4672,   203,  3639,  3536,  4709, 24792,\n",
      "         8395,   203,  3639,   468,  9852,    30,  4872,   733,   598, 11791,\n",
      "         1677,   598,  3454,   924,   203,  3639,   468,   938,   273, 10794,\n",
      "        10211,    67,  2425,    67,  2625,    18,  7665,    18,  3278,    67,\n",
      "         4631,    18, 27441,  1435,   225,   468,  8054,    30,   512,  9172,\n",
      "          203,  3639,  1342,   203,   203,   203,   430,  1001,   529,   972,\n",
      "          422,  4940,  5254,   972,  4278,   203,   565,  2836,  3813,    18,\n",
      "         5254,  1435,   203,     2,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print ((train_dataset[0]))\n",
    "train_dataset.save_to_disk(\"datasets/codet5_train.hf\")\n",
    "test_dataset.save_to_disk(\"datasets/codet5_test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\e57de2ba12d2b1d3cae7ce5921704890ac50789e8eb95100ff4c64dc98559729.c65001d1986897f4ae6d41d2c49f0e1621d3518cab63e0ffa3005e5deb5aae40\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/uclanlp/plbart-python-en_XX/resolve/main/config.json from cache at C:\\Users\\km201/.cache\\huggingface\\transformers\\68f40642f8534e3482166065ce817305c07e1f4b8ea96013fe62ab865088bddb.8929a51af95d04be1b1d966435fd1ad4a48aca2f2953bc4e4a354b1f1dfd0b55\n",
      "Model config PLBartConfig {\n",
      "  \"_name_or_path\": \"uclanlp/plbart-python-en_XX\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"PLBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"plbart\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50005\n",
      "}\n",
      "\n",
      "100%|██████████| 173/173 [14:57<00:00,  5.19s/ba]\n",
      "100%|██████████| 44/44 [03:45<00:00,  5.13s/ba]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"python\" )\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(tokenization, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_dataset.format['type']\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(tokenization, batched=True)\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(20), 'input_ids': tensor([  754,  6971, 33475,  ...,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print ((train_dataset[0]))\n",
    "train_dataset.save_to_disk(\"datasets/plbart_train.hf\")\n",
    "test_dataset.save_to_disk(\"datasets/plbart_test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'line_count': 37.0, 'comment_count': 0.0, 'comment_total_len': 0.0, 'comment_avg_len': 0.0, 'comment_density': 0.0, 'id_total': 35.0, 'lower_case': 26.0, 'id_total_var': 34.0, 'lower_case_var': 26.0, 'snake_case_ratio': 0.2, 'snake_case_var_ratio': 0.176471, 'snake_case_class_ratio': 0.0, 'snake_case_method_ratio': 1.0, 'lower_camel_case_ratio': 0.0, 'lower_camel_case_var_ratio': 0.0, 'lower_camel_case_class_ratio': 0.0, 'lower_camel_case_method_ratio': 0.0, 'upper_camel_case_ratio': 0.057143, 'upper_camel_case_var_ratio': 0.058824, 'upper_camel_case_class_ratio': 0.0, 'upper_camel_case_method_ratio': 0.0, 'lower_case_ratio': 0.742857, 'lower_case_var_ratio': 0.764706, 'lower_case_class_ratio': 0.0, 'lower_case_method_ratio': 0.0, 'upper_case_ratio': 0.0, 'upper_case_var_ratio': 0.0, 'upper_case_class_ratio': 0.0, 'upper_case_method_ratio': 0.0, 'other_case_ratio': 0.0, 'other_case_var_ratio': 0.0, 'other_case_class_ratio': 0.0, 'other_case_method_ratio': 0.0, 'func_decorators_avg': 0.0, 'func_async_ratio': 0.0, 'class_parents_avg': 0.0, 'class_decorators_avg': 0.0, 'ds_density': 0.0, 'ds_char_len_avg': 0.0, 'ds_word_len_avg': 0.0, 'comprehensions_avg': 0.0, 'generators_avg': 0.0, 'lambda_avg': 0.0, 'snake_case': 7.0, 'snake_case_var': 6.0, 'id_total_method': 1.0, 'snake_case_method': 1.0, 'func_count': 1.0, 'func_decorators_count': 0.0, 'lower_camel_case': 0.0, 'lower_camel_case_var': 0.0, 'upper_camel_case': 2.0, 'id_total_class': 0.0, 'upper_camel_case_class': 0.0, 'class_count': 0.0, 'class_parents_count': 0.0, 'class_decorators_count': 0.0, 'internal_method': 0.0, 'overridden_method': 0.0, 'upper_case': 0.0, 'upper_case_var': 0.0, 'upper_camel_case_var': 2.0, 'lower_case_method': 0.0, 'generators': 0.0, 'ds_count': 0.0, 'ds_char_len_total': 0.0, 'ds_word_len_total': 0.0, 'ds_line_count': 0.0, 'ds_of_method': 0.0, 'lambda': 0.0, 'other_case': 0.0, 'other_case_class': 0.0, 'other_case_var': 0.0, 'comprehensions': 0.0, 'lower_camel_case_method': 0.0, 'lower_case_class': 0.0, 'snake_case_class': 0.0, 'upper_camel_case_method': 0.0, 'other_case_method': 0.0, 'upper_case_method': 0.0, 'lower_camel_case_class': 0.0, 'upper_case_class': 0.0, 'repository': None, 'filepath': None, 'forks': None, 'issue_events': None, 'stars': None, 'parse_error': None, 'func_async_count': None, 'file': None, 'labels': -1, 'content': 'import os\\nimport shutil\\nimport argparse\\nfrom astropy.utils import data\\nfrom astroplan import download_IERS_A\\n\\n\\ndef download_all_files(data_folder=\"{}/astrometry/data\".format(os.getenv(\\'PANDIR\\'))):\\n    download_IERS_A()\\n\\n    for i in range(4214, 4219):\\n        fn = \\'index-{}.fits\\'.format(i)\\n        dest = \"{}/{}\".format(data_folder, fn)\\n\\n        if not os.path.exists(dest):\\n            url = \"http://data.astrometry.net/4200/{}\".format(fn)\\n            df = data.download_file(url)\\n            try:\\n                shutil.move(df, dest)\\n            except OSError as e:\\n                print(\"Problem saving. (Maybe permissions?): {}\".format(e))\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(\\n        description=__doc__,\\n        formatter_class=argparse.RawDescriptionHelpFormatter)\\n\\n    parser.add_argument(\\'--folder\\', help=\\'Folder to place astrometry data\\')\\n\\n    args = parser.parse_args()\\n\\n    if args.folder and not os.path.exists(args.folder):\\n        print(\"{} does not exist.\".format(args.folder))\\n\\n    download_all_files(data_folder=args.folder)\\n', '__index_level_0__': 52561, 'input_ids': [662, 763, 662, 8641, 662, 13280, 320, 4512, 10588, 33455, 3444, 662, 272, 320, 4512, 59, 4638, 662, 2546, 33456, 33462, 9984, 33456, 33482, 134, 2546, 33456, 278, 33456, 2507, 33460, 642, 33456, 6096, 286, 11177, 33488, 473, 240, 4156, 33488, 642, 2392, 2173, 33460, 338, 33455, 300, 2888, 574, 33486, 1368, 5197, 6474, 988, 2546, 33456, 33462, 9984, 33456, 33482, 292, 126, 25, 55, 1369, 33460, 7880, 17278, 5240, 2530, 988, 3619, 24, 35, 1377, 33489, 11177, 33455, 33458, 1147, 3764, 2173, 33460, 33443, 33459, 1798, 24, 40, 11177, 33488, 11177, 2392, 2173, 33460, 642, 33456, 6096, 33463, 3619, 33459, 105, 188, 763, 33455, 675, 33455, 6717, 33460, 6846, 988, 681, 24, 40, 1285, 682, 642, 33455, 473, 240, 4156, 33455, 1410, 10656, 3250, 33488, 11177, 2392, 2173, 33460, 3608, 33459, 1674, 24, 272, 33455, 6095, 33456, 564, 33460, 496, 33459, 367, 33475, 8641, 33455, 6246, 33460, 1616, 33463, 1798, 33459, 1004, 7994, 268, 168, 33475, 597, 533, 11078, 7985, 33455, 5, 28112, 6502, 33514, 988, 31217, 2173, 33460, 33439, 1580, 105, 429, 308, 387, 258, 10150, 2571, 17122, 1910, 24, 13280, 33455, 2104, 2472, 33460, 2106, 33470, 387, 2254, 19736, 7064, 33456, 913, 33470, 1506, 1787, 33455, 6412, 3365, 7672, 6237, 33459, 1910, 33455, 798, 33456, 4241, 574, 613, 6096, 484, 882, 1673, 4981, 71, 2257, 4512, 240, 4156, 272, 1156, 680, 24, 1910, 33455, 1787, 33456, 731, 292, 105, 680, 33455, 6096, 135, 188, 763, 33455, 675, 33455, 6717, 33460, 731, 33455, 6096, 988, 597, 533, 11177, 657, 188, 1227, 33455, 2392, 2173, 33460, 731, 33455, 6096, 1580, 2546, 33456, 278, 33456, 2507, 33460, 642, 33456, 6096, 33470, 731, 33455, 6096, 33459, 2, 50002, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "test = load_from_disk('datasets/plbart_test.hf')\n",
    "test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print ((test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "988cc1e00d3b5bb0a2e9024406047781d3e298e90a30d1fcc633613d0d680479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
