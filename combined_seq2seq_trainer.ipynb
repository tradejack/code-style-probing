{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "fname_prefix = \"/data/users/team2_capstone/code-style-probing/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pandas as pd\n",
    "from transformers import *\n",
    "import transformers.trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk, load_metric\n",
    "#import datasets\n",
    "import pickle\n",
    "#from utils.helper import read_py150k_code, read_file_to_string\n",
    "import regex as re\n",
    "from transformers.data.data_collator import DataCollator, InputDataClass\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from typing import List, Union, Dict\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "#from transformers.training_args import is_tpu_available\n",
    "#from transformers.trainer import get_tpu_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/Salesforce/codet5-small/resolve/main/vocab.json from cache at /soe/ksmunson/.cache/huggingface/transformers/12dd1d7e87ade1728f8382f3b875a47cdddb88bc50797ecc506957411661a39a.9a48c5abf25554713c6513ab01066e53569b9a2da0d6189715951cf7c6288805\n",
      "loading file https://huggingface.co/Salesforce/codet5-small/resolve/main/merges.txt from cache at /soe/ksmunson/.cache/huggingface/transformers/c2ada6c76bb6a90c2330323775cb4853dd2b0cfee29d6b2c5ecb419c5874b488.7179059568f1a130b0a79e4bac71f38545207cab0ec45ce82ca09afadb2649a3\n",
      "loading file https://huggingface.co/Salesforce/codet5-small/resolve/main/added_tokens.json from cache at /soe/ksmunson/.cache/huggingface/transformers/354ae288bdee65437fa8eedecf9a2770001b97bac23d7fc5a04badae8da42346.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/Salesforce/codet5-small/resolve/main/special_tokens_map.json from cache at /soe/ksmunson/.cache/huggingface/transformers/f420b33feb234cfc688fa17febbada1c21692d80c819c5fa8147ccdca57071ac.b9905d0575bde443a20834122b6e2d48e853b2e36444ce98ddeb43c38097eb3f\n",
      "loading file https://huggingface.co/Salesforce/codet5-small/resolve/main/tokenizer_config.json from cache at /soe/ksmunson/.cache/huggingface/transformers/467e224188b39d74622382aee9b73c080650b67706599bb5592d1dcf40149f9d.f1b0f4acf5601ca7b482b9f000524cffdc0c3950f7d8c45c32380bc213334af2\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/vocab.json from cache at /soe/ksmunson/.cache/huggingface/transformers/1e2aacf615bc83f25a9d748eccb762b335eee01a29ab7a8db9b8e86cc851d489.9a48c5abf25554713c6513ab01066e53569b9a2da0d6189715951cf7c6288805\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/merges.txt from cache at /soe/ksmunson/.cache/huggingface/transformers/7eaa9b856402f05e8fdd452951872ecd3c2692ea9abb86b7ab62b07e3bc5f7de.7179059568f1a130b0a79e4bac71f38545207cab0ec45ce82ca09afadb2649a3\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/added_tokens.json from cache at /soe/ksmunson/.cache/huggingface/transformers/a3e93db547e41cdd21f01826d07c5679e111b02d8e969c607611c30a6acbe191.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/special_tokens_map.json from cache at /soe/ksmunson/.cache/huggingface/transformers/5941df5e4315c5ab63b7b2ac791fb0bf0f209744a055c06b43b5274849137cdd.b9905d0575bde443a20834122b6e2d48e853b2e36444ce98ddeb43c38097eb3f\n",
      "loading file https://huggingface.co/Salesforce/codet5-base/resolve/main/tokenizer_config.json from cache at /soe/ksmunson/.cache/huggingface/transformers/c99468017f7cb1b243c80a5640fd483688c5ec58bcd18b64efa5b82d8df7bc24.f1b0f4acf5601ca7b482b9f000524cffdc0c3950f7d8c45c32380bc213334af2\n",
      "Adding <nl> to the vocabulary\n",
      "Adding </nl> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/data/users/team2_capstone/code-style-probing/datasets/combined_model_nl_prompt/codet5_train_case_bq_padded.hf/tmppjrqwfcg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#debugging_dataset = load_from_disk(fname_prefix + 'datasets/combined_model/codet5_train_comp_bq_padded.hf')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m train_comp_dataset \u001b[39m=\u001b[39m load_from_disk(fname_prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdatasets/combined_model_nl_prompt/codet5_train_comp_bq_padded.hf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m train_casing_dataset \u001b[39m=\u001b[39m load_from_disk(fname_prefix \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdatasets/combined_model_nl_prompt/codet5_train_case_bq_padded.hf\u001b[39;49m\u001b[39m'\u001b[39;49m, )\u001b[39m.\u001b[39;49mtrain_test_split(test_size \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m train_docstring_dataset \u001b[39m=\u001b[39m load_from_disk(fname_prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdatasets/combined_model_nl_prompt/codet5_train_docstring_fixed_bq_padded.hf\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtrain_test_split(test_size \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#split for \u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    512\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    513\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    514\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    515\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    516\u001b[0m }\n\u001b[1;32m    517\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    519\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    520\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/arrow_dataset.py:3665\u001b[0m, in \u001b[0;36mDataset.train_test_split\u001b[0;34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[0m\n\u001b[1;32m   3662\u001b[0m         test_indices \u001b[39m=\u001b[39m permutation[:n_test]\n\u001b[1;32m   3663\u001b[0m         train_indices \u001b[39m=\u001b[39m permutation[n_test : (n_test \u001b[39m+\u001b[39m n_train)]\n\u001b[0;32m-> 3665\u001b[0m train_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect(\n\u001b[1;32m   3666\u001b[0m     indices\u001b[39m=\u001b[39;49mtrain_indices,\n\u001b[1;32m   3667\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   3668\u001b[0m     indices_cache_file_name\u001b[39m=\u001b[39;49mtrain_indices_cache_file_name,\n\u001b[1;32m   3669\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   3670\u001b[0m     new_fingerprint\u001b[39m=\u001b[39;49mtrain_new_fingerprint,\n\u001b[1;32m   3671\u001b[0m )\n\u001b[1;32m   3672\u001b[0m test_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect(\n\u001b[1;32m   3673\u001b[0m     indices\u001b[39m=\u001b[39mtest_indices,\n\u001b[1;32m   3674\u001b[0m     keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3677\u001b[0m     new_fingerprint\u001b[39m=\u001b[39mtest_new_fingerprint,\n\u001b[1;32m   3678\u001b[0m )\n\u001b[1;32m   3680\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict({\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: train_split, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: test_split})\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    512\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    513\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    514\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    515\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    516\u001b[0m }\n\u001b[1;32m    517\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    519\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    520\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/arrow_dataset.py:3059\u001b[0m, in \u001b[0;36mDataset.select\u001b[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   3056\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_contiguous(start, length, new_fingerprint\u001b[39m=\u001b[39mnew_fingerprint)\n\u001b[1;32m   3058\u001b[0m \u001b[39m# If not contiguous, we need to create a new indices mapping\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_with_indices_mapping(\n\u001b[1;32m   3060\u001b[0m     indices,\n\u001b[1;32m   3061\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   3062\u001b[0m     indices_cache_file_name\u001b[39m=\u001b[39;49mindices_cache_file_name,\n\u001b[1;32m   3063\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   3064\u001b[0m     new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   3065\u001b[0m )\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    512\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    513\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    514\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    515\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    516\u001b[0m }\n\u001b[1;32m    517\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    519\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    520\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/datasets/arrow_dataset.py:3181\u001b[0m, in \u001b[0;36mDataset._select_with_indices_mapping\u001b[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   3179\u001b[0m     buf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCaching indices mapping at \u001b[39m\u001b[39m{\u001b[39;00mindices_cache_file_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 3181\u001b[0m     tmp_file \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39;49mNamedTemporaryFile(\u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(indices_cache_file_name), delete\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   3182\u001b[0m     writer \u001b[39m=\u001b[39m ArrowWriter(\n\u001b[1;32m   3183\u001b[0m         path\u001b[39m=\u001b[39mtmp_file\u001b[39m.\u001b[39mname, writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size, fingerprint\u001b[39m=\u001b[39mnew_fingerprint, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3184\u001b[0m     )\n\u001b[1;32m   3186\u001b[0m indices \u001b[39m=\u001b[39m indices \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(indices, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(indices)\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/tempfile.py:545\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m _os\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnt\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m delete:\n\u001b[1;32m    543\u001b[0m     flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39mO_TEMPORARY\n\u001b[0;32m--> 545\u001b[0m (fd, name) \u001b[39m=\u001b[39m _mkstemp_inner(\u001b[39mdir\u001b[39;49m, prefix, suffix, flags, output_type)\n\u001b[1;32m    546\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     file \u001b[39m=\u001b[39m _io\u001b[39m.\u001b[39mopen(fd, mode, buffering\u001b[39m=\u001b[39mbuffering,\n\u001b[1;32m    548\u001b[0m                     newline\u001b[39m=\u001b[39mnewline, encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/tempfile.py:255\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[1;32m    253\u001b[0m _sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mtempfile.mkstemp\u001b[39m\u001b[39m\"\u001b[39m, file)\n\u001b[1;32m    254\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     fd \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39;49mopen(file, flags, \u001b[39m0o600\u001b[39;49m)\n\u001b[1;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[39mcontinue\u001b[39;00m    \u001b[39m# try again\u001b[39;00m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/data/users/team2_capstone/code-style-probing/datasets/combined_model_nl_prompt/codet5_train_case_bq_padded.hf/tmppjrqwfcg'"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-base\", additional_special_tokens= [\"<nl>\", \"</nl>\"])\n",
    "\n",
    "#train_codet5_dataset = load_from_disk(fname_prefix + 'datasets/combined_model/codet5_train_combined_base.hf')\n",
    "test_codet5_dataset = load_from_disk(fname_prefix + 'datasets/combined_model_nl_prompt/codet5_test_case_bq_padded.hf')\n",
    "\n",
    "#debugging_dataset = load_from_disk(fname_prefix + 'datasets/combined_model/codet5_train_comp_bq_padded.hf')\n",
    "\n",
    "train_comp_dataset = load_from_disk(fname_prefix + 'datasets/combined_model_nl_prompt/codet5_train_comp_bq_padded.hf')\n",
    "train_casing_dataset = load_from_disk(fname_prefix + 'datasets/combined_model_nl_prompt/codet5_train_case_bq_padded.hf', ).train_test_split(test_size = 0.5)[\"train\"]\n",
    "train_docstring_dataset = load_from_disk(fname_prefix + 'datasets/combined_model_nl_prompt/codet5_train_docstring_fixed_bq_padded.hf').train_test_split(test_size = 0.9)[\"train\"]\n",
    "#split for \n",
    "train_comment_dataset = load_from_disk(fname_prefix + 'datasets/combined_model_nl_prompt/codet5_train_comment_bq_padded.hf').train_test_split(test_size = 0.5)[\"train\"]\n",
    "train_class_dataset = load_from_disk(fname_prefix + 'datasets/combined_model_nl_prompt/codet5_train_class_bq_padded.hf').train_test_split(test_size = 0.5)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import debug\n",
    "\n",
    "\n",
    "test_codet5_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_codet5_dataset = {\n",
    "    #'comp' : debugging_dataset,\n",
    "    \"comp\" : train_comp_dataset,\n",
    "    \"case\" : train_casing_dataset,\n",
    "    \"class\" : train_class_dataset,\n",
    "    \"docstring\" : train_docstring_dataset,\n",
    "    \"comment\" : train_comment_dataset,\n",
    "}\n",
    "\n",
    "for dataset in train_codet5_dataset:\n",
    "    train_codet5_dataset[dataset].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "#np.asarray(train_comp_dataset[1]['labels']).dtype\n",
    "#print (train_comp_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for input in train_codet5_dataset['comp'][\"input_ids\"]:\n",
    "    if len(input) > 512:\n",
    "        #print ((input[\"input_ids\"]))\n",
    "        #idx = (input[\"input_ids\"].index(2))\n",
    "        #print (idx)\n",
    "        #print (len(input['attention_mask']))\n",
    "        break\n",
    "        count +=1\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataCollator: #(DataCollator):\n",
    "    \"\"\"\n",
    "    Extending the existing DataCollator to work with NLP dataset batches\n",
    "    \"\"\"\n",
    "    def __call__(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "    #def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        first = features[0]\n",
    "        #print (first)\n",
    "        if isinstance(first, dict):\n",
    "          # NLP data sets current works presents features as lists of dictionary\n",
    "          # (one per example), so we  will adapt the collate_batch logic for that\n",
    "          if \"labels\" in first and first[\"labels\"] is not None:\n",
    "              if np.asarray(first[\"labels\"]).dtype == torch.int64:\n",
    "                  labels = [f[\"labels\"] for f in features]\n",
    "              else:\n",
    "                  labels = [f[\"labels\"] for f in features]\n",
    "              batch = {\"labels\": torch.stack(labels)}\n",
    "              #print (batch)\n",
    "          for k, v in first.items():\n",
    "              if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                  batch[k] = torch.stack([f[k] for f in features])\n",
    "          return batch\n",
    "        else:\n",
    "          # otherwise, revert to using the default collate_batch\n",
    "          return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    \"\"\"\n",
    "    Wrapper around a DataLoader to also yield a task name\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset) \n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name])    \n",
    "\n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        \"\"\"\n",
    "        Create a single-task data loader that also yields task names\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        #if is_tpu_available():\n",
    "        #    train_sampler = get_tpu_sampler(train_dataset)\n",
    "        #else:\n",
    "        train_sampler = (\n",
    "            RandomSampler(train_dataset)\n",
    "            if self.args.local_rank == -1\n",
    "            else DistributedSampler(train_dataset)\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "              train_dataset,\n",
    "              batch_size=self.args.train_batch_size,\n",
    "              sampler=train_sampler,\n",
    "              collate_fn=self.data_collator.__call__,#collate_batch,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        #if is_tpu_available():\n",
    "        #    data_loader = pl.ParallelLoader(\n",
    "        #        data_loader, [self.args.device]\n",
    "        #    ).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each \n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "            for task_name, task_dataset in self.train_dataset.items()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskModel(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        \"\"\"\n",
    "        Setting MultitaskModel up as a PretrainedModel allows us\n",
    "        to take better advantage of Trainer features\n",
    "        \"\"\"\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "\n",
    "        #self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_config_dict):\n",
    "        \"\"\"\n",
    "        #This creates a MultitaskModel using the model class and config objects\n",
    "        #from single-task models. \n",
    "\n",
    "        #We do this by creating each single-task model, and having them share\n",
    "        #the same encoder transformer.\n",
    "        \"\"\"\n",
    "        shared_encoder = None\n",
    "        # this could be configured to produce different structured models but for now is one unmodified T5\n",
    "        taskmodels_dict = {}\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name, \n",
    "                config=model_config_dict[task_name],\n",
    "            )\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "\n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict['model'](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Salesforce/codet5-small/resolve/main/config.json from cache at /soe/ksmunson/.cache/huggingface/transformers/ef13e715cbf36adda46c74774e8032ab573cfbb2ebe59748c9fc72b7cf67e418.96d28e790b8c3d3e3be663606a66e0793a173c78e745e3603be4c0f878319099\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Salesforce/codet5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Salesforce/codet5-small/resolve/main/pytorch_model.bin from cache at /soe/ksmunson/.cache/huggingface/transformers/72f18276a84809c5d2071dd664611f32bf6732788714ff4669de3b2d7becf4e8.f77f3daae184f7661a8837b1e043ddb86c279a9a71f866e7d286f68244bca50e\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Salesforce/codet5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#model config to factor for task name\n",
    "#model =AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\")\n",
    "\n",
    "model_name = \"Salesforce/codet5-small\"\n",
    "multitask_model = MultitaskModel.create(\n",
    "    model_name=model_name,\n",
    "    model_type_dict={\n",
    "        #\"stsb\": transformers.AutoModelForSequenceClassification,\n",
    "        #\"rte\": transformers.AutoModelForSequenceClassification,\n",
    "        #\"commonsense_qa\": transformers.AutoModelForMultipleChoice,\n",
    "        \"model\" : AutoModelForSeq2SeqLM\n",
    "    },\n",
    "    model_config_dict={\n",
    "        #\"stsb\": transformers.AutoConfig.from_pretrained(model_name, num_labels=1),\n",
    "        #\"rte\": transformers.AutoConfig.from_pretrained(model_name, num_labels=2),\n",
    "        #\"commonsense_qa\": transformers.AutoConfig.from_pretrained(model_name),\n",
    "        \"model\" : transformers.AutoConfig.from_pretrained(model_name),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24 # switch to 24 once space is available\n",
    "from transformers import AutoConfig\n",
    "\n",
    "#combined checkpoint\n",
    "#model =AutoModelForSeq2SeqLM.from_pretrained(fname_prefix + \"/seq2seq_results/combined_full_features_updated_mask_codet5small/checkpoint-97000\")\n",
    "\n",
    "\n",
    "multitask_model.config.max_length = 512\n",
    "#multitask_model.resize_token_embeddings(len(tokenizer))\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir= fname_prefix + \"/seq2seq_results/combined_nl_prompt_base_features_codet5small\",\n",
    "    evaluation_strategy='epoch',\n",
    "    #eval_steps=20000,\n",
    "    eval_accumulation_steps=200,\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=30,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60493312\n"
     ]
    }
   ],
   "source": [
    "print (multitask_model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4081682\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1020429\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='221' max='1020429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    221/1020429 01:22 < 107:07:03, 2.65 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m MultitaskTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     multitask_model,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mNLPDataCollator(),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnlp-gpu-01.soe.ucsc.edu/data/users/ksmunson/code-style-probing/combined_seq2seq_trainer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1414\u001b[0m )\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/transformers/trainer.py:1651\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1649\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1651\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1653\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1655\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1656\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1657\u001b[0m ):\n\u001b[1;32m   1658\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/transformers/trainer.py:2363\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2363\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2365\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/data/users/ksmunson/data/users/ksmunson/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = MultitaskTrainer(\n",
    "    multitask_model,\n",
    "    args,\n",
    "    train_dataset=train_codet5_dataset,\n",
    "    eval_dataset=test_codet5_dataset,\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=NLPDataCollator(),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = test_codet5_dataset\n",
    "eval_dataset = eval_dataset.train_test_split(test_size = 0.05)\n",
    "print (eval_dataset)\n",
    "eval_preds = trainer.predict(eval_dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "988cc1e00d3b5bb0a2e9024406047781d3e298e90a30d1fcc633613d0d680479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
