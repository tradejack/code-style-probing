{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /data/users/cting3/miniconda3/envs/py_3_8/lib/python3.8/site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration,AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "from threading import Lock\n",
    "from graphite.render.grammar_unsafe import grammar as _grammar\n",
    "\n",
    "\n",
    "def init(self):\n",
    "    self._lock = lock()\n",
    "\n",
    "def parsestring(self, instring):\n",
    "    with self._lock:\n",
    "        return _grammar.parseString(instring)\n",
    "grammar = threadsafegrammar()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/combined_nl_prompt_base_features_codet5small/epoch 2/checkpoint-85000\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = tokenizer(text,return_tensors = \"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_toks = tokenizer(\"<nl> add docstring and change casing </nl>\",add_special_tokens=False)\n",
    "# print(control_toks)\n",
    "# input_toks = control_toks[\"input_ids\"] + list(input_ids[0].numpy())\n",
    "# generated_ids = model.generate(torch.tensor([input_toks],dtype = torch.long))\n",
    "# print(tokenizer.decode(generated_ids[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_size = 512\n",
    "\n",
    "# def add_tokens(example, control_toks):\n",
    "#     # takes a tokenized data set and adds the control sequence to the code\n",
    "#     #print(\"Example input id is :\",example)\n",
    "#     input = example[\"input_ids\"].copy()\n",
    "#     mask = example[\"attention_mask\"].copy()\n",
    "\n",
    "#     idx_last = int(input.index(2))\n",
    "\n",
    "#     control_toks_len = len(control_toks[\"input_ids\"])\n",
    "#     free_space = max_size - idx_last - 1\n",
    "\n",
    "#     true_seq = input[: idx_last + 1]\n",
    "#     if free_space >= control_toks_len:\n",
    "#         input[:control_toks_len] = control_toks[\"input_ids\"]\n",
    "#         input[control_toks_len : control_toks_len + len(true_seq)] = true_seq\n",
    "#         mask = np.array(mask)\n",
    "#         mask[: control_toks_len + idx_last] = 1\n",
    "#     else:\n",
    "#         input[:control_toks_len] = control_toks[\"input_ids\"]\n",
    "#         fit_len = max_size - control_toks_len - 1\n",
    "#         input[control_toks_len:] = true_seq[:fit_len] + [2]\n",
    "#         mask = np.array(mask)\n",
    "#         mask[:] = 1\n",
    "\n",
    "#     example[\"attention_mask\"] = list(mask)\n",
    "#     example[\"input_ids\"] = input\n",
    "#     return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promptValue =  \"change indentifier casing and add class\"\n",
    "# input_toks =  tokenizer(text,max_length=1024, padding=\"max_length\", return_tensors=\"pt\")\n",
    "# prompt_toks = tokenizer(f\"<nl> {promptValue} </nl>\", add_special_tokens=False)\n",
    "\n",
    "# input_ids = add_tokens(input_toks,prompt_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks_with_prompt(text, prompt, tokenizer):\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    control_toks = tokenizer(f\"<nl>{prompt}</nl>\", add_special_tokens=False)\n",
    "    input_toks = control_toks[\"input_ids\"] + input_ids\n",
    "    return torch.tensor([input_toks], dtype=torch.long)\n",
    "input_tokens = toks_with_prompt(text, \"class and casing\", tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gemerated_ids = model.generate(input_tokens,dtype=torch.long)\n",
    "#generated_ids = model.generate(torch.tensor(input_tokens),dtype=torch.long)\n",
    "generated_ids = model.generate(input_tokens)\n",
    "\n",
    "#generated_ids = model.generate(torch.tensor([input_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from threading import Lock\n",
      "from graphite.render.grammar_unsafe import grammar as _grammar\n",
      "\n",
      "def init(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString(self):\n",
      "    self._lock = Lock()\n",
      "\n",
      "def parseString(self, inString):\n",
      "    with self._lock:\n",
      "        return _grammar.parseString(inString)\n",
      "grammar = _grammar_grammar()\n",
      "\n",
      "def parseString\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver 2\n",
    "def control_nl_toks_add(input_ids, prompt):\n",
    "    # takes a tokenized data set and adds the control sequence to the code\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    control_toks = tokenizer(f\"<nl>{prompt}</nl>\", add_special_tokens=False)\n",
    "\n",
    "    idx_last = int(input_ids.index(2))\n",
    "    control_toks_len = len(control_toks[\"input_ids\"])\n",
    "    free_space = max_size - idx_last - 1\n",
    "\n",
    "    true_seq = input_ids[: idx_last + 1]\n",
    "    if free_space >= control_toks_len:\n",
    "        input_ids[:control_toks_len] = control_toks[\"input_ids\"]\n",
    "        input_ids[control_toks_len : control_toks_len + len(true_seq)] = true_seq\n",
    "    else:\n",
    "        input_ids[:control_toks_len] = control_toks[\"input_ids\"]\n",
    "        fit_len = max_size - control_toks_len - 1\n",
    "        input_ids[control_toks_len:] = true_seq[:fit_len] + [2]\n",
    "\n",
    "    return torch.tensor([input_ids], dtype=torch.long)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52bb47d3ea77865c3131cc1e8c4a6a32dab9d45644b42f43838103ba8502951c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "52bb47d3ea77865c3131cc1e8c4a6a32dab9d45644b42f43838103ba8502951c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
