{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install ipywidgets\n",
    "#!pip install sentencepiece\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, Label\n",
    "from ipywidgets import Layout\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import T5ForConditionalGeneration,AutoModelForSeq2SeqLM,AutoTokenizer,RobertaTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def toks_with_prompt(text, prompt, tokenizer):\n",
    "#     input_ids = tokenizer(text,max_length=512, padding=\"max_length\").input_ids\n",
    "#     control_toks = tokenizer(f\"<nl>{prompt}</nl>\", add_special_tokens=False)\n",
    "#     input_toks = control_toks[\"input_ids\"] + input_ids\n",
    "#     return torch.tensor([input_toks], dtype=torch.long)\n",
    "\n",
    "max_size=512\n",
    "def toks_with_prompt(text, prompt,tokenizer):\n",
    "    # takes a tokenized data set and adds the control sequence to the code\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    control_toks = tokenizer(f\"<nl>{prompt}</nl>\", add_special_tokens=False)\n",
    "    idx_last = int(input_ids.index(2))\n",
    "    control_toks_len = len(control_toks[\"input_ids\"])\n",
    "    free_space = max_size - idx_last - 1\n",
    "\n",
    "    true_seq = input_ids[: idx_last + 1]\n",
    "    if free_space >= control_toks_len:\n",
    "        input_ids[:control_toks_len] = control_toks[\"input_ids\"]\n",
    "        input_ids[control_toks_len : control_toks_len + len(true_seq)] = true_seq\n",
    "    else:\n",
    "        input_ids[:control_toks_len] = control_toks[\"input_ids\"]\n",
    "        fit_len = max_size - control_toks_len - 1\n",
    "        input_ids[control_toks_len:] = true_seq[:fit_len] + [2]\n",
    "\n",
    "    return torch.tensor([input_ids], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(input_code,featureName,promptValue):\n",
    "    \n",
    "    if featureName.lower() == \"casing\":\n",
    "        checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/outlier_casing_codet5small/checkpoint-27000\"\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        input_ids =  tokenizer([input_code],max_length=512, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids)\n",
    "        return tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    elif featureName.lower() == \"comments\":\n",
    "        checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/outlier_codet5small/checkpoint-40500\"\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        input_ids =  tokenizer([input_code],max_length=512, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids)\n",
    "        return tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    elif featureName.lower() == \"class\":\n",
    "        checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/outlier_class_codet5small/checkpoint-49000\"\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        input_ids =  tokenizer([input_code],max_length=1024, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids)\n",
    "        return tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    elif featureName.lower() == \"docstring\":\n",
    "        checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/outlier_updated_docstring_codet5small/checkpoint-85500\"\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        input_ids =  tokenizer([input_code],max_length=512, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids)\n",
    "        return tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    elif featureName.lower() == \"list comprehensions\":\n",
    "        checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/outlier_comp_codet5small/checkpoint-9500\"\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        input_ids =  tokenizer([input_code],max_length=512, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "        generated_ids = model.generate(input_ids)\n",
    "        return tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    else:\n",
    "        \n",
    "        checkpoint = \"/data/users/team2_capstone/code-style-probing/seq2seq_results/combined_nl_prompt_base_features_contd_codet5small/checkpoint-144856\"        \n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        #tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "        input_tokens = toks_with_prompt(input_code, promptValue, tokenizer)\n",
    "        generated_ids = model.generate(input_tokens)\n",
    "        return tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Run Inference!\")\n",
    "output = widgets.Output(layout={'border': '1px solid black'})\n",
    "feature_selection = widgets.Dropdown(\n",
    "    options=['Casing', 'Comments', 'Class','Docstring','List Comprehensions','Combined'],\n",
    "    description='Features : ',\n",
    "    disabled=False,\n",
    ")\n",
    "l = Layout(flex='0 1 auto', height='250px', min_height='40px', width='550px')\n",
    "inputText = widgets.Textarea(\n",
    "    layout = l,\n",
    "    placeholder='Type something',\n",
    "    description='Input Code:',\n",
    "    disabled=False,\n",
    "    font_size = \"18px\"\n",
    ")\n",
    "\n",
    "promptText = widgets.Textarea(\n",
    "    #layout = l,\n",
    "    placeholder='Type something',\n",
    "    description='Prompt:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simple_colors\n",
    "import difflib\n",
    "from simple_colors import *\n",
    "red = lambda text: f\"\\033[38;2;255;0;0m{text}\\033[38;2;255;255;255m\"\n",
    "green = lambda text: f\"\\033[38;2;0;150;0m{text}\\033[38;2;255;255;255m\"\n",
    "blue = lambda text: f\"\\033[38;2;0;0;255m{text}\\033[38;2;255;255;255m\"\n",
    "white = lambda text: f\"\\033[38;2;0;0;0m{text}\\033[38;2;0;0;0m\"\n",
    "\n",
    "def get_edits_string(old, new):\n",
    "    result = \"\"\n",
    "    codes = difflib.SequenceMatcher(a=old, b=new).get_opcodes()\n",
    "    for code in codes:\n",
    "        if code[0] == \"equal\": \n",
    "            result += white(old[code[1]:code[2]])\n",
    "        elif code[0] == \"delete\":\n",
    "            result += red(old[code[1]:code[2]])\n",
    "        elif code[0] == \"insert\":\n",
    "            result += green(new[code[3]:code[4]])\n",
    "        elif code[0] == \"replace\":\n",
    "            result += (red(old[code[1]:code[2]]) + green(new[code[3]:code[4]]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a558c06b2bb6462f8fb85d85ecb716bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Features : ', options=('Casing', 'Comments', 'Class', 'Docstring', 'List Comprehensions'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cf05a005054faf875919765d3a73d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Prompt:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e998e54a394d5f99c7711729da03b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Input Code:', layout=Layout(flex='0 1 auto', height='250px', min_height='40px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280894520a3845c2828e828938129718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Inference!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e16e719ff64cab9fb85b2eb5218445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "\n",
    "display(feature_selection)\n",
    "display(promptText)\n",
    "display(inputText)\n",
    "display(button,output)\n",
    "\n",
    "@output.capture(clear_output=True)\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output_code = do_inference(str(inputText.value),feature_selection.value,promptText.value)\n",
    "        final_code=get_edits_string(inputText.value, output_code[0])\n",
    "        print(final_code)\n",
    "        #print(output_code[0])\n",
    "clickValue = button.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "56589f69b59c5a4ea14e4eb4ead3f55f549610d467a3277dc296368c285439c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
