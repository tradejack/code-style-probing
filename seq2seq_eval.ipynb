{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004a1eec-f018-476f-ad21-bd9d34a7fe00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: typer in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: transformers in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.22.2)\n",
      "Requirement already satisfied: datasets in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.5.1)\n",
      "Requirement already satisfied: astunparse in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.1.97)\n",
      "Requirement already satisfied: sklearn in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.0)\n",
      "Requirement already satisfied: tree_sitter in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.20.1)\n",
      "Requirement already satisfied: jupyterlab in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (3.4.7)\n",
      "Requirement already satisfied: ipywidgets in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (8.0.2)\n",
      "Requirement already satisfied: termcolor in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.0.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (1.23.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from typer->-r requirements.txt (line 2)) (8.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: filelock in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (3.8.0)\n",
      "Requirement already satisfied: requests in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 4)) (2022.8.2)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from astunparse->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from astunparse->-r requirements.txt (line 5)) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from sklearn->-r requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: tomli in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (2.0.1)\n",
      "Requirement already satisfied: ipython in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (8.5.0)\n",
      "Requirement already satisfied: jupyterlab-server~=2.10 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (2.15.2)\n",
      "Requirement already satisfied: tornado>=6.1.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (6.2)\n",
      "Requirement already satisfied: notebook<7 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (6.4.12)\n",
      "Requirement already satisfied: jupyter-core in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (4.11.1)\n",
      "Requirement already satisfied: jinja2>=2.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: jupyter-server~=1.16 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (1.19.1)\n",
      "Requirement already satisfied: nbclassic in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 9)) (0.4.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 10)) (6.16.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 10)) (3.0.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 10)) (5.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 10)) (4.0.3)\n",
      "Requirement already satisfied: joblib in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (22.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers->-r requirements.txt (line 3)) (4.3.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (7.3.5)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (24.0.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (1.5.6)\n",
      "Requirement already satisfied: psutil in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (5.9.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (4.8.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (2.13.0)\n",
      "Requirement already satisfied: stack-data in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (0.5.1)\n",
      "Requirement already satisfied: decorator in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (3.0.31)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from ipython->jupyterlab->-r requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 9)) (2.1.1)\n",
      "Requirement already satisfied: argon2-cffi in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (21.3.0)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (7.0.0)\n",
      "Requirement already satisfied: Send2Trash in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (3.6.1)\n",
      "Requirement already satisfied: nbformat>=5.2.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (5.6.1)\n",
      "Requirement already satisfied: prometheus-client in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (0.14.1)\n",
      "Requirement already satisfied: websocket-client in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 9)) (4.12.0)\n",
      "Requirement already satisfied: json5 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 9)) (0.9.10)\n",
      "Requirement already satisfied: babel in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 9)) (2.10.3)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 9)) (4.16.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbclassic->jupyterlab->-r requirements.txt (line 9)) (0.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 7)) (1.9.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from importlib-metadata>=3.6->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jedi>=0.16->ipython->jupyterlab->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: entrypoints in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (0.4)\n",
      "Requirement already satisfied: tinycss2 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: lxml in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (4.11.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (2.0.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (1.5.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (0.6.8)\n",
      "Requirement already satisfied: bleach in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (5.0.1)\n",
      "Requirement already satisfied: defusedxml in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (0.2.2)\n",
      "Requirement already satisfied: fastjsonschema in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from nbformat>=5.2.0->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (2.16.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from pexpect>4.3->ipython->jupyterlab->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython->jupyterlab->-r requirements.txt (line 9)) (0.2.5)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from argon2-cffi->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (21.2.0)\n",
      "Requirement already satisfied: pure-eval in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 9)) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 9)) (2.0.8)\n",
      "Requirement already satisfied: executing in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /home/ken/miniconda3/envs/code/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 9)) (2.21)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e2aa69-1a04-4aee-9d76-fc97ab27ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval on predictions\n",
    "import ast\n",
    "import re\n",
    "import difflib\n",
    "from termcolor import colored\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from evaluator.CodeBLEU.calc_code_bleu import get_codebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from utils.regex_parse import comment\n",
    "from utils.eval_utils import evaluate_codebleu, evaluate_pred_df, lookup_examples, get_valid_pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c655bb7a-0dd3-42d9-a833-f7ebd59d1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_same_io(df):\n",
    "    # excluding those input exactly same as the output\n",
    "    exact_match_bool = df[\"inputs\"] == df[\"labels\"]\n",
    "    df = df.drop(df[exact_match_bool].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e1c863-7d5b-4e1a-b1cf-611a53ba60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsable eval\n",
    "def is_parsable(input_code):\n",
    "    try:\n",
    "        ast.parse(input_code)\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(input_code)\n",
    "        print(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f2ecf06-5eb4-4209-968e-63272ef6442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_codebleu(pred_filename, weights=\"0.25,0.25,0.25,0.25\", replaced_df=None, dropna=False, is_exclude_same_io=False):\n",
    "    pred_df = None\n",
    "    if replaced_df is not None:\n",
    "        pred_df = replaced_df\n",
    "    else:\n",
    "        pred_df = pd.read_csv(pred_filename)\n",
    "    if dropna:\n",
    "        pred_df = pred_df.dropna()\n",
    "    if is_exclude_same_io:\n",
    "        pred_df = exclude_same_io(pred_df)\n",
    "    # a list of gold codes (which is just some variants of the same code, we can use every code of different styles)\n",
    "    refs = [\n",
    "        pred_df[\"labels\"]\n",
    "    ]\n",
    "    # the prediction code\n",
    "    hyp = pred_df[\"preds\"]\n",
    "    score = get_codebleu(refs, hyp, \"python\", weights)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca3538b7-23d1-4e5e-b69a-a8da14ec89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docstring(text):\n",
    "    regex_docstr = \"^\\s*\\'{3}([\\s\\S]*?)\\'{3}|^\\s*\\\"{3}([\\s\\S]*?)\\\"{3}\"\n",
    "    docstr_matches = re.findall(regex_docstr, text, re.M | re.S)\n",
    "    docstrs = []\n",
    "    for match in docstr_matches:\n",
    "        docstr_a, docstr_b = match\n",
    "        if docstr_a:\n",
    "            docstrs += [docstr_a]\n",
    "        else:\n",
    "            docstrs += [docstr_b]\n",
    "    return docstrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbec1b7a-63ba-4554-9702-3377247ddd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_split_line(s):\n",
    "    print(f\"\\n====================={s.upper()}=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7ce0d9f-3896-4d63-88de-0b8447043335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return re.split('\\s+', s)\n",
    "\n",
    "def get_diff_list(str_1, str_2):\n",
    "    s1 = tokenize(str_1)\n",
    "    s2 = tokenize(str_2)\n",
    "\n",
    "    matcher = difflib.SequenceMatcher(a=s1, b=s2)\n",
    "\n",
    "    diff_blocks_a = []\n",
    "    diff_blocks_b = []\n",
    "\n",
    "    prev_match = None\n",
    "    for idx, match in enumerate(matcher.get_matching_blocks()):\n",
    "\n",
    "        if idx == 0: \n",
    "            prev_match = match\n",
    "            if match.a != 0:\n",
    "                start_idx_a = 0\n",
    "                end_idx_a = match.a\n",
    "                diff_blocks_a += s1[start_idx_a:end_idx_a]\n",
    "            if match.b != 0:\n",
    "                start_idx_b = 0\n",
    "                end_idx_b = match.b\n",
    "                diff_blocks_b += s2[start_idx_b:end_idx_b]\n",
    "            continue\n",
    "\n",
    "        start_idx_a = prev_match.a + prev_match.size\n",
    "        end_idx_a = match.a\n",
    "\n",
    "        start_idx_b = prev_match.b + prev_match.size\n",
    "        end_idx_b = match.b\n",
    "\n",
    "        diff_list_a = s1[start_idx_a:end_idx_a]\n",
    "        diff_list_b = s2[start_idx_b:end_idx_b]\n",
    "        if len(diff_list_a):\n",
    "            diff_blocks_a += diff_list_a\n",
    "        if len(diff_list_b):\n",
    "            diff_blocks_b += diff_list_b\n",
    "\n",
    "        prev_match = match\n",
    "    return diff_blocks_a, diff_blocks_b\n",
    "\n",
    "def get_diff_str(input_str, output_str):\n",
    "    return \" \".join(get_diff_list(input_str, output_str)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c66bb73-65f0-4d8b-bc0f-987430ac1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_nl_prompt(script):\n",
    "    return re.sub(\"<nl>.*<\\/nl>\", \"\", script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2b2a4fe-1680-43af-bbdd-f9fb131e38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pred_df(pred_df, target_feats, is_nl=False, parse_test=True):\n",
    "    \n",
    "    inputs = pred_df[\"inputs\"].to_numpy()\n",
    "    labels = pred_df[\"labels\"].to_numpy()\n",
    "    preds = pred_df[\"preds\"].to_numpy()\n",
    "    \n",
    "    if is_nl:\n",
    "        inputs = [remove_nl_prompt(input_script) for input_script in inputs]\n",
    "    \n",
    "    code_scores = []\n",
    "    diff_bleu_scores = []\n",
    "    \n",
    "    # if comment, need to extract comment\n",
    "    gold_comments = []\n",
    "    pred_comments = []\n",
    "    comment_text_scores = []\n",
    "    \n",
    "    # if docstring, need to extract docstring\n",
    "    gold_docstrings = []\n",
    "    pred_docstrings = []\n",
    "    docstr_text_scores = []\n",
    "    \n",
    "    # if parse test\n",
    "    is_parsables = []\n",
    "    \n",
    "    pred_diffs = []\n",
    "    gold_diffs = []\n",
    "    \n",
    "    total_len = preds.shape[0]\n",
    "    \n",
    "    for idx in tqdm(range(total_len)):\n",
    "        input_code = inputs[idx]\n",
    "        gold = labels[idx]\n",
    "        pred = preds[idx]\n",
    "        \n",
    "        refs = [[gold]]\n",
    "        hyp = [pred]\n",
    "        \n",
    "        # get code bleu score\n",
    "        code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "        \n",
    "        if \"docstring\" in target_feats:\n",
    "            gold_docstr = get_docstring(gold)\n",
    "            pred_docstr = get_docstring(pred)\n",
    "            gold_docstr_text = \"\\n\".join(gold_docstr)\n",
    "            pred_docstr_text = \"\\n\".join(pred_docstr)\n",
    "            docstr_text_score = 0\n",
    "            if len(pred_docstr_text.split()) > 0:\n",
    "                docstr_text_score = sentence_bleu([gold_docstr_text.split()], pred_docstr_text.split(), auto_reweigh=True)\n",
    "            \n",
    "            gold_docstrings += [gold_docstr]\n",
    "            pred_docstrings += [pred_docstr]\n",
    "            docstr_text_scores += [docstr_text_score]\n",
    "            \n",
    "        if \"comment\" in target_feats:\n",
    "            gold_comment = comment(gold)\n",
    "            pred_comment = comment(pred)\n",
    "            gold_comment_text = \"\\n\".join(gold_comment)\n",
    "            pred_comment_text = \"\\n\".join(pred_comment)\n",
    "            comment_text_score = 0\n",
    "            if len(pred_comment_text.split()) > 0:\n",
    "                comment_text_score = sentence_bleu([gold_comment_text.split()], pred_comment_text.split(), auto_reweigh=True)\n",
    "            \n",
    "            gold_comments += [gold_comment]\n",
    "            pred_comments += [pred_comment]\n",
    "            comment_text_scores += [comment_text_score]\n",
    "    \n",
    "        # get the diff bleu score\n",
    "        gold_diff_str = get_diff_str(input_code, gold)\n",
    "        pred_diff_str = get_diff_str(input_code, pred)\n",
    "        \n",
    "        pred_diffs += [pred_diff_str]\n",
    "        gold_diffs += [gold_diff_str]\n",
    "        \n",
    "        diff_bleu_score = 0\n",
    "        if len(pred_diff_str.split()) > 0:\n",
    "            diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "        \n",
    "        code_scores += [code_score]\n",
    "        diff_bleu_scores += [diff_bleu_score]\n",
    "        if parse_test:\n",
    "            is_parsables += [is_parsable(pred)]\n",
    "        \n",
    "    \n",
    "    code_bleus = np.array([s[\"code_bleu\"] for s in code_scores])\n",
    "    \n",
    "    report = {\n",
    "        \"inputs\": inputs,\n",
    "        \"labels\": labels,\n",
    "        \"preds\": preds,\n",
    "        \"pred_diffs\": pred_diffs,\n",
    "        \"gold_diffs\": gold_diffs,\n",
    "        \"codebleu\": code_scores,\n",
    "        \"codebleu_perfect\": sum(code_bleus == 1) / total_len,\n",
    "        \"codebleu_above_90\": sum(code_bleus >= 0.9) / total_len,\n",
    "        \"diff_bleu\": diff_bleu_scores,\n",
    "        \"diff_bleu_avg\":  np.mean(diff_bleu_scores),\n",
    "        \"diff_bleu_perfect\": sum(np.array(diff_bleu_scores) == 1) / total_len,\n",
    "        \"diff_bleu_above_90\": sum(np.array(diff_bleu_scores) >= 0.9) / total_len,\n",
    "    }\n",
    "    \n",
    "    if \"docstring\" in target_feats:\n",
    "        report[\"gold_docstrings\"] = gold_docstrings\n",
    "        report[\"pred_docstrings\"] = pred_docstrings\n",
    "        report[\"docstr_text_scores\"] = docstr_text_scores\n",
    "        report[\"docstr_text_scores_avg\"] = np.array(docstr_text_scores).mean()\n",
    "        report[\"docstr_text_scores_perfect\"] = sum(np.array(docstr_text_scores) == 1) / total_len\n",
    "        report[\"docstr_text_scores_above_90\"] = sum(np.array(docstr_text_scores) >= 0.9) / total_len\n",
    "        \n",
    "        \n",
    "    if \"comment\" in target_feats:\n",
    "        report[\"gold_comments\"] = gold_comments\n",
    "        report[\"pred_comments\"] = pred_comments\n",
    "        report[\"comment_text_scores\"] = comment_text_scores\n",
    "        report[\"comment_text_scores_avg\"] = np.array(comment_text_scores).mean()\n",
    "        report[\"comment_text_scores_perfect\"] = sum(np.array(comment_text_scores) == 1) / total_len\n",
    "        report[\"comment_text_scores_above_90\"] = sum(np.array(comment_text_scores) >= 0.9) / total_len\n",
    "        \n",
    "    if parse_test:\n",
    "        report[\"parse_test_accuracy\"] = sum(np.array(is_parsables)) / total_len\n",
    "        \n",
    "    return report.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d740072f-3f95-4b4a-bf24-dbe794d7bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_colored_diff(str_1, str_2):\n",
    "    text_1 = \"\"\n",
    "    text_2 = \"\"\n",
    "    idx_1 = 0\n",
    "    idx_2 = 0\n",
    "    matcher = difflib.SequenceMatcher(a=str_1, b=str_2)\n",
    "    for match in matcher.get_matching_blocks():\n",
    "        diff_text_1 = \"\"\n",
    "        if idx_1 < match.a:\n",
    "            diff_text_1 += colored(str_1[idx_1:match.a], \"red\")\n",
    "\n",
    "\n",
    "        diff_text_2 = \"\"\n",
    "        if idx_2 < match.b:\n",
    "            diff_text_2 += colored(str_2[idx_2:match.b], \"red\")\n",
    "\n",
    "        match_text_1 = str_1[match.a:match.a+match.size]\n",
    "        match_text_2 = str_2[match.b:match.b+match.size]\n",
    "\n",
    "        idx_1 = match.a+match.size \n",
    "        idx_2 = match.b+match.size\n",
    "\n",
    "        text_1 += diff_text_1 + match_text_1\n",
    "        text_2 += diff_text_2 + match_text_2\n",
    "        \n",
    "    if idx_1 < len(str_1):\n",
    "        text_1 += colored(str_1[idx_1:], \"red\")\n",
    "        \n",
    "    if idx_2 < len(str_2):\n",
    "        text_2 += colored(str_2[idx_2:], \"red\")\n",
    "    return text_1, text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6826cf13-570e-489c-aa58-c3417d786bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_examples(report, score_upper_bound, score_lower_bound, metric=\"diff_bleu\", start_idx=0, count=10):\n",
    "    total = len(report[\"inputs\"])\n",
    "    if count == \"all\":\n",
    "        count = total\n",
    "    current_count = 0\n",
    "    for idx in range(total):\n",
    "        if current_count == count: break\n",
    "        if idx < start_idx: continue\n",
    "        \n",
    "        # checking upper bound\n",
    "        if report[metric][idx] > score_upper_bound: continue\n",
    "        # checking lower bound\n",
    "        if report[metric][idx] < score_lower_bound: continue\n",
    "        \n",
    "        input_code = report[\"inputs\"][idx]\n",
    "        pred_code = report[\"preds\"][idx]\n",
    "        gold_code = report[\"labels\"][idx]\n",
    "        \n",
    "        c_input, c_gold = print_colored_diff(input_code, gold_code)\n",
    "        _, c_pred = print_colored_diff(input_code, pred_code)\n",
    "        \n",
    "        print_split_line(f\"{idx}-input\")\n",
    "        print(c_input)\n",
    "        print_split_line(f\"{idx}-prediction\")\n",
    "        print(c_pred)\n",
    "        print_split_line(f\"{idx}-gold labels\")\n",
    "        print(c_gold)\n",
    "        print_split_line(f\"{idx}-{metric}\")\n",
    "        print(report[metric][idx])\n",
    "        \n",
    "        current_count += 1\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f77248-3072-40eb-bee8-9c6dda68163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data/ken/data/code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ec5b23-976b-4e4e-afc7-f157e037429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "codex_list_comp_df = pd.read_csv(f\"{DATA_DIR}/codex_output.csv\")\n",
    "codex_list_comp_df = exclude_same_io(codex_list_comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c3e86ca-0c05-482d-80e1-11e6be1ab1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>preds</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...</td>\n",
       "      <td>\\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...</td>\n",
       "      <td>\\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\\nimport re\\nfrom django import template\\nfrom...</td>\n",
       "      <td>\\nimport re\\nfrom django import template\\nfrom...</td>\n",
       "      <td>\\nimport re\\nfrom django import template\\nfrom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>\\nfrom __future__ import unicode_literals, pri...</td>\n",
       "      <td>\\nfrom __future__ import unicode_literals, pri...</td>\n",
       "      <td>\\nfrom __future__ import unicode_literals, pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>\\nfrom setuptools import setup, find_packages\\...</td>\n",
       "      <td>\\nfrom setuptools import setup, find_packages\\...</td>\n",
       "      <td>\\nfrom setuptools import setup, find_packages\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>\\nimport random\\nimport unittest\\nfrom algo.ch...</td>\n",
       "      <td>\\nimport random\\nimport unittest\\nfrom algo.ch...</td>\n",
       "      <td>\\nimport random\\nimport unittest\\nfrom algo.ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>\\nfrom CommonServerPython import *\\nimport jso...</td>\n",
       "      <td>\\nfrom CommonServerPython import *\\nimport jso...</td>\n",
       "      <td>\\nfrom CommonServerPython import *\\nimport jso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>\\nimport z3\\nfrom mythril.laser.smt.model impo...</td>\n",
       "      <td>\\nimport z3\\nfrom mythril.laser.smt.model impo...</td>\n",
       "      <td>\\nimport z3\\nfrom mythril.laser.smt.model impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>\\n'\\nCreated on Fri Mar 11 15:29:19 2016\\n\\n@a...</td>\n",
       "      <td>\\n'\\nCreated on Fri Mar 11 15:29:19 2016\\n\\n@a...</td>\n",
       "      <td>\\n'\\nCreated on Fri Mar 11 15:29:19 2016\\n\\n@a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>\\n'Helpers for training an agent using imitati...</td>\n",
       "      <td>\\n'Helpers for training an agent using imitati...</td>\n",
       "      <td>\\n'Helpers for training an agent using imitati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>\\nimport os\\nfrom setuptools import setup\\nROO...</td>\n",
       "      <td>\\nimport os\\nfrom setuptools import setup\\nROO...</td>\n",
       "      <td>\\nimport os\\nfrom setuptools import setup\\nROO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 inputs  \\\n",
       "0     \\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...   \n",
       "22    \\nimport re\\nfrom django import template\\nfrom...   \n",
       "49    \\nfrom __future__ import unicode_literals, pri...   \n",
       "76    \\nfrom setuptools import setup, find_packages\\...   \n",
       "86    \\nimport random\\nimport unittest\\nfrom algo.ch...   \n",
       "...                                                 ...   \n",
       "3886  \\nfrom CommonServerPython import *\\nimport jso...   \n",
       "3898  \\nimport z3\\nfrom mythril.laser.smt.model impo...   \n",
       "3909  \\n'\\nCreated on Fri Mar 11 15:29:19 2016\\n\\n@a...   \n",
       "3933  \\n'Helpers for training an agent using imitati...   \n",
       "3966  \\nimport os\\nfrom setuptools import setup\\nROO...   \n",
       "\n",
       "                                                  preds  \\\n",
       "0     \\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...   \n",
       "22    \\nimport re\\nfrom django import template\\nfrom...   \n",
       "49    \\nfrom __future__ import unicode_literals, pri...   \n",
       "76    \\nfrom setuptools import setup, find_packages\\...   \n",
       "86    \\nimport random\\nimport unittest\\nfrom algo.ch...   \n",
       "...                                                 ...   \n",
       "3886  \\nfrom CommonServerPython import *\\nimport jso...   \n",
       "3898  \\nimport z3\\nfrom mythril.laser.smt.model impo...   \n",
       "3909  \\n'\\nCreated on Fri Mar 11 15:29:19 2016\\n\\n@a...   \n",
       "3933  \\n'Helpers for training an agent using imitati...   \n",
       "3966  \\nimport os\\nfrom setuptools import setup\\nROO...   \n",
       "\n",
       "                                                 labels  \n",
       "0     \\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...  \n",
       "22    \\nimport re\\nfrom django import template\\nfrom...  \n",
       "49    \\nfrom __future__ import unicode_literals, pri...  \n",
       "76    \\nfrom setuptools import setup, find_packages\\...  \n",
       "86    \\nimport random\\nimport unittest\\nfrom algo.ch...  \n",
       "...                                                 ...  \n",
       "3886  \\nfrom CommonServerPython import *\\nimport jso...  \n",
       "3898  \\nimport z3\\nfrom mythril.laser.smt.model impo...  \n",
       "3909  \\n'\\nCreated on Fri Mar 11 15:29:19 2016\\n\\n@a...  \n",
       "3933  \\n'Helpers for training an agent using imitati...  \n",
       "3966  \\nimport os\\nfrom setuptools import setup\\nROO...  \n",
       "\n",
       "[248 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codex_list_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "487d1a14-8d8c-4c2b-be77-1f3d8f42d93b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a01707fae864c2fbca526f0bfaa7092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.0\n",
      "codebleu_above_90 : 0.12096774193548387\n",
      "diff_bleu_avg : 0.1124273545448584\n",
      "diff_bleu_perfect : 0.056451612903225805\n",
      "diff_bleu_above_90 : 0.056451612903225805\n",
      "parse_test_accuracy : 0.24596774193548387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_feats=['list_comp']\n",
    "codex_list_comp_report = evaluate_pred_df(codex_list_comp_df, target_feats, is_nl=False, parse_test=True)\n",
    "for key, val in codex_list_comp_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae92327-b8a5-4805-8fc4-b91f39fb9673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>preds</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...</td>\n",
       "      <td>\\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...</td>\n",
       "      <td>\\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n'security converge saved queries\\n\\nRevision...</td>\n",
       "      <td>\\n'security converge saved queries\\n\\nRevision...</td>\n",
       "      <td>\\n'security converge saved queries\\n\\nRevision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nfrom gng import GrowingNeuralGas\\nfrom sklea...</td>\n",
       "      <td>\\nfrom gng import GrowingNeuralGas\\nfrom sklea...</td>\n",
       "      <td>\\nfrom gng import GrowingNeuralGas\\nfrom sklea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n'\\nCreated on Mar 27, 2018\\n\\n@author: Starl...</td>\n",
       "      <td>\\n'\\nCreated on Mar 27, 2018\\n\\n@author: Starl...</td>\n",
       "      <td>\\n'\\nCreated on Mar 27, 2018\\n\\n@author: Starl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nimport os\\nimport tempfile\\nfrom os.path imp...</td>\n",
       "      <td>\\nimport os\\nimport tempfile\\nfrom os.path imp...</td>\n",
       "      <td>\\nimport os\\nimport tempfile\\nfrom os.path imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>\\nimport numpy as np\\nprint(np.empty(3))\\nprin...</td>\n",
       "      <td>\\nimport numpy as np\\nprint(np.empty(3))\\nprin...</td>\n",
       "      <td>\\nimport numpy as np\\nprint(np.empty(3))\\nprin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>\\nfrom math import cos, pi, sin\\nfrom PyQt5.Qt...</td>\n",
       "      <td>\\nfrom math import cos, pi, sin\\nfrom PyQt5.Qt...</td>\n",
       "      <td>\\nfrom math import cos, pi, sin\\nfrom PyQt5.Qt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>\\nimport numpy as np\\nimport pytest\\nfrom devi...</td>\n",
       "      <td>\\nimport numpy as np\\nimport pytest\\nfrom devi...</td>\n",
       "      <td>\\nimport numpy as np\\nimport pytest\\nfrom devi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>\\n'Run PEtab test suite (https://github.com/PE...</td>\n",
       "      <td>\\n'Run PEtab test suite (https://github.com/PE...</td>\n",
       "      <td>\\n'Run PEtab test suite (https://github.com/PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>\\nimport io\\nimport os\\nimport sys\\nfrom PIL i...</td>\n",
       "      <td>\\nimport io\\nimport os\\nimport sys\\nfrom PIL i...</td>\n",
       "      <td>\\nimport io\\nimport os\\nimport sys\\nfrom PIL i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 inputs  \\\n",
       "0     \\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...   \n",
       "1     \\n'security converge saved queries\\n\\nRevision...   \n",
       "2     \\nfrom gng import GrowingNeuralGas\\nfrom sklea...   \n",
       "3     \\n'\\nCreated on Mar 27, 2018\\n\\n@author: Starl...   \n",
       "4     \\nimport os\\nimport tempfile\\nfrom os.path imp...   \n",
       "...                                                 ...   \n",
       "3995  \\nimport numpy as np\\nprint(np.empty(3))\\nprin...   \n",
       "3996  \\nfrom math import cos, pi, sin\\nfrom PyQt5.Qt...   \n",
       "3997  \\nimport numpy as np\\nimport pytest\\nfrom devi...   \n",
       "3998  \\n'Run PEtab test suite (https://github.com/PE...   \n",
       "3999  \\nimport io\\nimport os\\nimport sys\\nfrom PIL i...   \n",
       "\n",
       "                                                  preds  \\\n",
       "0     \\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...   \n",
       "1     \\n'security converge saved queries\\n\\nRevision...   \n",
       "2     \\nfrom gng import GrowingNeuralGas\\nfrom sklea...   \n",
       "3     \\n'\\nCreated on Mar 27, 2018\\n\\n@author: Starl...   \n",
       "4     \\nimport os\\nimport tempfile\\nfrom os.path imp...   \n",
       "...                                                 ...   \n",
       "3995  \\nimport numpy as np\\nprint(np.empty(3))\\nprin...   \n",
       "3996  \\nfrom math import cos, pi, sin\\nfrom PyQt5.Qt...   \n",
       "3997  \\nimport numpy as np\\nimport pytest\\nfrom devi...   \n",
       "3998  \\n'Run PEtab test suite (https://github.com/PE...   \n",
       "3999  \\nimport io\\nimport os\\nimport sys\\nfrom PIL i...   \n",
       "\n",
       "                                                 labels  \n",
       "0     \\n__author__ = 'morta@digitus.itk.ppke.hu'\\nim...  \n",
       "1     \\n'security converge saved queries\\n\\nRevision...  \n",
       "2     \\nfrom gng import GrowingNeuralGas\\nfrom sklea...  \n",
       "3     \\n'\\nCreated on Mar 27, 2018\\n\\n@author: Starl...  \n",
       "4     \\nimport os\\nimport tempfile\\nfrom os.path imp...  \n",
       "...                                                 ...  \n",
       "3995  \\nimport numpy as np\\nprint(np.empty(3))\\nprin...  \n",
       "3996  \\nfrom math import cos, pi, sin\\nfrom PyQt5.Qt...  \n",
       "3997  \\nimport numpy as np\\nimport pytest\\nfrom devi...  \n",
       "3998  \\n'Run PEtab test suite (https://github.com/PE...  \n",
       "3999  \\nimport io\\nimport os\\nimport sys\\nfrom PIL i...  \n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ca79b4-d704-4091-abc0-625d549cca91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================0-INPUT=====================\n",
      "\n",
      "\n",
      "__author__ = 'morta@digitus.itk.ppke.hu'\n",
      "import os\n",
      "from docmodel import token\n",
      "from purepos.common.analysisqueue import AnalysisQueue\n",
      "STEM_FILTER_FILE = 'purepos_stems.txt'\n",
      "UNKOWN_VALUE = (- 99.0)\n",
      "LEMMA_MAPPER = None\n",
      "analysis_queue = AnalysisQueue()\n",
      "CONFIGURATION = None\n",
      "\n",
      "class Constants():\n",
      "\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "class StemFilter():\n",
      "\n",
      "    def __init__(self, filename: str):\n",
      "        self.stems = set()\n",
      "        with open(filename) as file:\n",
      "            self.stems = set(file.readlines())\n",
      "\n",
      "    def filter_stem(self, candidates) -> list:\n",
      "        if (len(self.stems) == 0):\n",
      "            return candidates\n",
      "        ret = []\u001b[31m\n",
      "        for t in candidates:\n",
      "            if (t.stem in self.stems):\n",
      "                ret.append(t)\u001b[0m\n",
      "        if (len(ret) == 0):\n",
      "            return candidates\n",
      "        return ret\n",
      "\n",
      "    @staticmethod\n",
      "    def create_stem_filter():\n",
      "        if os.path.isfile(STEM_FILTER_FILE):\n",
      "            return StemFilter(STEM_FILTER_FILE)\n",
      "\n",
      "def simplify_lemma(t: token.Token):\n",
      "    if (LEMMA_MAPPER is not None):\n",
      "        return token.ModToken(t.token, original_stem=t.stem, stem=LEMMA_MAPPER.map(t.stem), tag=t.tag)\n",
      "    return t\n",
      "\n",
      "\n",
      "=====================0-PREDICTION=====================\n",
      "\n",
      "\n",
      "__author__ = 'morta@digitus.itk.ppke.hu'\n",
      "import os\n",
      "from docmodel import token\n",
      "from purepos.common.analysisqueue import AnalysisQueue\n",
      "STEM_FILTER_FILE = 'purepos_stems.txt'\n",
      "UNKOWN_VALUE = (- 99.0)\n",
      "LEMMA_MAPPER = None\n",
      "analysis_queue = AnalysisQueue()\n",
      "CONFIGURATION = None\n",
      "\n",
      "class Constants():\n",
      "\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "class StemFilter():\n",
      "\n",
      "    def __init__(self, filename: str):\n",
      "        self.stems = set()\n",
      "        with open(filename) as file:\n",
      "            self.stems = set(file.readlines())\n",
      "\n",
      "    def filter_stem(self, candidates) -> list:\n",
      "        if (len(self.stems) == 0):\n",
      "            return candidates\n",
      "        ret = []\n",
      "        for t in candidates:\n",
      "            if (t.stem in self.stems):\n",
      "                ret.append(t)\n",
      "        if (len(ret) == 0):\n",
      "            return candidates\n",
      "        return ret\n",
      "\n",
      "    @staticmethod\n",
      "    def create_stem_filter():\n",
      "        if os.path.isfile(STEM_FILTER_FILE):\n",
      "            return StemFilter(STEM_FILTER_FILE)\n",
      "\n",
      "def simplify_lemma(t: token.Token):\n",
      "    if (LEMMA_MAPPER is not None):\n",
      "        return token.ModToken(t.token, original_stem=t.stem, stem=LEMMA_MAPPER.map\n",
      "\n",
      "=====================0-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "__author__ = 'morta@digitus.itk.ppke.hu'\n",
      "import os\n",
      "from docmodel import token\n",
      "from purepos.common.analysisqueue import AnalysisQueue\n",
      "STEM_FILTER_FILE = 'purepos_stems.txt'\n",
      "UNKOWN_VALUE = (- 99.0)\n",
      "LEMMA_MAPPER = None\n",
      "analysis_queue = AnalysisQueue()\n",
      "CONFIGURATION = None\n",
      "\n",
      "class Constants():\n",
      "\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "class StemFilter():\n",
      "\n",
      "    def __init__(self, filename: str):\n",
      "        self.stems = set()\n",
      "        with open(filename) as file:\n",
      "            self.stems = set(file.readlines())\n",
      "\n",
      "    def filter_stem(self, candidates) -> list:\n",
      "        if (len(self.stems) == 0):\n",
      "            return candidates\n",
      "        ret = [\u001b[31mt for t in candidates if (t.stem in self.stems)\u001b[0m]\n",
      "        if (len(ret) == 0):\n",
      "            return candidates\n",
      "        return ret\n",
      "\n",
      "    @staticmethod\n",
      "    def create_stem_filter():\n",
      "        if os.path.isfile(STEM_FILTER_FILE):\n",
      "            return StemFilter(STEM_FILTER_FILE)\n",
      "\n",
      "def simplify_lemma(t: token.Token):\n",
      "    if (LEMMA_MAPPER is not None):\n",
      "        return token.ModToken(t.token, original_stem=t.stem, stem=LEMMA_MAPPER.map(t.stem), tag=t.tag)\n",
      "    return t\n",
      "\n",
      "\n",
      "=====================0-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================1-INPUT=====================\n",
      "\n",
      "\n",
      "'security converge saved queries\\n\\nRevision ID: e38177dbf641\\nRevises: a8173232b786\\nCreate Date: 2020-11-20 14:24:03.643031\\n\\n'\n",
      "revision = 'e38177dbf641'\n",
      "down_revision = 'a8173232b786'\n",
      "from alembic import op\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "from sqlalchemy.orm import Session\n",
      "from superset.migrations.shared.security_converge import add_pvms, get_reversed_new_pvms, get_reversed_pvm_map, migrate_roles, Pvm\n",
      "NEW_PVMS = {'SavedQuery': ('can_read', 'can_write')}\n",
      "PVM_MAP = {Pvm('SavedQueryView', 'can_list'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryView', 'can_show'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryView', 'can_add'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_edit'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_delete'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'muldelete'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_mulexport'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryViewApi', 'can_show'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryViewApi', 'can_edit'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryViewApi', 'can_list'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryViewApi', 'can_add'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryViewApi', 'muldelete'): (Pvm('SavedQuery', 'can_write'),)}\n",
      "\n",
      "def upgrade():\n",
      "    bind = op.get_bind()\n",
      "    session = Session(bind=bind)\n",
      "    add_pvms(session, NEW_PVMS)\n",
      "    migrate_roles(session, PVM_MAP)\n",
      "    try:\n",
      "        session.commit()\n",
      "    except SQLAlchemyError as ex:\n",
      "        print(f'An error occurred while upgrading permissions: {ex}')\n",
      "        session.rollback()\n",
      "\n",
      "def downgrade():\n",
      "    bind = op.get_bind()\n",
      "    session = Session(bind=bind)\n",
      "    add_pvms(session, get_reversed_new_pvms(PVM_MAP))\n",
      "    migrate_roles(session, get_reversed_pvm_map(PVM_MAP))\n",
      "    try:\n",
      "        session.commit()\n",
      "    except SQLAlchemyError as ex:\n",
      "        print(f'An error occurred while downgrading permissions: {ex}')\n",
      "        session.rollback()\n",
      "    pass\n",
      "\n",
      "\n",
      "=====================1-PREDICTION=====================\n",
      "\n",
      "\n",
      "'security converge saved queries\\n\\nRevision ID: e38177dbf641\\nRevises: a8173232b786\\nCreate Date: 2020-11-20 14:24:03.643031\\n\\n'\n",
      "revision = 'e38177dbf641'\n",
      "down_revision = 'a8173232b786'\n",
      "from alembic import op\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "from sqlalchemy.orm import Session\n",
      "from superset.migrations.shared.security_converge import add_pvms, get_reversed_new_pvms, get_reversed_pvm_map, migrate_roles, Pvm\n",
      "NEW_PVMS = {'SavedQuery': ('can_read', 'can_write')}\n",
      "PVM_MAP = {Pvm('SavedQueryView', 'can_list'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryView', 'can_show'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryView', 'can_add'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_edit'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView',\n",
      "\n",
      "=====================1-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "'security converge saved queries\\n\\nRevision ID: e38177dbf641\\nRevises: a8173232b786\\nCreate Date: 2020-11-20 14:24:03.643031\\n\\n'\n",
      "revision = 'e38177dbf641'\n",
      "down_revision = 'a8173232b786'\n",
      "from alembic import op\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "from sqlalchemy.orm import Session\n",
      "from superset.migrations.shared.security_converge import add_pvms, get_reversed_new_pvms, get_reversed_pvm_map, migrate_roles, Pvm\n",
      "NEW_PVMS = {'SavedQuery': ('can_read', 'can_write')}\n",
      "PVM_MAP = {Pvm('SavedQueryView', 'can_list'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryView', 'can_show'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryView', 'can_add'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_edit'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_delete'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'muldelete'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryView', 'can_mulexport'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryViewApi', 'can_show'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryViewApi', 'can_edit'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryViewApi', 'can_list'): (Pvm('SavedQuery', 'can_read'),), Pvm('SavedQueryViewApi', 'can_add'): (Pvm('SavedQuery', 'can_write'),), Pvm('SavedQueryViewApi', 'muldelete'): (Pvm('SavedQuery', 'can_write'),)}\n",
      "\n",
      "def upgrade():\n",
      "    bind = op.get_bind()\n",
      "    session = Session(bind=bind)\n",
      "    add_pvms(session, NEW_PVMS)\n",
      "    migrate_roles(session, PVM_MAP)\n",
      "    try:\n",
      "        session.commit()\n",
      "    except SQLAlchemyError as ex:\n",
      "        print(f'An error occurred while upgrading permissions: {ex}')\n",
      "        session.rollback()\n",
      "\n",
      "def downgrade():\n",
      "    bind = op.get_bind()\n",
      "    session = Session(bind=bind)\n",
      "    add_pvms(session, get_reversed_new_pvms(PVM_MAP))\n",
      "    migrate_roles(session, get_reversed_pvm_map(PVM_MAP))\n",
      "    try:\n",
      "        session.commit()\n",
      "    except SQLAlchemyError as ex:\n",
      "        print(f'An error occurred while downgrading permissions: {ex}')\n",
      "        session.rollback()\n",
      "    pass\n",
      "\n",
      "\n",
      "=====================1-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================2-INPUT=====================\n",
      "\n",
      "\n",
      "from gng import GrowingNeuralGas\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import os\n",
      "import shutil\n",
      "__authors__ = 'Adrien Guille'\n",
      "__email__ = 'adrien.guille@univ-lyon2.fr'\n",
      "if (__name__ == '__main__'):\n",
      "    if os.path.exists('visualization/sequence'):\n",
      "        shutil.rmtree('visualization/sequence')\n",
      "    os.makedirs('visualization/sequence')\n",
      "    n_samples = 2000\n",
      "    dataset_type = 'moons'\n",
      "    data = None\n",
      "    print('Preparing data...')\n",
      "    if (dataset_type == 'blobs'):\n",
      "        data = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
      "    elif (dataset_type == 'moons'):\n",
      "        data = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
      "    elif (dataset_type == 'circles'):\n",
      "        data = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
      "    data = StandardScaler().fit_transform(data[0])\n",
      "    print('Done.')\n",
      "    print('Fitting neural network...')\n",
      "    gng = GrowingNeuralGas(data)\n",
      "    gng.fit_network(e_b=0.1, e_n=0.006, a_max=10, l=200, a=0.5, d=0.995, passes=8, plot_evolution=True)\n",
      "    print(('Found %d clusters.' % gng.number_of_clusters()))\n",
      "    gng.plot_clusters(gng.cluster_data())\n",
      "\n",
      "\n",
      "=====================2-PREDICTION=====================\n",
      "\n",
      "\n",
      "from gng import GrowingNeuralGas\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import os\n",
      "import shutil\n",
      "__authors__ = 'Adrien Guille'\n",
      "__email__ = 'adrien.guille@univ-lyon2.fr'\n",
      "if (__name__ == '__main__'):\n",
      "    if os.path.exists('visualization/sequence'):\n",
      "        shutil.rmtree('visualization/sequence')\n",
      "    os.makedirs('visualization/sequence')\n",
      "    n_samples = 2000\n",
      "    dataset_type = 'moons'\n",
      "    data = None\n",
      "    print('Preparing data...')\n",
      "    if (dataset_type == 'blobs'):\n",
      "        data = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
      "    elif (dataset_type == 'moons'):\n",
      "        data = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
      "    elif (dataset_type == 'circles'):\n",
      "        data =\n",
      "\n",
      "=====================2-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "from gng import GrowingNeuralGas\n",
      "from sklearn import datasets\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import os\n",
      "import shutil\n",
      "__authors__ = 'Adrien Guille'\n",
      "__email__ = 'adrien.guille@univ-lyon2.fr'\n",
      "if (__name__ == '__main__'):\n",
      "    if os.path.exists('visualization/sequence'):\n",
      "        shutil.rmtree('visualization/sequence')\n",
      "    os.makedirs('visualization/sequence')\n",
      "    n_samples = 2000\n",
      "    dataset_type = 'moons'\n",
      "    data = None\n",
      "    print('Preparing data...')\n",
      "    if (dataset_type == 'blobs'):\n",
      "        data = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
      "    elif (dataset_type == 'moons'):\n",
      "        data = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
      "    elif (dataset_type == 'circles'):\n",
      "        data = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
      "    data = StandardScaler().fit_transform(data[0])\n",
      "    print('Done.')\n",
      "    print('Fitting neural network...')\n",
      "    gng = GrowingNeuralGas(data)\n",
      "    gng.fit_network(e_b=0.1, e_n=0.006, a_max=10, l=200, a=0.5, d=0.995, passes=8, plot_evolution=True)\n",
      "    print(('Found %d clusters.' % gng.number_of_clusters()))\n",
      "    gng.plot_clusters(gng.cluster_data())\n",
      "\n",
      "\n",
      "=====================2-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================3-INPUT=====================\n",
      "\n",
      "\n",
      "'\\nCreated on Mar 27, 2018\\n\\n@author: StarlitGhost\\n'\n",
      "import re\n",
      "import zlib\n",
      "from collections import OrderedDict\n",
      "from typing import List\n",
      "import requests\n",
      "from twisted.internet import threads\n",
      "from twisted.plugin import IPlugin\n",
      "from zope.interface import implementer\n",
      "from desertbot.message import IRCMessage\n",
      "from desertbot.moduleinterface import IModule\n",
      "from desertbot.modules.commandinterface import BotCommand\n",
      "from desertbot.response import IRCResponse\n",
      "\n",
      "@implementer(IPlugin, IModule)\n",
      "class Lang(BotCommand):\n",
      "\n",
      "    def triggers(self):\n",
      "        return self.commands.keys()\n",
      "\n",
      "    def help(self, query: List[str]) -> str:\n",
      "        command = query[0].lower()\n",
      "        helpText = re.sub('\\\\s+', ' ', self.commands[command].__doc__)\n",
      "        return '{cmdChar}{help}'.format(cmdChar=self.bot.commandChar, help=helpText)\n",
      "\n",
      "    def onLoad(self):\n",
      "        self.languages = None\n",
      "        d = threads.deferToThread(self._fetchLanguages)\n",
      "        d.addCallback(self._setLanguages)\n",
      "        self.templates = {'rust': 'fn main() {{\\n    println!(\"{{:?}}\", {{\\n        {code}\\n    }});\\n}}', 'cpp-clang': '#include <iostream>\\nint main() {{\\n    std::cout << ({code}) << std::endl;\\n}}', 'cpp-gcc': '#include <iostream>\\nint main() {{\\n    std::cout << ({code}) << std::endl;\\n}}', 'cobol-gnu': 'PROGRAM-ID.H.PROCEDURE\\n{code}\\n'}\n",
      "        self.cflags = {'cobol-gnu': ['-free']}\n",
      "\n",
      "    def _lang(self, message: IRCMessage):\n",
      "        'lang <lang> <code> - evaluates the given <code> as <lang>, using\\n        https://tio.run'\n",
      "        if (len(message.parameterList) > 0):\n",
      "            lang = message.parameterList[0].lower()\n",
      "            code = ' '.join(message.parameterList[1:])\n",
      "            if (lang in self.templates):\n",
      "                code = self.templates[lang].format(code=code)\n",
      "            result = self._tio(lang, code)\n",
      "            return IRCResponse(result.rstrip('\\n').replace('\\n', ' '), message.replyTo)\n",
      "        else:\n",
      "            return IRCResponse(self.help(message.command), message.replyTo)\n",
      "\n",
      "    def _langurl(self, message: IRCMessage):\n",
      "        'langurl <lang> <url> <input> - evaluates the <code> at <url>\\n        as <lang> with <input> as stdin, using https://tio.run'\n",
      "        if (len(message.parameterList) > 1):\n",
      "            lang = message.parameterList[0].lower()\n",
      "            url = message.parameterList[1]\n",
      "            userInput = ' '.join(message.parameterList[2:])\n",
      "            response = self.bot.moduleHandler.runActionUntilValue('fetch-url', url)\n",
      "            if (not response):\n",
      "                return IRCResponse('Page not found at {!r}'.format(url), message.replyTo)\n",
      "            code = response.content\n",
      "            result = self._tio(lang, code, userInput)\n",
      "            return IRCResponse(result.rstrip('\\n').replace('\\n', ' '), message.replyTo)\n",
      "        else:\n",
      "            return IRCResponse(self.help(message.command), message.replyTo)\n",
      "    commands = OrderedDict([('lang', _lang), ('langurl', _langurl)])\n",
      "\n",
      "    def execute(self, message: IRCMessage):\n",
      "        return self.commands[message.command.lower()](self, message)\n",
      "\n",
      "    def _fetchLanguages(self):\n",
      "        self.logger.info('Loading language list from TryItOnline...')\n",
      "        langUrl = 'https://raw.githubusercontent.com/TryItOnline/tryitonline/master/usr/share/tio.run/languages.json'\n",
      "        response = requests.get(langUrl)\n",
      "        return response.json().keys()\n",
      "\n",
      "    def _setLanguages(self, langs: List[str]):\n",
      "        self.languages = langs\n",
      "        self.logger.info('Language list loaded')\n",
      "\n",
      "    def _tio(self, lang: str, code: str, userInput: str='') -> str:\n",
      "        if (self.languages is None):\n",
      "            self._setLanguages(self._fetchLanguages())\n",
      "        if (lang not in self.languages):\n",
      "            langList = self.bot.moduleHandler.runActionUntilValue('closest-matches', lang, self.languages, 10, 0.8)\n",
      "            langString = ', '.join(langList)\n",
      "            return '[Language {!r} unknown on tio.run. Perhaps you want: {}]'.format(lang, langString)\n",
      "        if (lang in self.cflags):\n",
      "            cflags = self.cflags[lang]\n",
      "        else:\n",
      "            cflags = ['']\n",
      "        request = [{'command': 'V', 'payload': {'lang': [lang]}}, {'command': 'V', 'payload': {'TIO_CFLAGS': cflags}}, {'command': 'F', 'payload': {'.code.tio': code}}, {'command': 'F', 'payload': {'.input.tio': userInput}}, {'command': 'RC'}]\n",
      "        req = b''\n",
      "        for instr in request:\n",
      "            req += instr['command'].encode()\n",
      "            if ('payload' in instr):\n",
      "                [(name, value)] = instr['payload'].items()\n",
      "                req += (b'%s\\x00' % name.encode())\n",
      "                if (type(value) == str):\n",
      "                    value = value.encode()\n",
      "                req += (b'%u\\x00' % len(value))\n",
      "                if (type(value) != bytes):\n",
      "                    value = ('\\x00'.join(value).encode() + b'\\x00')\n",
      "                req += value\n",
      "        req_raw = zlib.compress(req, 9)[2:(- 4)]\n",
      "        url = 'https://tio.run/cgi-bin/static/b666d85ff48692ae95f24a66f7612256-run/93d25ed21c8d2bb5917e6217ac439d61'\n",
      "        res = requests.post(url, data=req_raw)\n",
      "        res = zlib.decompress(res.content, 31)\n",
      "        delim = res[:16]\n",
      "        ret = res[16:].split(delim)\n",
      "        count = (len(ret) >> 1)\n",
      "        (returned, errors) = (ret[:count], ret[count:])\n",
      "        errors = errors[0].decode('utf-8', 'ignore')\n",
      "        if (int(errors.splitlines()[(- 1)][(len('Exit code: ') - 1):]) != 0):\n",
      "            paste = '{code}\\n\\n/* --- stdout ---\\n{stdout}\\n*/\\n\\n/* --- stderr ---\\n{stderr}\\n*/'.format(code=code, stdout=returned[0].decode('utf-8', 'ignore'), stderr=errors)\n",
      "            url = self.bot.moduleHandler.runActionUntilValue('upload-dbco', paste, (10 * 60))\n",
      "            error = 'Errors occurred! Output: {url}'.format(url=url)\n",
      "            if (lang in self.templates):\n",
      "                error += ' (language uses a template, see link for framing code)'\n",
      "            return error\n",
      "        return ' | '.join((r.decode('utf-8', 'ignore') for r in returned))\n",
      "lang = Lang()\n",
      "\n",
      "\n",
      "=====================3-PREDICTION=====================\n",
      "\n",
      "\n",
      "'\\nCreated on Mar 27, 2018\\n\\n@author: StarlitGhost\\n'\n",
      "import re\n",
      "import zlib\n",
      "from collections import OrderedDict\n",
      "from typing import List\n",
      "import requests\n",
      "from twisted.internet import threads\n",
      "from twisted.plugin import IPlugin\n",
      "from zope.interface import implementer\n",
      "from desertbot.message import IRCMessage\n",
      "from desertbot.moduleinterface import IModule\n",
      "from desertbot.modules.commandinterface import BotCommand\n",
      "from desertbot.response import IRCResponse\n",
      "\n",
      "@implementer(IPlugin, IModule)\n",
      "class Lang(BotCommand):\n",
      "\n",
      "    def triggers(self):\n",
      "        return self.commands.keys()\n",
      "\n",
      "    def help(self, query: List[str]) -> str:\n",
      "        command = query[0].lower()\n",
      "        helpText = re.sub('\\\\s+', ' ', self.commands[command].__doc__)\n",
      "        return '{cmdChar}{help}'.format(cmdChar=self.bot.commandChar, help=helpText)\n",
      "\n",
      "    def onLoad(self):\n",
      "        self.languages = None\n",
      "        d = threads.deferToThread(self._fetchLanguages)\n",
      "        d.addCallback(self._setLanguages)\n",
      "        self.templates = {'rust': 'fn main() {{\\n    println!(\"{{:?}}\", {{\\n        {code}\\n    }});\\n}}', 'cpp-clang': '#include <iostream>\\nint main() {{\\n    std::cout << ({code}) << std::endl;\\n}}', 'cpp-gcc': '#include <iostream>\\nint main() {{\\n    std::cout << ({code}) << std::endl;\\n}}', 'cobol-gnu': 'PROGRAM-ID.H.PROCEDURE\\n{code}\\n'}\n",
      "        self.cflags = {'cobol-gnu': ['-free']}\n",
      "\n",
      "    def _lang(self, message: IRCMessage):\n",
      "        'lang <lang> <code> - evaluates the given <code> as <lang>, using\\n        https://tio.run'\n",
      "        if (len(message.parameterList) > 0):\n",
      "            lang = message.parameterList[0].lower()\n",
      "            code = ' '.join(message.parameterList[1:])\n",
      "            if (lang in self.templates):\n",
      "                code = self.templates[lang].format(code=code)\n",
      "            result = self._tio(lang, code)\n",
      "            return IRCResponse(result.rstrip('\\n').replace('\\n', ' '), message.replyTo)\n",
      "        else:\n",
      "            return IRCResponse(self.help(message.command), message.replyTo)\n",
      "\n",
      "    def _langurl(self, message: IRCMessage):\n",
      "        'langurl <lang> <url> <input> - evaluates the <code> at <url>\\n        as <lang> with <input> as stdin, using https://tio.run'\n",
      "        if (len(message.parameterList) > 1):\n",
      "            lang = message.parameterList[0].lower()\n",
      "            url = message.parameterList[1]\n",
      "            userInput = ' '.join(message.parameterList[2:])\n",
      "            response = self.bot.moduleHandler.runActionUntilValue('fetch-url', url)\n",
      "            if (not response):\n",
      "                return IRCResponse('Page not found at {!r}'.format(url), message.replyTo)\n",
      "            code = response.content\n",
      "            result = self._tio(lang, code, userInput)\n",
      "            return IRCResponse(result.rstrip('\\n').replace('\\n', ' '), message.replyTo)\n",
      "        else:\n",
      "            return IRCResponse(self.help(message.command), message.replyTo)\n",
      "    commands = OrderedDict([('lang', _lang), ('langurl', _langurl)])\n",
      "\n",
      "    def execute(self, message: IRCMessage):\n",
      "        return self.commands[message.command.lower()](self, message)\n",
      "\n",
      "    def _fetchLanguages(self):\n",
      "        self.logger.info('Loading language list from TryItOnline...')\n",
      "        langUrl = 'https://raw.githubusercontent.com/TryItOnline/tryitonline/master/usr/share/tio.run/languages.json'\n",
      "        response = requests.get(langUrl)\n",
      "        return response.json().keys()\n",
      "\n",
      "    def _setLanguages(self, langs: List[str]):\n",
      "        self.languages = langs\n",
      "        self.logger.info('Language list loaded')\n",
      "\n",
      "    def _tio(self, lang: str, code: str, userInput: str='') -> str:\n",
      "        if (self.languages is None):\n",
      "            self._setLanguages(self._fetchLanguages())\n",
      "        if (lang not in self.languages):\n",
      "            langList = self.bot.moduleHandler.runActionUntilValue('closest-matches', lang, self.languages, 10, 0.8)\n",
      "            langString = ', '.join(langList)\n",
      "            return '[Language {!r} unknown on tio.run. Perhaps you want: {}]'.format(lang, langString)\n",
      "        if (lang in self.cflags):\n",
      "            cflags = self.cflags[lang]\n",
      "        else:\n",
      "            cflags = ['']\n",
      "        request = [{'command': 'V', 'payload': {'lang': [lang]}}, {'command': 'V', 'payload': {'TIO_CFLAGS': cflags}}, {'command': 'F', 'payload': {'.code.tio': code}}, {'command': 'F', 'payload': {'.input.tio': userInput}}, {'command': 'RC'}]\n",
      "        req = b''\n",
      "        for instr in request:\n",
      "            req += instr['command'].encode()\n",
      "            if ('payload' in instr):\n",
      "                [(name, value)] = instr['payload'].items()\n",
      "                req += (b'%s\\x00' % name\n",
      "\n",
      "=====================3-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "'\\nCreated on Mar 27, 2018\\n\\n@author: StarlitGhost\\n'\n",
      "import re\n",
      "import zlib\n",
      "from collections import OrderedDict\n",
      "from typing import List\n",
      "import requests\n",
      "from twisted.internet import threads\n",
      "from twisted.plugin import IPlugin\n",
      "from zope.interface import implementer\n",
      "from desertbot.message import IRCMessage\n",
      "from desertbot.moduleinterface import IModule\n",
      "from desertbot.modules.commandinterface import BotCommand\n",
      "from desertbot.response import IRCResponse\n",
      "\n",
      "@implementer(IPlugin, IModule)\n",
      "class Lang(BotCommand):\n",
      "\n",
      "    def triggers(self):\n",
      "        return self.commands.keys()\n",
      "\n",
      "    def help(self, query: List[str]) -> str:\n",
      "        command = query[0].lower()\n",
      "        helpText = re.sub('\\\\s+', ' ', self.commands[command].__doc__)\n",
      "        return '{cmdChar}{help}'.format(cmdChar=self.bot.commandChar, help=helpText)\n",
      "\n",
      "    def onLoad(self):\n",
      "        self.languages = None\n",
      "        d = threads.deferToThread(self._fetchLanguages)\n",
      "        d.addCallback(self._setLanguages)\n",
      "        self.templates = {'rust': 'fn main() {{\\n    println!(\"{{:?}}\", {{\\n        {code}\\n    }});\\n}}', 'cpp-clang': '#include <iostream>\\nint main() {{\\n    std::cout << ({code}) << std::endl;\\n}}', 'cpp-gcc': '#include <iostream>\\nint main() {{\\n    std::cout << ({code}) << std::endl;\\n}}', 'cobol-gnu': 'PROGRAM-ID.H.PROCEDURE\\n{code}\\n'}\n",
      "        self.cflags = {'cobol-gnu': ['-free']}\n",
      "\n",
      "    def _lang(self, message: IRCMessage):\n",
      "        'lang <lang> <code> - evaluates the given <code> as <lang>, using\\n        https://tio.run'\n",
      "        if (len(message.parameterList) > 0):\n",
      "            lang = message.parameterList[0].lower()\n",
      "            code = ' '.join(message.parameterList[1:])\n",
      "            if (lang in self.templates):\n",
      "                code = self.templates[lang].format(code=code)\n",
      "            result = self._tio(lang, code)\n",
      "            return IRCResponse(result.rstrip('\\n').replace('\\n', ' '), message.replyTo)\n",
      "        else:\n",
      "            return IRCResponse(self.help(message.command), message.replyTo)\n",
      "\n",
      "    def _langurl(self, message: IRCMessage):\n",
      "        'langurl <lang> <url> <input> - evaluates the <code> at <url>\\n        as <lang> with <input> as stdin, using https://tio.run'\n",
      "        if (len(message.parameterList) > 1):\n",
      "            lang = message.parameterList[0].lower()\n",
      "            url = message.parameterList[1]\n",
      "            userInput = ' '.join(message.parameterList[2:])\n",
      "            response = self.bot.moduleHandler.runActionUntilValue('fetch-url', url)\n",
      "            if (not response):\n",
      "                return IRCResponse('Page not found at {!r}'.format(url), message.replyTo)\n",
      "            code = response.content\n",
      "            result = self._tio(lang, code, userInput)\n",
      "            return IRCResponse(result.rstrip('\\n').replace('\\n', ' '), message.replyTo)\n",
      "        else:\n",
      "            return IRCResponse(self.help(message.command), message.replyTo)\n",
      "    commands = OrderedDict([('lang', _lang), ('langurl', _langurl)])\n",
      "\n",
      "    def execute(self, message: IRCMessage):\n",
      "        return self.commands[message.command.lower()](self, message)\n",
      "\n",
      "    def _fetchLanguages(self):\n",
      "        self.logger.info('Loading language list from TryItOnline...')\n",
      "        langUrl = 'https://raw.githubusercontent.com/TryItOnline/tryitonline/master/usr/share/tio.run/languages.json'\n",
      "        response = requests.get(langUrl)\n",
      "        return response.json().keys()\n",
      "\n",
      "    def _setLanguages(self, langs: List[str]):\n",
      "        self.languages = langs\n",
      "        self.logger.info('Language list loaded')\n",
      "\n",
      "    def _tio(self, lang: str, code: str, userInput: str='') -> str:\n",
      "        if (self.languages is None):\n",
      "            self._setLanguages(self._fetchLanguages())\n",
      "        if (lang not in self.languages):\n",
      "            langList = self.bot.moduleHandler.runActionUntilValue('closest-matches', lang, self.languages, 10, 0.8)\n",
      "            langString = ', '.join(langList)\n",
      "            return '[Language {!r} unknown on tio.run. Perhaps you want: {}]'.format(lang, langString)\n",
      "        if (lang in self.cflags):\n",
      "            cflags = self.cflags[lang]\n",
      "        else:\n",
      "            cflags = ['']\n",
      "        request = [{'command': 'V', 'payload': {'lang': [lang]}}, {'command': 'V', 'payload': {'TIO_CFLAGS': cflags}}, {'command': 'F', 'payload': {'.code.tio': code}}, {'command': 'F', 'payload': {'.input.tio': userInput}}, {'command': 'RC'}]\n",
      "        req = b''\n",
      "        for instr in request:\n",
      "            req += instr['command'].encode()\n",
      "            if ('payload' in instr):\n",
      "                [(name, value)] = instr['payload'].items()\n",
      "                req += (b'%s\\x00' % name.encode())\n",
      "                if (type(value) == str):\n",
      "                    value = value.encode()\n",
      "                req += (b'%u\\x00' % len(value))\n",
      "                if (type(value) != bytes):\n",
      "                    value = ('\\x00'.join(value).encode() + b'\\x00')\n",
      "                req += value\n",
      "        req_raw = zlib.compress(req, 9)[2:(- 4)]\n",
      "        url = 'https://tio.run/cgi-bin/static/b666d85ff48692ae95f24a66f7612256-run/93d25ed21c8d2bb5917e6217ac439d61'\n",
      "        res = requests.post(url, data=req_raw)\n",
      "        res = zlib.decompress(res.content, 31)\n",
      "        delim = res[:16]\n",
      "        ret = res[16:].split(delim)\n",
      "        count = (len(ret) >> 1)\n",
      "        (returned, errors) = (ret[:count], ret[count:])\n",
      "        errors = errors[0].decode('utf-8', 'ignore')\n",
      "        if (int(errors.splitlines()[(- 1)][(len('Exit code: ') - 1):]) != 0):\n",
      "            paste = '{code}\\n\\n/* --- stdout ---\\n{stdout}\\n*/\\n\\n/* --- stderr ---\\n{stderr}\\n*/'.format(code=code, stdout=returned[0].decode('utf-8', 'ignore'), stderr=errors)\n",
      "            url = self.bot.moduleHandler.runActionUntilValue('upload-dbco', paste, (10 * 60))\n",
      "            error = 'Errors occurred! Output: {url}'.format(url=url)\n",
      "            if (lang in self.templates):\n",
      "                error += ' (language uses a template, see link for framing code)'\n",
      "            return error\n",
      "        return ' | '.join((r.decode('utf-8', 'ignore') for r in returned))\n",
      "lang = Lang()\n",
      "\n",
      "\n",
      "=====================3-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================4-INPUT=====================\n",
      "\n",
      "\n",
      "import os\n",
      "import tempfile\n",
      "from os.path import abspath, dirname, join, normcase, sep\n",
      "from django.core.exceptions import SuspiciousFileOperation\n",
      "abspathu = abspath\n",
      "\n",
      "def upath(path):\n",
      "    'Always return a unicode path (did something for Python 2).'\n",
      "    return path\n",
      "\n",
      "def npath(path):\n",
      "    '\\n    Always return a native path, that is unicode on Python 3 and bytestring on\\n    Python 2. Noop for Python 3.\\n    '\n",
      "    return path\n",
      "\n",
      "def safe_join(base, *paths):\n",
      "    \"\\n    Join one or more path components to the base path component intelligently.\\n    Return a normalized, absolute version of the final path.\\n\\n    Raise ValueError if the final path isn't located inside of the base path\\n    component.\\n    \"\n",
      "    final_path = abspath(join(base, *paths))\n",
      "    base_path = abspath(base)\n",
      "    if ((not normcase(final_path).startswith(normcase((base_path + sep)))) and (normcase(final_path) != normcase(base_path)) and (dirname(normcase(base_path)) != normcase(base_path))):\n",
      "        raise SuspiciousFileOperation('The joined path ({}) is located outside of the base path component ({})'.format(final_path, base_path))\n",
      "    return final_path\n",
      "\n",
      "def symlinks_supported():\n",
      "    '\\n    Return whether or not creating symlinks are supported in the host platform\\n    and/or if they are allowed to be created (e.g. on Windows it requires admin\\n    permissions).\\n    '\n",
      "    with tempfile.TemporaryDirectory() as temp_dir:\n",
      "        original_path = os.path.join(temp_dir, 'original')\n",
      "        symlink_path = os.path.join(temp_dir, 'symlink')\n",
      "        os.makedirs(original_path)\n",
      "        try:\n",
      "            os.symlink(original_path, symlink_path)\n",
      "            supported = True\n",
      "        except (OSError, NotImplementedError):\n",
      "            supported = False\n",
      "        return supported\n",
      "\n",
      "\n",
      "=====================4-PREDICTION=====================\n",
      "\n",
      "\n",
      "import os\n",
      "import tempfile\n",
      "from os.path import abspath, dirname, join, normcase, sep\n",
      "from django.core.exceptions import SuspiciousFileOperation\n",
      "abspathu = abspath\n",
      "\n",
      "def upath(path):\n",
      "    'Always return a unicode path (did something for Python 2).'\n",
      "    return path\n",
      "\n",
      "def npath(path):\n",
      "    '\\n    Always return a native path, that is unicode on Python 3 and bytestring on\\n    Python 2. Noop for Python 3.\\n    '\n",
      "    return path\n",
      "\n",
      "def safe_join(base, *paths):\n",
      "    \"\\n    Join one or more path components to the base path component intelligently.\\n    Return a normalized, absolute version of the final path.\\n\\n    Raise ValueError if the final path isn't located inside of the base path\\n    component.\\n    \"\n",
      "    final_path = abspath(join(base, *paths))\n",
      "    base_path = abspath(base)\n",
      "    if ((not normcase(final_path).startswith(normcase((base_path + sep)))) and (normcase(final_path) != normcase(base_path)) and (dirname(normcase(base_path)) != normcase(base_path))):\n",
      "        raise SuspiciousFileOperation('The joined path ({}) is located outside of the base path component ({})'.format(final_path, base_path))\n",
      "    return final_path\n",
      "\n",
      "def symlinks_supported():\n",
      "    '\\n    Return whether or not creating symlinks are supported in the host platform\\n    and/or if they are allowed to be created (e.g. on Windows it requires admin\\n    permissions).\\n    '\n",
      "    with tempfile.TemporaryDirectory() as temp_dir:\n",
      "        original_path = os.path.join(temp_dir, 'original')\n",
      "        symlink_path = os\n",
      "\n",
      "=====================4-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import os\n",
      "import tempfile\n",
      "from os.path import abspath, dirname, join, normcase, sep\n",
      "from django.core.exceptions import SuspiciousFileOperation\n",
      "abspathu = abspath\n",
      "\n",
      "def upath(path):\n",
      "    'Always return a unicode path (did something for Python 2).'\n",
      "    return path\n",
      "\n",
      "def npath(path):\n",
      "    '\\n    Always return a native path, that is unicode on Python 3 and bytestring on\\n    Python 2. Noop for Python 3.\\n    '\n",
      "    return path\n",
      "\n",
      "def safe_join(base, *paths):\n",
      "    \"\\n    Join one or more path components to the base path component intelligently.\\n    Return a normalized, absolute version of the final path.\\n\\n    Raise ValueError if the final path isn't located inside of the base path\\n    component.\\n    \"\n",
      "    final_path = abspath(join(base, *paths))\n",
      "    base_path = abspath(base)\n",
      "    if ((not normcase(final_path).startswith(normcase((base_path + sep)))) and (normcase(final_path) != normcase(base_path)) and (dirname(normcase(base_path)) != normcase(base_path))):\n",
      "        raise SuspiciousFileOperation('The joined path ({}) is located outside of the base path component ({})'.format(final_path, base_path))\n",
      "    return final_path\n",
      "\n",
      "def symlinks_supported():\n",
      "    '\\n    Return whether or not creating symlinks are supported in the host platform\\n    and/or if they are allowed to be created (e.g. on Windows it requires admin\\n    permissions).\\n    '\n",
      "    with tempfile.TemporaryDirectory() as temp_dir:\n",
      "        original_path = os.path.join(temp_dir, 'original')\n",
      "        symlink_path = os.path.join(temp_dir, 'symlink')\n",
      "        os.makedirs(original_path)\n",
      "        try:\n",
      "            os.symlink(original_path, symlink_path)\n",
      "            supported = True\n",
      "        except (OSError, NotImplementedError):\n",
      "            supported = False\n",
      "        return supported\n",
      "\n",
      "\n",
      "=====================4-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================5-INPUT=====================\n",
      "\n",
      "\n",
      "from unittest import TestCase\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pandas.util.testing as tm\n",
      "import ts_charting.figure as figure\n",
      "from ts_charting.figure import process_series\n",
      "\n",
      "class Testprocess_data(TestCase):\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        TestCase.__init__(self, *args, **kwargs)\n",
      "\n",
      "    def runTest(self):\n",
      "        pass\n",
      "\n",
      "    def setUp(self):\n",
      "        pass\n",
      "\n",
      "    def test_already_aligned(self):\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        plot_series = process_series(series, plot_index)\n",
      "        tm.assert_almost_equal(series, plot_series)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "\n",
      "    def test_partial_plot(self):\n",
      "        '\\n        Test plotting series that is a subset of plot_index.\\n        Should align and fill with nans\\n        '\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        series = series[:50]\n",
      "        plot_series = process_series(series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert (plot_series.count() == 50)\n",
      "        assert np.all(plot_series[50:].isnull())\n",
      "        assert np.all((plot_series[:50] == series[:50]))\n",
      "\n",
      "    def test_unaligned_indexes(self):\n",
      "        '\\n        Test when series.index and plot_index have no common datetimes\\n        '\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        shift_series = series.tshift((- 1), '1h')\n",
      "        plot_series = process_series(shift_series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all(plot_series.isnull())\n",
      "        plot_series = process_series(shift_series, plot_index, method='ffill')\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        tm.assert_almost_equal(plot_series, series)\n",
      "\n",
      "    def test_different_freqs(self):\n",
      "        '\\n        Tests indexes of differeing frequencies. This is more of repeat\\n        test of test_partial_plot but with many holes instead of one half missing\\n        value.\\n        '\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        grouped_series = series.resample('MS', 'max')\n",
      "        plot_series = process_series(grouped_series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.dropna(), grouped_series)\n",
      "        plot_series = process_series(grouped_series, plot_index, method='ffill')\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert (plot_series.isnull().sum() == 0)\n",
      "        month_ind = (plot_series.index.month - 1)\n",
      "        assert np.all((grouped_series[month_ind] == plot_series))\n",
      "\n",
      "    def test_scalar(self):\n",
      "        '\\n        Test the various ways we handle scalars. \\n        '\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        plot_series = process_series(5, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all((plot_series == 5))\n",
      "        plot_series = process_series(5, plot_index, series_index=plot_index[10:20])\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all((plot_series[10:20] == 5))\n",
      "        assert (plot_series.isnull().sum() == 90)\n",
      "        plot_series = process_series(5, None, series_index=plot_index[10:20])\n",
      "        correct = pd.Series(5, index=plot_index[10:20])\n",
      "        tm.assert_almost_equal(correct, plot_series)\n",
      "        try:\n",
      "            plot_series = process_series(5, None)\n",
      "        except:\n",
      "            pass\n",
      "        else:\n",
      "            assert False, 'scalar should fail without plot_index or series_index'\n",
      "\n",
      "    def test_iterable(self):\n",
      "        '\\n        Non pd.Series iterables require an equal length series_index or \\n        plot_index.\\n        '\n",
      "        try:\n",
      "            plot_series = process_series(range(10), None)\n",
      "        except:\n",
      "            pass\n",
      "        else:\n",
      "            assert False, 'iterable should fail without plot_index or series_index'\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        try:\n",
      "            plot_series = process_series(range(10), plot_index)\n",
      "        except:\n",
      "            pass\n",
      "        else:\n",
      "            assert False, 'iterable requires an index of same length'\n",
      "        plot_series = process_series(range(10), plot_index[:10])\n",
      "        correct = pd.Series(range(10), index=plot_index[:10])\n",
      "        tm.assert_almost_equal(correct, plot_series)\n",
      "if (__name__ == '__main__'):\n",
      "    import nose\n",
      "    nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'], exit=False)\n",
      "\n",
      "\n",
      "=====================5-PREDICTION=====================\n",
      "\n",
      "\n",
      "from unittest import TestCase\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pandas.util.testing as tm\n",
      "import ts_charting.figure as figure\n",
      "from ts_charting.figure import process_series\n",
      "\n",
      "class Testprocess_data(TestCase):\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        TestCase.__init__(self, *args, **kwargs)\n",
      "\n",
      "    def runTest(self):\n",
      "        pass\n",
      "\n",
      "    def setUp(self):\n",
      "        pass\n",
      "\n",
      "    def test_already_aligned(self):\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        plot_series = process_series(series, plot_index)\n",
      "        tm.assert_almost_equal(series, plot_series)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "\n",
      "    def test_partial_plot(self):\n",
      "        '\\n        Test plotting series that is a subset of plot_index.\\n        Should align and fill with nans\\n        '\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        series = series[:50]\n",
      "        plot_series = process_series(series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert (plot_series.count() == 50)\n",
      "        assert np.all(plot_series[50:].isnull())\n",
      "        assert np.all((plot_series[:50] == series[:50]))\n",
      "\n",
      "    def test_unaligned_indexes(self):\n",
      "        '\\n        Test when series.index and plot_index have no common datetimes\\n        '\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        shift_series = series.tshift((- 1), '1h')\n",
      "        plot_series = process_series(shift_series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all(plot_series.isnull())\n",
      "        plot_series = process_series(shift_series, plot_index, method='ffill')\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        tm.assert_almost_equal(plot_series, series)\n",
      "\n",
      "    def test_different_freqs(self):\n",
      "        '\\n        Tests indexes of differeing frequencies. This is more of repeat\\n        test of test_partial_plot but with many holes instead of one half missing\\n        value.\\n        '\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        grouped_series = series.resample('MS', 'max')\n",
      "        plot_series = process_series(grouped_series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.dropna(), grouped_series)\n",
      "        plot_series = process_series(grouped_series, plot_index, method='ffill')\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert (plot_series.isnull().sum() == 0)\n",
      "        month_ind = (plot_series.index.month - 1)\n",
      "        assert np.all((grouped_series[month_ind] == plot_series))\n",
      "\n",
      "    def test_scalar(self):\n",
      "        '\\n        Test the various ways we handle scalars. \\n        '\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        plot_series = process_series(5, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all((plot_series == 5))\n",
      "        plot_series = process_series(5, plot_index, series_index=plot_index[10:20])\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all((plot_series[10:20] == 5))\n",
      "        assert (plot_series.isnull().sum() == 90)\n",
      "        plot_series = process_series(5, None, series_index=plot_index[10:20])\n",
      "        correct = pd\n",
      "\n",
      "=====================5-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "from unittest import TestCase\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pandas.util.testing as tm\n",
      "import ts_charting.figure as figure\n",
      "from ts_charting.figure import process_series\n",
      "\n",
      "class Testprocess_data(TestCase):\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        TestCase.__init__(self, *args, **kwargs)\n",
      "\n",
      "    def runTest(self):\n",
      "        pass\n",
      "\n",
      "    def setUp(self):\n",
      "        pass\n",
      "\n",
      "    def test_already_aligned(self):\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        plot_series = process_series(series, plot_index)\n",
      "        tm.assert_almost_equal(series, plot_series)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "\n",
      "    def test_partial_plot(self):\n",
      "        '\\n        Test plotting series that is a subset of plot_index.\\n        Should align and fill with nans\\n        '\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        series = series[:50]\n",
      "        plot_series = process_series(series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert (plot_series.count() == 50)\n",
      "        assert np.all(plot_series[50:].isnull())\n",
      "        assert np.all((plot_series[:50] == series[:50]))\n",
      "\n",
      "    def test_unaligned_indexes(self):\n",
      "        '\\n        Test when series.index and plot_index have no common datetimes\\n        '\n",
      "        plot_index = pd.date_range(start='2000', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        shift_series = series.tshift((- 1), '1h')\n",
      "        plot_series = process_series(shift_series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all(plot_series.isnull())\n",
      "        plot_series = process_series(shift_series, plot_index, method='ffill')\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        tm.assert_almost_equal(plot_series, series)\n",
      "\n",
      "    def test_different_freqs(self):\n",
      "        '\\n        Tests indexes of differeing frequencies. This is more of repeat\\n        test of test_partial_plot but with many holes instead of one half missing\\n        value.\\n        '\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        series = pd.Series(range(100), index=plot_index)\n",
      "        grouped_series = series.resample('MS', 'max')\n",
      "        plot_series = process_series(grouped_series, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.dropna(), grouped_series)\n",
      "        plot_series = process_series(grouped_series, plot_index, method='ffill')\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert (plot_series.isnull().sum() == 0)\n",
      "        month_ind = (plot_series.index.month - 1)\n",
      "        assert np.all((grouped_series[month_ind] == plot_series))\n",
      "\n",
      "    def test_scalar(self):\n",
      "        '\\n        Test the various ways we handle scalars. \\n        '\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        plot_series = process_series(5, plot_index)\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all((plot_series == 5))\n",
      "        plot_series = process_series(5, plot_index, series_index=plot_index[10:20])\n",
      "        tm.assert_almost_equal(plot_series.index, plot_index)\n",
      "        assert np.all((plot_series[10:20] == 5))\n",
      "        assert (plot_series.isnull().sum() == 90)\n",
      "        plot_series = process_series(5, None, series_index=plot_index[10:20])\n",
      "        correct = pd.Series(5, index=plot_index[10:20])\n",
      "        tm.assert_almost_equal(correct, plot_series)\n",
      "        try:\n",
      "            plot_series = process_series(5, None)\n",
      "        except:\n",
      "            pass\n",
      "        else:\n",
      "            assert False, 'scalar should fail without plot_index or series_index'\n",
      "\n",
      "    def test_iterable(self):\n",
      "        '\\n        Non pd.Series iterables require an equal length series_index or \\n        plot_index.\\n        '\n",
      "        try:\n",
      "            plot_series = process_series(range(10), None)\n",
      "        except:\n",
      "            pass\n",
      "        else:\n",
      "            assert False, 'iterable should fail without plot_index or series_index'\n",
      "        plot_index = pd.date_range(start='2000-01-01', freq='D', periods=100)\n",
      "        try:\n",
      "            plot_series = process_series(range(10), plot_index)\n",
      "        except:\n",
      "            pass\n",
      "        else:\n",
      "            assert False, 'iterable requires an index of same length'\n",
      "        plot_series = process_series(range(10), plot_index[:10])\n",
      "        correct = pd.Series(range(10), index=plot_index[:10])\n",
      "        tm.assert_almost_equal(correct, plot_series)\n",
      "if (__name__ == '__main__'):\n",
      "    import nose\n",
      "    nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'], exit=False)\n",
      "\n",
      "\n",
      "=====================5-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================6-INPUT=====================\n",
      "\n",
      "\n",
      "import logging\n",
      "from typing import Generator\n",
      "import arrow\n",
      "from oandakey import access_token, accountID\n",
      "from OnePy.custom_module.api.oanda_api import OandaAPI\n",
      "from OnePy.custom_module.oanda_bar import OandaBar\n",
      "from OnePy.sys_module.base_reader import ReaderBase\n",
      "from OnePy.sys_module.components.logger import LoggerFactory\n",
      "\n",
      "class OandaReader(ReaderBase):\n",
      "    host = 'localhost'\n",
      "    port = 27017\n",
      "\n",
      "    def __init__(self, ticker: str, log: bool=True, no_console: bool=True):\n",
      "        super().__init__(ticker)\n",
      "        self.ticker = ticker\n",
      "        if (not (accountID and access_token)):\n",
      "            raise Exception(\"oandakey should hasn't been settled!\")\n",
      "        self.oanda = OandaAPI(accountID, access_token)\n",
      "        if log:\n",
      "            LoggerFactory('oandapyV20', info_log=False)\n",
      "        if no_console:\n",
      "            logging.getLogger('oandapyV20').propagate = False\n",
      "\n",
      "    def load(self, fromdate=None, todate=None, frequency=None, count=2):\n",
      "        return self.get_candles(count)\n",
      "\n",
      "    def get_candles(self, count: int, frequency: str=None) -> list:\n",
      "        if (not frequency):\n",
      "            frequency = self.env.sys_frequency\n",
      "        data = self.oanda.get_candlestick_list(ticker=self.ticker, granularity=frequency, count=count)\n",
      "        return self._format_candle(data)\n",
      "\n",
      "    def load_by_cleaner(self, fromdate: str, todate: str, frequency: str) -> Generator:\n",
      "        return (i for i in self.get_candles(5000, frequency))\n",
      "\n",
      "    @staticmethod\n",
      "    def _format_candle(candle_data: list):\n",
      "        ohlc_list = []\n",
      "        for candle in candle_data['candles']:\n",
      "            ohlc = candle['mid']\n",
      "            ohlc['open'] = float(ohlc.pop('o'))\n",
      "            ohlc['high'] = float(ohlc.pop('h'))\n",
      "            ohlc['low'] = float(ohlc.pop('l'))\n",
      "            ohlc['close'] = float(ohlc.pop('c'))\n",
      "            ohlc['date'] = arrow.get(candle['time']).format('YYYY-MM-DD HH:mm:ss')\n",
      "            ohlc['volume'] = float(candle['volume'])\n",
      "            ohlc['complete'] = candle['complete']\n",
      "            ohlc_list.append(ohlc)\n",
      "        return ohlc_list\n",
      "\n",
      "    @property\n",
      "    def bar_class(self):\n",
      "        raise OandaBar\n",
      "\n",
      "\n",
      "=====================6-PREDICTION=====================\n",
      "\n",
      "\n",
      "import logging\n",
      "from typing import Generator\n",
      "import arrow\n",
      "from oandakey import access_token, accountID\n",
      "from OnePy.custom_module.api.oanda_api import OandaAPI\n",
      "from OnePy.custom_module.oanda_bar import OandaBar\n",
      "from OnePy.sys_module.base_reader import ReaderBase\n",
      "from OnePy.sys_module.components.logger import LoggerFactory\n",
      "\n",
      "class OandaReader(ReaderBase):\n",
      "    host = 'localhost'\n",
      "    port = 27017\n",
      "\n",
      "    def __init__(self, ticker: str, log: bool=True, no_console: bool=True):\n",
      "        super().__init__(ticker)\n",
      "        self.ticker = ticker\n",
      "        if (not (accountID and access_token)):\n",
      "            raise Exception(\"oandakey should hasn't been settled!\")\n",
      "        self.oanda = OandaAPI(accountID, access_token)\n",
      "        if log:\n",
      "            LoggerFactory('oandapyV20', info_log=False)\n",
      "        if no_console:\n",
      "            logging.getLogger('oandapyV20').propagate = False\n",
      "\n",
      "    def load(self, fromdate=None, todate=None, frequency=None, count=2):\n",
      "        return self.get_candles(count)\n",
      "\n",
      "    def get_candles(self, count: int, frequency: str=None) -> list:\n",
      "        if (not frequency):\n",
      "            frequency = self.env.sys_frequency\n",
      "        data = self.oanda.get_candlestick_list(ticker=self.ticker, granularity=frequency, count=count)\n",
      "        return self._format_candle(data)\n",
      "\n",
      "    def load_by_cleaner(self, fromdate: str, todate: str, frequency: str) -> Generator:\n",
      "        return (i for i in self.get_candles(5000, frequency))\n",
      "\n",
      "    @staticmethod\n",
      "    def _format_candle(candle_data: list):\n",
      "        ohlc_list = []\n",
      "        for candle in candle_data['candles']:\n",
      "            ohlc = candle['mid']\n",
      "            ohlc['open'] = float(ohlc.pop('o'))\n",
      "            ohlc['high'] = float(ohlc.pop('h'))\n",
      "            ohlc['low'] = float(ohlc.\n",
      "\n",
      "=====================6-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import logging\n",
      "from typing import Generator\n",
      "import arrow\n",
      "from oandakey import access_token, accountID\n",
      "from OnePy.custom_module.api.oanda_api import OandaAPI\n",
      "from OnePy.custom_module.oanda_bar import OandaBar\n",
      "from OnePy.sys_module.base_reader import ReaderBase\n",
      "from OnePy.sys_module.components.logger import LoggerFactory\n",
      "\n",
      "class OandaReader(ReaderBase):\n",
      "    host = 'localhost'\n",
      "    port = 27017\n",
      "\n",
      "    def __init__(self, ticker: str, log: bool=True, no_console: bool=True):\n",
      "        super().__init__(ticker)\n",
      "        self.ticker = ticker\n",
      "        if (not (accountID and access_token)):\n",
      "            raise Exception(\"oandakey should hasn't been settled!\")\n",
      "        self.oanda = OandaAPI(accountID, access_token)\n",
      "        if log:\n",
      "            LoggerFactory('oandapyV20', info_log=False)\n",
      "        if no_console:\n",
      "            logging.getLogger('oandapyV20').propagate = False\n",
      "\n",
      "    def load(self, fromdate=None, todate=None, frequency=None, count=2):\n",
      "        return self.get_candles(count)\n",
      "\n",
      "    def get_candles(self, count: int, frequency: str=None) -> list:\n",
      "        if (not frequency):\n",
      "            frequency = self.env.sys_frequency\n",
      "        data = self.oanda.get_candlestick_list(ticker=self.ticker, granularity=frequency, count=count)\n",
      "        return self._format_candle(data)\n",
      "\n",
      "    def load_by_cleaner(self, fromdate: str, todate: str, frequency: str) -> Generator:\n",
      "        return (i for i in self.get_candles(5000, frequency))\n",
      "\n",
      "    @staticmethod\n",
      "    def _format_candle(candle_data: list):\n",
      "        ohlc_list = []\n",
      "        for candle in candle_data['candles']:\n",
      "            ohlc = candle['mid']\n",
      "            ohlc['open'] = float(ohlc.pop('o'))\n",
      "            ohlc['high'] = float(ohlc.pop('h'))\n",
      "            ohlc['low'] = float(ohlc.pop('l'))\n",
      "            ohlc['close'] = float(ohlc.pop('c'))\n",
      "            ohlc['date'] = arrow.get(candle['time']).format('YYYY-MM-DD HH:mm:ss')\n",
      "            ohlc['volume'] = float(candle['volume'])\n",
      "            ohlc['complete'] = candle['complete']\n",
      "            ohlc_list.append(ohlc)\n",
      "        return ohlc_list\n",
      "\n",
      "    @property\n",
      "    def bar_class(self):\n",
      "        raise OandaBar\n",
      "\n",
      "\n",
      "=====================6-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================7-INPUT=====================\n",
      "\n",
      "\n",
      "import sys\n",
      "from threading import Event, RLock, Thread\n",
      "from types import TracebackType\n",
      "from typing import IO, Any, Callable, List, Optional, TextIO, Type, cast\n",
      "from . import get_console\n",
      "from .console import Console, ConsoleRenderable, RenderableType, RenderHook\n",
      "from .control import Control\n",
      "from .file_proxy import FileProxy\n",
      "from .jupyter import JupyterMixin\n",
      "from .live_render import LiveRender, VerticalOverflowMethod\n",
      "from .screen import Screen\n",
      "from .text import Text\n",
      "\n",
      "class _RefreshThread(Thread):\n",
      "    'A thread that calls refresh() at regular intervals.'\n",
      "\n",
      "    def __init__(self, live: 'Live', refresh_per_second: float) -> None:\n",
      "        self.live = live\n",
      "        self.refresh_per_second = refresh_per_second\n",
      "        self.done = Event()\n",
      "        super().__init__(daemon=True)\n",
      "\n",
      "    def stop(self) -> None:\n",
      "        self.done.set()\n",
      "\n",
      "    def run(self) -> None:\n",
      "        while (not self.done.wait((1 / self.refresh_per_second))):\n",
      "            with self.live._lock:\n",
      "                if (not self.done.is_set()):\n",
      "                    self.live.refresh()\n",
      "\n",
      "class Live(JupyterMixin, RenderHook):\n",
      "    'Renders an auto-updating live display of any given renderable.\\n\\n    Args:\\n        renderable (RenderableType, optional): The renderable to live display. Defaults to displaying nothing.\\n        console (Console, optional): Optional Console instance. Default will an internal Console instance writing to stdout.\\n        screen (bool, optional): Enable alternate screen mode. Defaults to False.\\n        auto_refresh (bool, optional): Enable auto refresh. If disabled, you will need to call `refresh()` or `update()` with refresh flag. Defaults to True\\n        refresh_per_second (float, optional): Number of times per second to refresh the live display. Defaults to 4.\\n        transient (bool, optional): Clear the renderable on exit (has no effect when screen=True). Defaults to False.\\n        redirect_stdout (bool, optional): Enable redirection of stdout, so ``print`` may be used. Defaults to True.\\n        redirect_stderr (bool, optional): Enable redirection of stderr. Defaults to True.\\n        vertical_overflow (VerticalOverflowMethod, optional): How to handle renderable when it is too tall for the console. Defaults to \"ellipsis\".\\n        get_renderable (Callable[[], RenderableType], optional): Optional callable to get renderable. Defaults to None.\\n    '\n",
      "\n",
      "    def __init__(self, renderable: Optional[RenderableType]=None, *, console: Optional[Console]=None, screen: bool=False, auto_refresh: bool=True, refresh_per_second: float=4, transient: bool=False, redirect_stdout: bool=True, redirect_stderr: bool=True, vertical_overflow: VerticalOverflowMethod='ellipsis', get_renderable: Optional[Callable[([], RenderableType)]]=None) -> None:\n",
      "        assert (refresh_per_second > 0), 'refresh_per_second must be > 0'\n",
      "        self._renderable = renderable\n",
      "        self.console = (console if (console is not None) else get_console())\n",
      "        self._screen = screen\n",
      "        self._alt_screen = False\n",
      "        self._redirect_stdout = redirect_stdout\n",
      "        self._redirect_stderr = redirect_stderr\n",
      "        self._restore_stdout: Optional[IO[str]] = None\n",
      "        self._restore_stderr: Optional[IO[str]] = None\n",
      "        self._lock = RLock()\n",
      "        self.ipy_widget: Optional[Any] = None\n",
      "        self.auto_refresh = auto_refresh\n",
      "        self._started: bool = False\n",
      "        self.transient = (True if screen else transient)\n",
      "        self._refresh_thread: Optional[_RefreshThread] = None\n",
      "        self.refresh_per_second = refresh_per_second\n",
      "        self.vertical_overflow = vertical_overflow\n",
      "        self._get_renderable = get_renderable\n",
      "        self._live_render = LiveRender(self.get_renderable(), vertical_overflow=vertical_overflow)\n",
      "\n",
      "    @property\n",
      "    def is_started(self) -> bool:\n",
      "        'Check if live display has been started.'\n",
      "        return self._started\n",
      "\n",
      "    def get_renderable(self) -> RenderableType:\n",
      "        renderable = (self._get_renderable() if (self._get_renderable is not None) else self._renderable)\n",
      "        return (renderable or '')\n",
      "\n",
      "    def start(self, refresh: bool=False) -> None:\n",
      "        'Start live rendering display.\\n\\n        Args:\\n            refresh (bool, optional): Also refresh. Defaults to False.\\n        '\n",
      "        with self._lock:\n",
      "            if self._started:\n",
      "                return\n",
      "            self.console.set_live(self)\n",
      "            self._started = True\n",
      "            if self._screen:\n",
      "                self._alt_screen = self.console.set_alt_screen(True)\n",
      "            self.console.show_cursor(False)\n",
      "            self._enable_redirect_io()\n",
      "            self.console.push_render_hook(self)\n",
      "            if refresh:\n",
      "                self.refresh()\n",
      "            if self.auto_refresh:\n",
      "                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)\n",
      "                self._refresh_thread.start()\n",
      "\n",
      "    def stop(self) -> None:\n",
      "        'Stop live rendering display.'\n",
      "        with self._lock:\n",
      "            if (not self._started):\n",
      "                return\n",
      "            self.console.clear_live()\n",
      "            self._started = False\n",
      "            if (self.auto_refresh and (self._refresh_thread is not None)):\n",
      "                self._refresh_thread.stop()\n",
      "                self._refresh_thread = None\n",
      "            self.vertical_overflow = 'visible'\n",
      "            with self.console:\n",
      "                try:\n",
      "                    if ((not self._alt_screen) and (not self.console.is_jupyter)):\n",
      "                        self.refresh()\n",
      "                finally:\n",
      "                    self._disable_redirect_io()\n",
      "                    self.console.pop_render_hook()\n",
      "                    if ((not self._alt_screen) and self.console.is_terminal):\n",
      "                        self.console.line()\n",
      "                    self.console.show_cursor(True)\n",
      "                    if self._alt_screen:\n",
      "                        self.console.set_alt_screen(False)\n",
      "                    if (self.transient and (not self._alt_screen)):\n",
      "                        self.console.control(self._live_render.restore_cursor())\n",
      "                    if ((self.ipy_widget is not None) and self.transient):\n",
      "                        self.ipy_widget.close()\n",
      "\n",
      "    def __enter__(self) -> 'Live':\n",
      "        self.start(refresh=(self._renderable is not None))\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:\n",
      "        self.stop()\n",
      "\n",
      "    def _enable_redirect_io(self) -> None:\n",
      "        'Enable redirecting of stdout / stderr.'\n",
      "        if (self.console.is_terminal or self.console.is_jupyter):\n",
      "            if (self._redirect_stdout and (not isinstance(sys.stdout, FileProxy))):\n",
      "                self._restore_stdout = sys.stdout\n",
      "                sys.stdout = cast('TextIO', FileProxy(self.console, sys.stdout))\n",
      "            if (self._redirect_stderr and (not isinstance(sys.stderr, FileProxy))):\n",
      "                self._restore_stderr = sys.stderr\n",
      "                sys.stderr = cast('TextIO', FileProxy(self.console, sys.stderr))\n",
      "\n",
      "    def _disable_redirect_io(self) -> None:\n",
      "        'Disable redirecting of stdout / stderr.'\n",
      "        if self._restore_stdout:\n",
      "            sys.stdout = cast('TextIO', self._restore_stdout)\n",
      "            self._restore_stdout = None\n",
      "        if self._restore_stderr:\n",
      "            sys.stderr = cast('TextIO', self._restore_stderr)\n",
      "            self._restore_stderr = None\n",
      "\n",
      "    @property\n",
      "    def renderable(self) -> RenderableType:\n",
      "        'Get the renderable that is being displayed\\n\\n        Returns:\\n            RenderableType: Displayed renderable.\\n        '\n",
      "        renderable = self.get_renderable()\n",
      "        return (Screen(renderable) if self._alt_screen else renderable)\n",
      "\n",
      "    def update(self, renderable: RenderableType, *, refresh: bool=False) -> None:\n",
      "        'Update the renderable that is being displayed\\n\\n        Args:\\n            renderable (RenderableType): New renderable to use.\\n            refresh (bool, optional): Refresh the display. Defaults to False.\\n        '\n",
      "        with self._lock:\n",
      "            self._renderable = renderable\n",
      "            if refresh:\n",
      "                self.refresh()\n",
      "\n",
      "    def refresh(self) -> None:\n",
      "        'Update the display of the Live Render.'\n",
      "        with self._lock:\n",
      "            self._live_render.set_renderable(self.renderable)\n",
      "            if self.console.is_jupyter:\n",
      "                try:\n",
      "                    from IPython.display import display\n",
      "                    from ipywidgets import Output\n",
      "                except ImportError:\n",
      "                    import warnings\n",
      "                    warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
      "                else:\n",
      "                    if (self.ipy_widget is None):\n",
      "                        self.ipy_widget = Output()\n",
      "                        display(self.ipy_widget)\n",
      "                    with self.ipy_widget:\n",
      "                        self.ipy_widget.clear_output(wait=True)\n",
      "                        self.console.print(self._live_render.renderable)\n",
      "            elif (self.console.is_terminal and (not self.console.is_dumb_terminal)):\n",
      "                with self.console:\n",
      "                    self.console.print(Control())\n",
      "            elif ((not self._started) and (not self.transient)):\n",
      "                with self.console:\n",
      "                    self.console.print(Control())\n",
      "\n",
      "    def process_renderables(self, renderables: List[ConsoleRenderable]) -> List[ConsoleRenderable]:\n",
      "        'Process renderables to restore cursor and display progress.'\n",
      "        self._live_render.vertical_overflow = self.vertical_overflow\n",
      "        if self.console.is_interactive:\n",
      "            with self._lock:\n",
      "                reset = (Control.home() if self._alt_screen else self._live_render.position_cursor())\n",
      "                renderables = [reset, *renderables, self._live_render]\n",
      "        elif ((not self._started) and (not self.transient)):\n",
      "            renderables = [*renderables, self._live_render]\n",
      "        return renderables\n",
      "if (__name__ == '__main__'):\n",
      "    import random\n",
      "    import time\n",
      "    from itertools import cycle\n",
      "    from typing import Dict, List, Tuple\n",
      "    from .align import Align\n",
      "    from .console import Console\n",
      "    from .live import Live as Live\n",
      "    from .panel import Panel\n",
      "    from .rule import Rule\n",
      "    from .syntax import Syntax\n",
      "    from .table import Table\n",
      "    console = Console()\n",
      "    syntax = Syntax('def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\\n    \"\"\"Iterate and generate a tuple with a flag for last value.\"\"\"\\n    iter_values = iter(values)\\n    try:\\n        previous_value = next(iter_values)\\n    except StopIteration:\\n        return\\n    for value in iter_values:\\n        yield False, previous_value\\n        previous_value = value\\n    yield True, previous_value', 'python', line_numbers=True)\n",
      "    table = Table('foo', 'bar', 'baz')\n",
      "    table.add_row('1', '2', '3')\n",
      "    progress_renderables = ['You can make the terminal shorter and taller to see the live table hideText may be printed while the progress bars are rendering.', Panel('In fact, [i]any[/i] renderable will work'), 'Such as [magenta]tables[/]...', table, 'Pretty printed structures...', {'type': 'example', 'text': 'Pretty printed'}, 'Syntax...', syntax, Rule('Give it a try!')]\n",
      "    examples = cycle(progress_renderables)\n",
      "    exchanges = ['SGD', 'MYR', 'EUR', 'USD', 'AUD', 'JPY', 'CNH', 'HKD', 'CAD', 'INR', 'DKK', 'GBP', 'RUB', 'NZD', 'MXN', 'IDR', 'TWD', 'THB', 'VND']\n",
      "    with Live(console=console) as live_table:\n",
      "        exchange_rate_dict: Dict[(Tuple[(str, str)], float)] = {}\n",
      "        for index in range(100):\n",
      "            select_exchange = exchanges[(index % len(exchanges))]\n",
      "            for exchange in exchanges:\n",
      "                if (exchange == select_exchange):\n",
      "                    continue\n",
      "                time.sleep(0.4)\n",
      "                if (random.randint(0, 10) < 1):\n",
      "                    console.log(next(examples))\n",
      "                exchange_rate_dict[(select_exchange, exchange)] = (200 / ((random.random() * 320) + 1))\n",
      "                if (len(exchange_rate_dict) > (len(exchanges) - 1)):\n",
      "                    exchange_rate_dict.pop(list(exchange_rate_dict.keys())[0])\n",
      "                table = Table(title='Exchange Rates')\n",
      "                table.add_column('Source Currency')\n",
      "                table.add_column('Destination Currency')\n",
      "                table.add_column('Exchange Rate')\n",
      "                for ((source, dest), exchange_rate) in exchange_rate_dict.items():\n",
      "                    table.add_row(source, dest, Text(f'{exchange_rate:.4f}', style=('red' if (exchange_rate < 1.0) else 'green')))\n",
      "                live_table.update(Align.center(table))\n",
      "\n",
      "\n",
      "=====================7-PREDICTION=====================\n",
      "\n",
      "\n",
      "import sys\n",
      "from threading import Event, RLock, Thread\n",
      "from types import TracebackType\n",
      "from typing import IO, Any, Callable, List, Optional, TextIO, Type, cast\n",
      "from . import get_console\n",
      "from .console import Console, ConsoleRenderable, RenderableType, RenderHook\n",
      "from .control import Control\n",
      "from .file_proxy import FileProxy\n",
      "from .jupyter import JupyterMixin\n",
      "from .live_render import LiveRender, VerticalOverflowMethod\n",
      "from .screen import Screen\n",
      "from .text import Text\n",
      "\n",
      "class _RefreshThread(Thread):\n",
      "    'A thread that calls refresh() at regular intervals.'\n",
      "\n",
      "    def __init__(self, live: 'Live', refresh_per_second: float) -> None:\n",
      "        self.live = live\n",
      "        self.refresh_per_second = refresh_per_second\n",
      "        self.done = Event()\n",
      "        super().__init__(daemon=True)\n",
      "\n",
      "    def stop(self) -> None:\n",
      "        self.done.set()\n",
      "\n",
      "    def run(self) -> None:\n",
      "        while (not self.done.wait((1 / self.refresh_per_second))):\n",
      "            with self.live._lock:\n",
      "                if (not self.done.is_set()):\n",
      "                    self.live.refresh()\n",
      "\n",
      "class Live(JupyterMixin, RenderHook):\n",
      "    'Renders an auto-updating live display of any given renderable.\\n\\n    Args:\\n        renderable (RenderableType, optional): The renderable to live display. Defaults to displaying nothing.\\n        console (Console, optional): Optional Console instance. Default will an internal Console instance writing to stdout.\\n        screen (bool, optional): Enable alternate screen mode. Defaults to False.\\n        auto_refresh (bool, optional): Enable auto refresh. If disabled, you will need to call `refresh()` or `update()` with refresh flag. Defaults to True\\n        refresh_per_second (float, optional): Number of times per second to refresh the live display. Defaults to 4.\\n        transient (bool, optional): Clear the renderable on exit (has no effect when screen=True). Defaults to False.\\n        redirect_stdout (bool, optional): Enable redirection of stdout, so ``print`` may be used. Defaults to True.\\n        redirect_stderr (bool, optional): Enable redirection of stderr. Defaults to True.\\n        vertical_overflow (VerticalOverflowMethod, optional): How to handle renderable when it is too tall for the console. Defaults to \"ellipsis\".\\n        get_renderable (Callable[[], RenderableType], optional): Optional callable to get renderable. Defaults to None.\\n    '\n",
      "\n",
      "    def __init__(self, renderable: Optional[RenderableType]=None, *, console: Optional[Console]=None, screen: bool=False, auto_refresh: bool=True, refresh_per_second: float=4, transient: bool=False, redirect_stdout: bool=True, redirect_stderr: bool=True, vertical_overflow: VerticalOverflowMethod='ellipsis', get_renderable: Optional[Callable[([], RenderableType)]]=None) -> None:\n",
      "        assert (refresh_per_second > 0), 'refresh_per_second must be > 0'\n",
      "        self._renderable = renderable\n",
      "        self.console = (console if (console is not None) else get_console())\n",
      "        self._screen = screen\n",
      "        self._alt_screen = False\n",
      "        self._redirect_stdout = redirect_stdout\n",
      "        self._redirect_stderr = redirect_stderr\n",
      "        self._restore_stdout: Optional[IO[str]] = None\n",
      "        self._restore_stderr: Optional[IO[str]] = None\n",
      "        self._lock = RLock()\n",
      "        self.ipy_widget: Optional[Any] = None\n",
      "        self.auto_refresh = auto_refresh\n",
      "        self._started: bool = False\n",
      "        self.transient = (True if screen else transient)\n",
      "        self._refresh_thread: Optional[_RefreshThread] = None\n",
      "        self.refresh_per_second = refresh_per_second\n",
      "        self.vertical_overflow = vertical_overflow\n",
      "        self._get_renderable = get_renderable\n",
      "        self._live_render = LiveRender(self.get_renderable(), vertical_overflow=vertical_overflow)\n",
      "\n",
      "    @property\n",
      "    def is_started(self) -> bool:\n",
      "        'Check if live display has been started.'\n",
      "        return self._started\n",
      "\n",
      "    def get_renderable(self) -> RenderableType:\n",
      "        renderable = (self._get_renderable() if (self._get_renderable is not None) else self._renderable)\n",
      "        return (renderable or '')\n",
      "\n",
      "    def start(self, refresh: bool=False) -> None:\n",
      "        'Start live rendering display.\\n\\n        Args:\\n            refresh (bool, optional): Also refresh. Defaults to False.\\n        '\n",
      "        with self._lock:\n",
      "            if self._started:\n",
      "                return\n",
      "            self.console.set_live(self)\n",
      "            self._started = True\n",
      "            if self._screen:\n",
      "                self._alt_screen = self.console.set_alt_screen(True)\n",
      "            self.console.show_cursor(False)\n",
      "            self._enable_redirect_io()\n",
      "            self.console.push_render_hook(self)\n",
      "            if refresh:\n",
      "                self.refresh()\n",
      "            if self.auto_refresh:\n",
      "                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)\n",
      "                self._refresh_thread.start()\n",
      "\n",
      "    def stop(self) -> None:\n",
      "        'Stop live rendering display.'\n",
      "        with self._lock:\n",
      "            if (not self._started):\n",
      "                return\n",
      "            self.console.clear_live()\n",
      "            self._started = False\n",
      "            if (self.auto_refresh and (self._refresh_thread is not None)):\n",
      "                self._refresh_thread.stop()\n",
      "                self._refresh_thread = None\n",
      "            self.vertical_overflow = 'visible'\n",
      "            with self.console:\n",
      "                try:\n",
      "                    if ((not self._alt_screen) and (not self.console.is_jupyter)):\n",
      "                        self.refresh()\n",
      "                finally:\n",
      "                    self._disable_redirect_io()\n",
      "                    self.console.pop_render_hook()\n",
      "                    if ((not self._alt_screen) and self.console.is_terminal):\n",
      "                        self.console.line()\n",
      "                    self.console.show_cursor(True)\n",
      "                    if self._alt_screen:\n",
      "                        self.console.set_alt_screen(False)\n",
      "                    if (self.transient and (not self._alt_screen)):\n",
      "                        self.console.control(self._live_render.restore_cursor())\n",
      "                    if ((self.ipy_widget is not None) and self.transient):\n",
      "                        self.ipy_widget.close()\n",
      "\n",
      "    def __enter__(self) -> 'Live':\n",
      "        self.start(refresh=(self._renderable is not None))\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:\n",
      "        self.stop()\n",
      "\n",
      "    def _enable_redirect_io(self) -> None:\n",
      "        'Enable redirecting of stdout / stderr.'\n",
      "        if (self.console.is_terminal or self.console.is_jupyter):\n",
      "            if (self._redirect_stdout and (not isinstance(sys.stdout, FileProxy))):\n",
      "                self._restore_stdout = sys.stdout\n",
      "                sys.stdout = cast('TextIO', FileProxy(self.console, sys.stdout))\n",
      "            if (self._redirect_stderr and (not isinstance(sys.stderr, FileProxy))):\n",
      "                self._restore_stderr = sys.stderr\n",
      "                sys.stderr = cast('TextIO', FileProxy(self.console, sys.stderr))\n",
      "\n",
      "    def _disable_redirect_io(self) -> None:\n",
      "        'Disable redirecting of stdout / stderr.'\n",
      "        if self._restore_stdout:\n",
      "            sys.stdout = cast('TextIO', self._restore_stdout)\n",
      "            self._restore_stdout = None\n",
      "        if self._restore_stderr:\n",
      "            sys.stderr = cast('TextIO', self._restore_stderr)\n",
      "            self._restore_stderr = None\n",
      "\n",
      "    @property\n",
      "    def renderable(self) -> RenderableType:\n",
      "        'Get the renderable that is being displayed\\n\\n        Returns:\\n            RenderableType: Displayed renderable.\\n        '\n",
      "        renderable = self.get_renderable()\n",
      "        return (Screen(renderable) if self._alt_screen else renderable)\n",
      "\n",
      "    def update(self, renderable: RenderableType, *, refresh: bool=False) -> None:\n",
      "        'Update the renderable that is being displayed\\n\\n        Args:\\n            renderable (RenderableType): New renderable to use.\\n            refresh (bool, optional): Refresh the display. Defaults to False.\\n        '\n",
      "        with self._lock:\n",
      "            self._renderable = renderable\n",
      "            if refresh:\n",
      "                self.refresh()\n",
      "\n",
      "    def refresh(self) -> None:\n",
      "        'Update the display of the Live Render.'\n",
      "        with self._lock:\n",
      "            self._live_render.set_renderable(self.renderable)\n",
      "            if self.console.is_jupyter:\n",
      "                try:\n",
      "                    from IPython.display import display\n",
      "                    from ipywidgets import Output\n",
      "                except ImportError:\n",
      "                    import warnings\n",
      "                    warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
      "                else:\n",
      "                    if (self.ipy_widget is None):\n",
      "                        self.ipy_widget = Output()\n",
      "                        display(self.ipy_widget)\n",
      "                    with self.ipy_widget:\n",
      "                        self.ipy_widget.clear_output(wait=True)\n",
      "                        self.console.print(self._live_render.renderable)\n",
      "            elif (self.console.is_terminal and (not self.console.is_dumb_terminal)):\n",
      "                with self.console:\n",
      "                    self.console.print(Control())\n",
      "            elif ((not self._started) and (not self.transient)):\n",
      "                with self.console:\n",
      "                    self.console.print(Control())\n",
      "\n",
      "    def process_renderables(self, renderables: List[ConsoleRenderable]) -> List[ConsoleRenderable]:\n",
      "        'Process renderables to restore cursor and display progress.'\n",
      "        self._live_render.vertical_overflow = self.vertical_overflow\n",
      "        if self.console.is_interactive:\n",
      "            with self._lock:\n",
      "                reset = (Control.home() if self._alt_screen else self._live_render.position_cursor())\n",
      "                renderables = [reset, *renderables, self._live_render]\n",
      "        elif ((not self._started) and (not self.transient)):\n",
      "            renderables = [*renderables, self._live_render]\n",
      "        return renderables\n",
      "if (__name__ == '__main__'):\n",
      "    import random\n",
      "    import time\n",
      "    from itertools import cycle\n",
      "    from typing import Dict, List, Tuple\n",
      "    from .align import Align\n",
      "    from .console import Console\n",
      "    from .live import Live as Live\n",
      "    from .panel import Panel\n",
      "    from .rule import Rule\n",
      "    from .syntax import Syntax\n",
      "    from .table import Table\n",
      "    console = Console()\n",
      "    syntax = Syntax('def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\\n    \"\"\"Iterate and generate a tuple with a flag for last value.\"\"\"\\n    iter_values = iter(values)\\n    try:\\n        previous_value = next(iter_values)\\n    except StopIteration:\\n        return\\n    for value in iter_values:\\n        yield False, previous_value\\n        previous_value = value\\n    yield True, previous_value', 'python', line_numbers=True)\n",
      "    table = Table('foo', 'bar', 'baz')\n",
      "    table.add_row('1', '2', '3')\n",
      "    progress_renderables = ['You can make the terminal shorter and taller to see the live table hideText may be printed while the progress bars are rendering.', Panel('In fact, [i]any[/i] renderable will work'), 'Such as [magenta]tables[/]...', table, 'Pretty printed structures...', {'type': 'example', 'text': 'Pretty printed'}, 'Syntax...', syntax, Rule('Give it a try!')]\n",
      "    examples = cycle(progress_renderables)\n",
      "    exchanges = ['SGD', 'MYR', 'EUR', 'USD', 'AUD', 'JPY', 'CNH', 'HKD', 'CAD', 'INR', 'DKK', 'GBP', 'RUB', 'NZD', 'MXN', 'IDR', 'TWD', 'THB', 'VND']\n",
      "    with Live(console=console) as live_table:\n",
      "        exchange_rate_dict: Dict[(Tuple[(str, str)], float)] = {}\n",
      "        for index in range(100):\n",
      "            select_exchange = exchanges[(index % len(exchanges))]\n",
      "            for exchange in exchanges:\n",
      "                if (exchange == select_exchange):\n",
      "                    continue\n",
      "                time.sleep(0.4)\n",
      "                if (random.randint(0, 10) < 1):\n",
      "                    console.log(next(examples))\n",
      "                exchange_rate_dict[(select_exchange, exchange)] = (200 / ((random.random() * 320) + 1))\n",
      "                if (len(exchange_rate_dict) > (len(ex\n",
      "\n",
      "=====================7-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import sys\n",
      "from threading import Event, RLock, Thread\n",
      "from types import TracebackType\n",
      "from typing import IO, Any, Callable, List, Optional, TextIO, Type, cast\n",
      "from . import get_console\n",
      "from .console import Console, ConsoleRenderable, RenderableType, RenderHook\n",
      "from .control import Control\n",
      "from .file_proxy import FileProxy\n",
      "from .jupyter import JupyterMixin\n",
      "from .live_render import LiveRender, VerticalOverflowMethod\n",
      "from .screen import Screen\n",
      "from .text import Text\n",
      "\n",
      "class _RefreshThread(Thread):\n",
      "    'A thread that calls refresh() at regular intervals.'\n",
      "\n",
      "    def __init__(self, live: 'Live', refresh_per_second: float) -> None:\n",
      "        self.live = live\n",
      "        self.refresh_per_second = refresh_per_second\n",
      "        self.done = Event()\n",
      "        super().__init__(daemon=True)\n",
      "\n",
      "    def stop(self) -> None:\n",
      "        self.done.set()\n",
      "\n",
      "    def run(self) -> None:\n",
      "        while (not self.done.wait((1 / self.refresh_per_second))):\n",
      "            with self.live._lock:\n",
      "                if (not self.done.is_set()):\n",
      "                    self.live.refresh()\n",
      "\n",
      "class Live(JupyterMixin, RenderHook):\n",
      "    'Renders an auto-updating live display of any given renderable.\\n\\n    Args:\\n        renderable (RenderableType, optional): The renderable to live display. Defaults to displaying nothing.\\n        console (Console, optional): Optional Console instance. Default will an internal Console instance writing to stdout.\\n        screen (bool, optional): Enable alternate screen mode. Defaults to False.\\n        auto_refresh (bool, optional): Enable auto refresh. If disabled, you will need to call `refresh()` or `update()` with refresh flag. Defaults to True\\n        refresh_per_second (float, optional): Number of times per second to refresh the live display. Defaults to 4.\\n        transient (bool, optional): Clear the renderable on exit (has no effect when screen=True). Defaults to False.\\n        redirect_stdout (bool, optional): Enable redirection of stdout, so ``print`` may be used. Defaults to True.\\n        redirect_stderr (bool, optional): Enable redirection of stderr. Defaults to True.\\n        vertical_overflow (VerticalOverflowMethod, optional): How to handle renderable when it is too tall for the console. Defaults to \"ellipsis\".\\n        get_renderable (Callable[[], RenderableType], optional): Optional callable to get renderable. Defaults to None.\\n    '\n",
      "\n",
      "    def __init__(self, renderable: Optional[RenderableType]=None, *, console: Optional[Console]=None, screen: bool=False, auto_refresh: bool=True, refresh_per_second: float=4, transient: bool=False, redirect_stdout: bool=True, redirect_stderr: bool=True, vertical_overflow: VerticalOverflowMethod='ellipsis', get_renderable: Optional[Callable[([], RenderableType)]]=None) -> None:\n",
      "        assert (refresh_per_second > 0), 'refresh_per_second must be > 0'\n",
      "        self._renderable = renderable\n",
      "        self.console = (console if (console is not None) else get_console())\n",
      "        self._screen = screen\n",
      "        self._alt_screen = False\n",
      "        self._redirect_stdout = redirect_stdout\n",
      "        self._redirect_stderr = redirect_stderr\n",
      "        self._restore_stdout: Optional[IO[str]] = None\n",
      "        self._restore_stderr: Optional[IO[str]] = None\n",
      "        self._lock = RLock()\n",
      "        self.ipy_widget: Optional[Any] = None\n",
      "        self.auto_refresh = auto_refresh\n",
      "        self._started: bool = False\n",
      "        self.transient = (True if screen else transient)\n",
      "        self._refresh_thread: Optional[_RefreshThread] = None\n",
      "        self.refresh_per_second = refresh_per_second\n",
      "        self.vertical_overflow = vertical_overflow\n",
      "        self._get_renderable = get_renderable\n",
      "        self._live_render = LiveRender(self.get_renderable(), vertical_overflow=vertical_overflow)\n",
      "\n",
      "    @property\n",
      "    def is_started(self) -> bool:\n",
      "        'Check if live display has been started.'\n",
      "        return self._started\n",
      "\n",
      "    def get_renderable(self) -> RenderableType:\n",
      "        renderable = (self._get_renderable() if (self._get_renderable is not None) else self._renderable)\n",
      "        return (renderable or '')\n",
      "\n",
      "    def start(self, refresh: bool=False) -> None:\n",
      "        'Start live rendering display.\\n\\n        Args:\\n            refresh (bool, optional): Also refresh. Defaults to False.\\n        '\n",
      "        with self._lock:\n",
      "            if self._started:\n",
      "                return\n",
      "            self.console.set_live(self)\n",
      "            self._started = True\n",
      "            if self._screen:\n",
      "                self._alt_screen = self.console.set_alt_screen(True)\n",
      "            self.console.show_cursor(False)\n",
      "            self._enable_redirect_io()\n",
      "            self.console.push_render_hook(self)\n",
      "            if refresh:\n",
      "                self.refresh()\n",
      "            if self.auto_refresh:\n",
      "                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)\n",
      "                self._refresh_thread.start()\n",
      "\n",
      "    def stop(self) -> None:\n",
      "        'Stop live rendering display.'\n",
      "        with self._lock:\n",
      "            if (not self._started):\n",
      "                return\n",
      "            self.console.clear_live()\n",
      "            self._started = False\n",
      "            if (self.auto_refresh and (self._refresh_thread is not None)):\n",
      "                self._refresh_thread.stop()\n",
      "                self._refresh_thread = None\n",
      "            self.vertical_overflow = 'visible'\n",
      "            with self.console:\n",
      "                try:\n",
      "                    if ((not self._alt_screen) and (not self.console.is_jupyter)):\n",
      "                        self.refresh()\n",
      "                finally:\n",
      "                    self._disable_redirect_io()\n",
      "                    self.console.pop_render_hook()\n",
      "                    if ((not self._alt_screen) and self.console.is_terminal):\n",
      "                        self.console.line()\n",
      "                    self.console.show_cursor(True)\n",
      "                    if self._alt_screen:\n",
      "                        self.console.set_alt_screen(False)\n",
      "                    if (self.transient and (not self._alt_screen)):\n",
      "                        self.console.control(self._live_render.restore_cursor())\n",
      "                    if ((self.ipy_widget is not None) and self.transient):\n",
      "                        self.ipy_widget.close()\n",
      "\n",
      "    def __enter__(self) -> 'Live':\n",
      "        self.start(refresh=(self._renderable is not None))\n",
      "        return self\n",
      "\n",
      "    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:\n",
      "        self.stop()\n",
      "\n",
      "    def _enable_redirect_io(self) -> None:\n",
      "        'Enable redirecting of stdout / stderr.'\n",
      "        if (self.console.is_terminal or self.console.is_jupyter):\n",
      "            if (self._redirect_stdout and (not isinstance(sys.stdout, FileProxy))):\n",
      "                self._restore_stdout = sys.stdout\n",
      "                sys.stdout = cast('TextIO', FileProxy(self.console, sys.stdout))\n",
      "            if (self._redirect_stderr and (not isinstance(sys.stderr, FileProxy))):\n",
      "                self._restore_stderr = sys.stderr\n",
      "                sys.stderr = cast('TextIO', FileProxy(self.console, sys.stderr))\n",
      "\n",
      "    def _disable_redirect_io(self) -> None:\n",
      "        'Disable redirecting of stdout / stderr.'\n",
      "        if self._restore_stdout:\n",
      "            sys.stdout = cast('TextIO', self._restore_stdout)\n",
      "            self._restore_stdout = None\n",
      "        if self._restore_stderr:\n",
      "            sys.stderr = cast('TextIO', self._restore_stderr)\n",
      "            self._restore_stderr = None\n",
      "\n",
      "    @property\n",
      "    def renderable(self) -> RenderableType:\n",
      "        'Get the renderable that is being displayed\\n\\n        Returns:\\n            RenderableType: Displayed renderable.\\n        '\n",
      "        renderable = self.get_renderable()\n",
      "        return (Screen(renderable) if self._alt_screen else renderable)\n",
      "\n",
      "    def update(self, renderable: RenderableType, *, refresh: bool=False) -> None:\n",
      "        'Update the renderable that is being displayed\\n\\n        Args:\\n            renderable (RenderableType): New renderable to use.\\n            refresh (bool, optional): Refresh the display. Defaults to False.\\n        '\n",
      "        with self._lock:\n",
      "            self._renderable = renderable\n",
      "            if refresh:\n",
      "                self.refresh()\n",
      "\n",
      "    def refresh(self) -> None:\n",
      "        'Update the display of the Live Render.'\n",
      "        with self._lock:\n",
      "            self._live_render.set_renderable(self.renderable)\n",
      "            if self.console.is_jupyter:\n",
      "                try:\n",
      "                    from IPython.display import display\n",
      "                    from ipywidgets import Output\n",
      "                except ImportError:\n",
      "                    import warnings\n",
      "                    warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
      "                else:\n",
      "                    if (self.ipy_widget is None):\n",
      "                        self.ipy_widget = Output()\n",
      "                        display(self.ipy_widget)\n",
      "                    with self.ipy_widget:\n",
      "                        self.ipy_widget.clear_output(wait=True)\n",
      "                        self.console.print(self._live_render.renderable)\n",
      "            elif (self.console.is_terminal and (not self.console.is_dumb_terminal)):\n",
      "                with self.console:\n",
      "                    self.console.print(Control())\n",
      "            elif ((not self._started) and (not self.transient)):\n",
      "                with self.console:\n",
      "                    self.console.print(Control())\n",
      "\n",
      "    def process_renderables(self, renderables: List[ConsoleRenderable]) -> List[ConsoleRenderable]:\n",
      "        'Process renderables to restore cursor and display progress.'\n",
      "        self._live_render.vertical_overflow = self.vertical_overflow\n",
      "        if self.console.is_interactive:\n",
      "            with self._lock:\n",
      "                reset = (Control.home() if self._alt_screen else self._live_render.position_cursor())\n",
      "                renderables = [reset, *renderables, self._live_render]\n",
      "        elif ((not self._started) and (not self.transient)):\n",
      "            renderables = [*renderables, self._live_render]\n",
      "        return renderables\n",
      "if (__name__ == '__main__'):\n",
      "    import random\n",
      "    import time\n",
      "    from itertools import cycle\n",
      "    from typing import Dict, List, Tuple\n",
      "    from .align import Align\n",
      "    from .console import Console\n",
      "    from .live import Live as Live\n",
      "    from .panel import Panel\n",
      "    from .rule import Rule\n",
      "    from .syntax import Syntax\n",
      "    from .table import Table\n",
      "    console = Console()\n",
      "    syntax = Syntax('def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\\n    \"\"\"Iterate and generate a tuple with a flag for last value.\"\"\"\\n    iter_values = iter(values)\\n    try:\\n        previous_value = next(iter_values)\\n    except StopIteration:\\n        return\\n    for value in iter_values:\\n        yield False, previous_value\\n        previous_value = value\\n    yield True, previous_value', 'python', line_numbers=True)\n",
      "    table = Table('foo', 'bar', 'baz')\n",
      "    table.add_row('1', '2', '3')\n",
      "    progress_renderables = ['You can make the terminal shorter and taller to see the live table hideText may be printed while the progress bars are rendering.', Panel('In fact, [i]any[/i] renderable will work'), 'Such as [magenta]tables[/]...', table, 'Pretty printed structures...', {'type': 'example', 'text': 'Pretty printed'}, 'Syntax...', syntax, Rule('Give it a try!')]\n",
      "    examples = cycle(progress_renderables)\n",
      "    exchanges = ['SGD', 'MYR', 'EUR', 'USD', 'AUD', 'JPY', 'CNH', 'HKD', 'CAD', 'INR', 'DKK', 'GBP', 'RUB', 'NZD', 'MXN', 'IDR', 'TWD', 'THB', 'VND']\n",
      "    with Live(console=console) as live_table:\n",
      "        exchange_rate_dict: Dict[(Tuple[(str, str)], float)] = {}\n",
      "        for index in range(100):\n",
      "            select_exchange = exchanges[(index % len(exchanges))]\n",
      "            for exchange in exchanges:\n",
      "                if (exchange == select_exchange):\n",
      "                    continue\n",
      "                time.sleep(0.4)\n",
      "                if (random.randint(0, 10) < 1):\n",
      "                    console.log(next(examples))\n",
      "                exchange_rate_dict[(select_exchange, exchange)] = (200 / ((random.random() * 320) + 1))\n",
      "                if (len(exchange_rate_dict) > (len(exchanges) - 1)):\n",
      "                    exchange_rate_dict.pop(list(exchange_rate_dict.keys())[0])\n",
      "                table = Table(title='Exchange Rates')\n",
      "                table.add_column('Source Currency')\n",
      "                table.add_column('Destination Currency')\n",
      "                table.add_column('Exchange Rate')\n",
      "                for ((source, dest), exchange_rate) in exchange_rate_dict.items():\n",
      "                    table.add_row(source, dest, Text(f'{exchange_rate:.4f}', style=('red' if (exchange_rate < 1.0) else 'green')))\n",
      "                live_table.update(Align.center(table))\n",
      "\n",
      "\n",
      "=====================7-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================8-INPUT=====================\n",
      "\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import shlex\n",
      "extensions = ['sphinxcontrib.github_ribbon']\n",
      "templates_path = ['_templates']\n",
      "source_suffix = '.rst'\n",
      "master_doc = 'index'\n",
      "project = 'Janome'\n",
      "copyright = '2022, Tomoko Uchida'\n",
      "author = 'Tomoko Uchida'\n",
      "version = '0.4'\n",
      "language = 'en'\n",
      "exclude_patterns = ['_build']\n",
      "pygments_style = 'sphinx'\n",
      "todo_include_todos = False\n",
      "html_theme = 'haiku'\n",
      "html_title = ('%s v%s documentation (en)' % (project, version))\n",
      "html_favicon = '../img/bronze-25C9.png'\n",
      "html_static_path = ['_static']\n",
      "htmlhelp_basename = 'Janomedoc'\n",
      "github_ribbon_repo = 'mocobeta/janome'\n",
      "github_ribbon_position = 'right'\n",
      "github_ribbon_color = 'red'\n",
      "latex_elements = {}\n",
      "latex_documents = [(master_doc, 'Janome.tex', 'Janome Documentation', 'Tomoko Uchida', 'manual')]\n",
      "man_pages = [(master_doc, 'janome', 'Janome Documentation', [author], 1)]\n",
      "texinfo_documents = [(master_doc, 'Janome', 'Janome Documentation', author, 'Janome', 'One line description of project.', 'Miscellaneous')]\n",
      "\n",
      "def setup(app):\n",
      "    app.add_css_file('custom.css')\n",
      "\n",
      "\n",
      "=====================8-PREDICTION=====================\n",
      "\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import shlex\n",
      "extensions = ['sphinxcontrib.github_ribbon']\n",
      "templates_path = ['_templates']\n",
      "source_suffix = '.rst'\n",
      "master_doc = 'index'\n",
      "project = 'Janome'\n",
      "copyright = '2022, Tomoko Uchida'\n",
      "author = 'Tomoko Uchida'\n",
      "version = '0.4'\n",
      "language = 'en'\n",
      "exclude_patterns = ['_build']\n",
      "pygments_style = 'sphinx'\n",
      "todo_include_todos = False\n",
      "html_theme = 'haiku'\n",
      "html_title = ('%s v%s documentation (en)' % (project, version))\n",
      "html_favicon = '../img/bronze-25C9.png'\n",
      "html_\n",
      "\n",
      "=====================8-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import shlex\n",
      "extensions = ['sphinxcontrib.github_ribbon']\n",
      "templates_path = ['_templates']\n",
      "source_suffix = '.rst'\n",
      "master_doc = 'index'\n",
      "project = 'Janome'\n",
      "copyright = '2022, Tomoko Uchida'\n",
      "author = 'Tomoko Uchida'\n",
      "version = '0.4'\n",
      "language = 'en'\n",
      "exclude_patterns = ['_build']\n",
      "pygments_style = 'sphinx'\n",
      "todo_include_todos = False\n",
      "html_theme = 'haiku'\n",
      "html_title = ('%s v%s documentation (en)' % (project, version))\n",
      "html_favicon = '../img/bronze-25C9.png'\n",
      "html_static_path = ['_static']\n",
      "htmlhelp_basename = 'Janomedoc'\n",
      "github_ribbon_repo = 'mocobeta/janome'\n",
      "github_ribbon_position = 'right'\n",
      "github_ribbon_color = 'red'\n",
      "latex_elements = {}\n",
      "latex_documents = [(master_doc, 'Janome.tex', 'Janome Documentation', 'Tomoko Uchida', 'manual')]\n",
      "man_pages = [(master_doc, 'janome', 'Janome Documentation', [author], 1)]\n",
      "texinfo_documents = [(master_doc, 'Janome', 'Janome Documentation', author, 'Janome', 'One line description of project.', 'Miscellaneous')]\n",
      "\n",
      "def setup(app):\n",
      "    app.add_css_file('custom.css')\n",
      "\n",
      "\n",
      "=====================8-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================9-INPUT=====================\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "from __future__ import unicode_literals\n",
      "from lego_extract import RIFF, MxSt, MxOb, MxCh\n",
      "from glob import glob\n",
      "import struct\n",
      "import sys\n",
      "import logging\n",
      "import argparse\n",
      "import os\n",
      "try:\n",
      "    from StringIO import StringIO\n",
      "except:\n",
      "    from io import BytesIO as StringIO\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def dump(path):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Dumping {0}.'.format(filename))\n",
      "                riff = RIFF(f)\n",
      "                for subchunk in riff.subchunks():\n",
      "                    logger.info(subchunk)\n",
      "\n",
      "def extract(path, dest):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            import os\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Extracting {0}.'.format(filename))\n",
      "                try:\n",
      "                    os.mkdir(os.path.join(dest, os.path.basename(filename)))\n",
      "                except OSError as exc:\n",
      "                    pass\n",
      "                riff = RIFF(f)\n",
      "                d = {}\n",
      "                d2 = {}\n",
      "                subchunk_iter = riff.subchunks()\n",
      "                for subchunk in subchunk_iter:\n",
      "                    if isinstance(subchunk, MxOb):\n",
      "                        d[subchunk.id] = subchunk\n",
      "                    if isinstance(subchunk, MxCh):\n",
      "                        if (not (subchunk.id in d2)):\n",
      "                            d2[subchunk.id] = []\n",
      "                        d2[subchunk.id].append(subchunk)\n",
      "                for b in d.keys():\n",
      "                    print(b)\n",
      "                    mxob = d[b]\n",
      "                    print(mxob)\n",
      "                    if (mxob.s1 == 4):\n",
      "                        x = d2[b][0]\n",
      "                        try:\n",
      "                            import binascii\n",
      "                            data = StringIO(x.data[14:])\n",
      "                            data.read(4)\n",
      "                            bitrate1 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            bitrate2 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            idk = struct.unpack('<H', data.read(2))[0]\n",
      "                            bits = struct.unpack('<H', data.read(2))[0]\n",
      "                            logger.info(('Audio format might be %dHz or %dHz, with %d bits\\n' % (bitrate1, bitrate2, bits)))\n",
      "                        except Exception as exc:\n",
      "                            logger.error(exc)\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import wave\n",
      "                        w = wave.open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb')\n",
      "                        w.setnchannels(1)\n",
      "                        w.setframerate(int(bitrate1))\n",
      "                        w.setsampwidth((int(bits) / 8))\n",
      "                        for x in d2[b][1:]:\n",
      "                            w.writeframesraw(x.data[14:])\n",
      "                        w.close()\n",
      "                    elif (mxob.s1 == 10):\n",
      "                        x = d2[b][0]\n",
      "                        '\\n                        000080020000c4000000010008000000000000ea0100130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        000080020000c4000000010008000000000000ea0100130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        00003900000013000000010008000000000074040000130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        '\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        import binascii\n",
      "                        print(binascii.hexlify(x.data[14:]))\n",
      "                        header_size = struct.unpack('<L', data.read(4))[0]\n",
      "                        width = struct.unpack('<L', data.read(4))[0]\n",
      "                        height = struct.unpack('<L', data.read(4))[0]\n",
      "                        number_of_color_planes = struct.unpack('<H', data.read(2))[0]\n",
      "                        number_of_bits_per_pixel = struct.unpack('<H', data.read(2))[0]\n",
      "                        compression = struct.unpack('<L', data.read(4))[0]\n",
      "                        image_size = struct.unpack('<L', data.read(4))[0]\n",
      "                        hor_res = struct.unpack('<L', data.read(4))[0]\n",
      "                        vert_res = struct.unpack('<L', data.read(4))[0]\n",
      "                        colors_in_palette = struct.unpack('<L', data.read(4))[0]\n",
      "                        important_colors = struct.unpack('<L', data.read(4))[0]\n",
      "                        logger.info('Header size {0} Width {1} Height {2} nofcp {3} nobpp {4} com {5} size {6} hor {7} vert {8} pal {9} impo {10}'.format(header_size, width, height, number_of_color_planes, number_of_bits_per_pixel, image_size, compression, hor_res, vert_res, colors_in_palette, important_colors))\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.bmp'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import os\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.bmp'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            f.write(b'BM')\n",
      "                            f.write(b'\\x00\\x00\\x00\\x00')\n",
      "                            f.write(b'\\x00\\x00')\n",
      "                            f.write(b'\\x00\\x00')\n",
      "                            f.write(struct.pack('<L', len(d2[b][0].data[14:])))\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                    elif (mxob.s1 == 4):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                    elif (mxob.s1 == 6):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 8):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 7):\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 11):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    else:\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                print('<<<<', str(mxob))\n",
      "if (__name__ == '__main__'):\n",
      "    logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=[logging.StreamHandler()])\n",
      "    import argparse\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('-v', '--verbose', help='verbose', action='store_true', default=False)\n",
      "    group = parser.add_argument_group(title='Extract contents')\n",
      "    group.add_argument('--extract', '-e', help='Extract contents', action='store_true')\n",
      "    group.add_argument('--output', help='Output folder')\n",
      "    group = parser.add_argument_group(title='Dump contents')\n",
      "    group.add_argument('--dump', '-d', help='Extract contents', action='store_true')\n",
      "    parser.add_argument('paths', nargs='+', help='Path to SI files')\n",
      "    args = parser.parse_args()\n",
      "    if args.dump:\n",
      "        dump(args.paths)\n",
      "    elif args.extract:\n",
      "        extract(args.paths, args.output)\n",
      "    else:\n",
      "        parser.print_help()\n",
      "\n",
      "\n",
      "=====================9-PREDICTION=====================\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "from __future__ import unicode_literals\n",
      "from lego_extract import RIFF, MxSt, MxOb, MxCh\n",
      "from glob import glob\n",
      "import struct\n",
      "import sys\n",
      "import logging\n",
      "import argparse\n",
      "import os\n",
      "try:\n",
      "    from StringIO import StringIO\n",
      "except:\n",
      "    from io import BytesIO as StringIO\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def dump(path):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Dumping {0}.'.format(filename))\n",
      "                riff = RIFF(f)\n",
      "                for subchunk in riff.subchunks():\n",
      "                    logger.info(subchunk)\n",
      "\n",
      "def extract(path, dest):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            import os\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Extracting {0}.'.format(filename))\n",
      "                try:\n",
      "                    os.mkdir(os.path.join(dest, os.path.basename(filename)))\n",
      "                except OSError as exc:\n",
      "                    pass\n",
      "                riff = RIFF(f)\n",
      "                d = {}\n",
      "                d2 = {}\n",
      "                subchunk_iter = riff.subchunks()\n",
      "                for subchunk in subchunk_iter:\n",
      "                    if isinstance(subchunk, MxOb):\n",
      "                        d[subchunk.id] = subchunk\n",
      "                    if isinstance(subchunk, MxCh):\n",
      "                        if (not (subchunk.id in d2)):\n",
      "                            d2[subchunk.id] = []\n",
      "                        d2[subchunk.id].append(subchunk)\n",
      "                for b in d.keys():\n",
      "                    print(b)\n",
      "                    mxob = d[b]\n",
      "                    print(mxob)\n",
      "                    if (mxob.s1 == 4):\n",
      "                        x = d2[b][0]\n",
      "                        try:\n",
      "                            import binascii\n",
      "                            data = StringIO(x.data[14:])\n",
      "                            data.read(4)\n",
      "                            bitrate1 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            bitrate2 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            idk = struct.unpack('<H', data.read(2))[0]\n",
      "                            bits = struct.unpack('<H', data.read(2))[0]\n",
      "                            logger.info(('Audio format might be %dHz or %dHz, with %d bits\\n' % (bitrate1, bitrate2, bits)))\n",
      "                        except Exception as exc:\n",
      "                            logger.error(exc)\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import wave\n",
      "                        w = wave.open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb')\n",
      "                        w.setnchannels(1)\n",
      "                        w.setframerate(int(bitrate1))\n",
      "                        w.setsampwidth((int(bits) / 8))\n",
      "                        for x in d2[b][1:]:\n",
      "                            w.writeframesraw(x.data[14:])\n",
      "                        w.close()\n",
      "                    elif (mxob.s1 == 10):\n",
      "                        x = d2[b][0]\n",
      "                        '\\n                        000080020000c4000000010008000000000000ea0100130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        000080020000c4000000010008000000000000ea0100130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        00003900000013000000010008000000000074040000130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        '\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        import binascii\n",
      "                        print(binascii.hexlify(x.data[14:]))\n",
      "                        header_size = struct.unpack('<L', data.read(4))[0]\n",
      "                        width = struct.unpack('<L', data.read(4))[0]\n",
      "                        height = struct.unpack('<L', data.read(4))[0]\n",
      "                        number_of_color_planes = struct.unpack('<H', data.read(2))[0]\n",
      "                        number_of_bits_per_pixel = struct.unpack('<H', data.read(2))[0]\n",
      "                        compression = struct.unpack('<L', data.read(4))[0]\n",
      "                        image_size = struct.unpack('<L', data.read(4))[0]\n",
      "                        hor_res = struct.unpack('<L', data.read(4))[0]\n",
      "                        vert_res = struct.unpack('<L', data.read(4))[0]\n",
      "                        colors_in_palette = struct.unpack('<L', data.read(4))[0]\n",
      "                        important_colors = struct.unpack('<L', data.read(4))[0]\n",
      "                        logger.info('Header size {0} Width {1} Height {2} nofcp {3} nobpp {4} com {5} size {6} hor {7} vert {8} pal {9} impo {10}'.format(header_size, width, height, number_of_color_planes, number_of_bits_per_pixel, image_size, compression, hor_res, vert_res, colors_in_palette, important_colors))\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.bmp'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import os\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.bmp'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            f.write(b'BM')\n",
      "                            f.write(b'\\x00\\x00\\x00\\x00')\n",
      "                            f.write(b'\\x00\\x00')\n",
      "                            f.write(b'\\x00\\x00')\n",
      "                            f.write(struct.pack('<L', len(d2[b][0].data[14:])))\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                    elif (mxob.s1 == 4):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                    elif (mxob.s1 == 6):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 8):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 7):\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 11):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    else:\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                print('<<<<', str(mxob))\n",
      "if (__name__ == '__main__'):\n",
      "    logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=[logging.StreamHandler()])\n",
      "    import argparse\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('-v', '--verbose', help='verbose', action='store_true', default=False)\n",
      "    group = parser.add_argument_group(title='Extract contents')\n",
      "    group.add_argument('--extract', '-e', help='Extract contents', action='store_true')\n",
      "    group.add_argument('--output', help='Output folder')\n",
      "    group = parser.add_argument_group(title='Dump contents')\n",
      "    group.add_argument('--dump', '-d', help='Extract contents', action='store_true')\n",
      "    parser.add_argument('paths', nargs='+', help='Path to SI files')\n",
      "    args = parser.parse_args()\n",
      "    if args.dump:\n",
      "        dump(args.paths)\n",
      "    elif args.extract:\n",
      "        extract(args.paths, args.output)\n",
      "    else:\n",
      "        parser.print_help()\n",
      "\u001b[31m\n",
      "\n",
      "Original code:\n",
      "\n",
      "from __future__ import print_function\n",
      "from __future__ import unicode_literals\n",
      "from lego_extract import RIFF, MxSt, MxOb, MxCh\n",
      "from glob import glob\n",
      "import struct\n",
      "import sys\n",
      "import logging\n",
      "import argparse\n",
      "import os\n",
      "try:\n",
      "    from StringIO import StringIO\n",
      "except:\n",
      "    from io import BytesIO as StringIO\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def dump(path):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Dumping {0}.'.format(filename))\n",
      "                riff = RIFF(f)\n",
      "                for subchunk in riff.subchunks():\n",
      "                    logger.info(subchunk)\n",
      "\n",
      "def extract(path, dest):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            import os\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Extracting {0}.'.format(filename))\n",
      "                try:\n",
      "                    os.mkdir(os.path.join(dest, os.path.basename(filename)))\n",
      "                except OSError as exc:\n",
      "                    pass\n",
      "                riff = RIFF(f)\n",
      "                d = {}\n",
      "                d2 = {}\n",
      "                subchunk_iter = riff.subchunks()\n",
      "                for subchunk in subchunk_iter:\n",
      "                    if isinstance(subchunk, MxOb):\n",
      "                        d[subchunk.id] = subchunk\n",
      "                    if isinstance(subchunk, MxCh):\n",
      "                        if (not (subchunk.id in d2)):\n",
      "                            d2[subchunk.id] = []\n",
      "                        d2[subchunk.id].append(subchunk)\n",
      "                for b in d.keys():\n",
      "                    print(b)\n",
      "                    mxob = d[b]\n",
      "                    print(mxob)\n",
      "                    if (mxob.s1 == 4):\n",
      "                        x = d2[b][0]\n",
      "                        try:\n",
      "                            import binascii\n",
      "                            data = StringIO(x.data[14:])\n",
      "                            data.read(4)\n",
      "                            bitrate1 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            bitrate2 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            idk = struct.unpack('<H', data.read(2))[0]\n",
      "                            bits = struct.unpack('<H', data.read(2))[0]\n",
      "                            logger.info(('Audio format might be %dHz or %dHz, with %d bits\\n' % (bitrate1, bitrate2, bits)))\n",
      "                        except Exception as exc:\n",
      "                            logger.error(exc)\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import wave\n",
      "                        w = wave.open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb')\n",
      "                        w.setnchannels(1)\n",
      "                        w.setframerate(int(bitrate1))\n",
      "                        w.setsampwidth((int(bits) / 8))\n",
      "                        for x in d2[b][1:]:\n",
      "                            w.writeframesraw(x.data[14:])\n",
      "                        w.close()\n",
      "                    elif (mxob.\u001b[0m\n",
      "\n",
      "=====================9-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "from __future__ import unicode_literals\n",
      "from lego_extract import RIFF, MxSt, MxOb, MxCh\n",
      "from glob import glob\n",
      "import struct\n",
      "import sys\n",
      "import logging\n",
      "import argparse\n",
      "import os\n",
      "try:\n",
      "    from StringIO import StringIO\n",
      "except:\n",
      "    from io import BytesIO as StringIO\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def dump(path):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Dumping {0}.'.format(filename))\n",
      "                riff = RIFF(f)\n",
      "                for subchunk in riff.subchunks():\n",
      "                    logger.info(subchunk)\n",
      "\n",
      "def extract(path, dest):\n",
      "    for path in args.paths:\n",
      "        for filename in glob(path):\n",
      "            import os\n",
      "            with open(filename, 'rb') as f:\n",
      "                logger.info('Extracting {0}.'.format(filename))\n",
      "                try:\n",
      "                    os.mkdir(os.path.join(dest, os.path.basename(filename)))\n",
      "                except OSError as exc:\n",
      "                    pass\n",
      "                riff = RIFF(f)\n",
      "                d = {}\n",
      "                d2 = {}\n",
      "                subchunk_iter = riff.subchunks()\n",
      "                for subchunk in subchunk_iter:\n",
      "                    if isinstance(subchunk, MxOb):\n",
      "                        d[subchunk.id] = subchunk\n",
      "                    if isinstance(subchunk, MxCh):\n",
      "                        if (not (subchunk.id in d2)):\n",
      "                            d2[subchunk.id] = []\n",
      "                        d2[subchunk.id].append(subchunk)\n",
      "                for b in d.keys():\n",
      "                    print(b)\n",
      "                    mxob = d[b]\n",
      "                    print(mxob)\n",
      "                    if (mxob.s1 == 4):\n",
      "                        x = d2[b][0]\n",
      "                        try:\n",
      "                            import binascii\n",
      "                            data = StringIO(x.data[14:])\n",
      "                            data.read(4)\n",
      "                            bitrate1 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            bitrate2 = struct.unpack('<H', data.read(2))[0]\n",
      "                            data.read(2)\n",
      "                            idk = struct.unpack('<H', data.read(2))[0]\n",
      "                            bits = struct.unpack('<H', data.read(2))[0]\n",
      "                            logger.info(('Audio format might be %dHz or %dHz, with %d bits\\n' % (bitrate1, bitrate2, bits)))\n",
      "                        except Exception as exc:\n",
      "                            logger.error(exc)\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import wave\n",
      "                        w = wave.open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.wav'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb')\n",
      "                        w.setnchannels(1)\n",
      "                        w.setframerate(int(bitrate1))\n",
      "                        w.setsampwidth((int(bits) / 8))\n",
      "                        for x in d2[b][1:]:\n",
      "                            w.writeframesraw(x.data[14:])\n",
      "                        w.close()\n",
      "                    elif (mxob.s1 == 10):\n",
      "                        x = d2[b][0]\n",
      "                        '\\n                        000080020000c4000000010008000000000000ea0100130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        000080020000c4000000010008000000000000ea0100130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        00003900000013000000010008000000000074040000130b0000130b00000000000000000000ff00ff0000008000008000000080800080000000800080008080000080808000\\n                        '\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        import binascii\n",
      "                        print(binascii.hexlify(x.data[14:]))\n",
      "                        header_size = struct.unpack('<L', data.read(4))[0]\n",
      "                        width = struct.unpack('<L', data.read(4))[0]\n",
      "                        height = struct.unpack('<L', data.read(4))[0]\n",
      "                        number_of_color_planes = struct.unpack('<H', data.read(2))[0]\n",
      "                        number_of_bits_per_pixel = struct.unpack('<H', data.read(2))[0]\n",
      "                        compression = struct.unpack('<L', data.read(4))[0]\n",
      "                        image_size = struct.unpack('<L', data.read(4))[0]\n",
      "                        hor_res = struct.unpack('<L', data.read(4))[0]\n",
      "                        vert_res = struct.unpack('<L', data.read(4))[0]\n",
      "                        colors_in_palette = struct.unpack('<L', data.read(4))[0]\n",
      "                        important_colors = struct.unpack('<L', data.read(4))[0]\n",
      "                        logger.info('Header size {0} Width {1} Height {2} nofcp {3} nobpp {4} com {5} size {6} hor {7} vert {8} pal {9} impo {10}'.format(header_size, width, height, number_of_color_planes, number_of_bits_per_pixel, image_size, compression, hor_res, vert_res, colors_in_palette, important_colors))\n",
      "                        import os\n",
      "                        logger.info(('Exporting wav to ' + os.path.join(dest, os.path.basename(filename), '{0}_{1}.bmp'.format(str(mxob.id), os.path.basename(mxob.s3)))))\n",
      "                        import os\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.bmp'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            f.write(b'BM')\n",
      "                            f.write(b'\\x00\\x00\\x00\\x00')\n",
      "                            f.write(b'\\x00\\x00')\n",
      "                            f.write(b'\\x00\\x00')\n",
      "                            f.write(struct.pack('<L', len(d2[b][0].data[14:])))\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                    elif (mxob.s1 == 4):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                    elif (mxob.s1 == 6):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 8):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 7):\n",
      "                        pass\n",
      "                    elif (mxob.s1 == 11):\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                    else:\n",
      "                        import os\n",
      "                        x = d2[b][0]\n",
      "                        data = StringIO(x.data[14:])\n",
      "                        with open(os.path.join(dest, os.path.basename(filename), '{0}_{1}.unknown'.format(str(mxob.id), os.path.basename(mxob.s3))), 'wb') as f:\n",
      "                            for x in d2[b]:\n",
      "                                f.write(x.data[14:])\n",
      "                        pass\n",
      "                print('<<<<', str(mxob))\n",
      "if (__name__ == '__main__'):\n",
      "    logging.basicConfig(level=logging.DEBUG, format='%(message)s', handlers=[logging.StreamHandler()])\n",
      "    import argparse\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('-v', '--verbose', help='verbose', action='store_true', default=False)\n",
      "    group = parser.add_argument_group(title='Extract contents')\n",
      "    group.add_argument('--extract', '-e', help='Extract contents', action='store_true')\n",
      "    group.add_argument('--output', help='Output folder')\n",
      "    group = parser.add_argument_group(title='Dump contents')\n",
      "    group.add_argument('--dump', '-d', help='Extract contents', action='store_true')\n",
      "    parser.add_argument('paths', nargs='+', help='Path to SI files')\n",
      "    args = parser.parse_args()\n",
      "    if args.dump:\n",
      "        dump(args.paths)\n",
      "    elif args.extract:\n",
      "        extract(args.paths, args.output)\n",
      "    else:\n",
      "        parser.print_help()\n",
      "\n",
      "\n",
      "=====================9-DIFF_BLEU=====================\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "lookup_examples(codex_list_comp_report, 0.01, 0, metric=\"diff_bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbbbb7-9d28-4146-b3fb-174176b08393",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CodeT5 Individual Finetuned - Short Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b86b7-106c-485f-84c5-65e12bf89bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drwxr-xr-x 27 cting3 grads 4096 Aug  5 12:15 outlier_fixed_list_comp_codet5small\n",
    "# drwxr-xr-x 32 cting3 grads 4096 Aug  5 11:34 outlier_updated_docstring_codet5small\n",
    "# drwxr-xr-x 25 cting3 grads 4096 Jul 28 14:36 outlier_codet5small\n",
    "# drwxr-xr-x 32 cting3 grads 4096 Jul 28 14:18 outlier_casing_codet5small\n",
    "# drwxr-xr-x 32 cting3 grads 4096 Jul 24 15:47 outlier_class_codet5small\n",
    "\n",
    "# checkpoint-12000\n",
    "# checkpoint-85500\n",
    "# checkpoint-40500\n",
    "# checkpoint-27000\n",
    "# checkpoint-49000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79704290-85f5-46ca-92fc-29aee0499adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38c46d11-464f-44e9-81cf-cabeeb9798b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a65c6-49fa-44eb-9790-43f2699257c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_casing_codet5small\"\n",
    "ckpt = \"checkpoint-27000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc274519-b5f6-4013-ad8f-efd176bab3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "pred_csvfile = \"codet5_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"casing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9160b7a-b0f0-49d2-8263-fa8106ba6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codebleu score\n",
    "casing_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "casing_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bacb8-a5d9-4a09-b0b3-4a59d31168bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting prediction\n",
    "casing_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c54e9b-0774-40d9-ac13-4cc645d891c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the score report, containing all the eval metrics\n",
    "casing_report = evaluate_pred_df(casing_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c22e0-f301-4d83-b3b8-9fdada511a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can use this function for looking up the example\n",
    "# you can search with score upper bound and lower bound with a certain eval metric\n",
    "# and you can also pick how many examples you want to see by using `count` argument\n",
    "lookup_examples(casing_report, 0.6, 0.4, metric=\"diff_bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b8b20-c561-4eab-8f20-6a9a2f00526d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ccf0e-2c1d-4265-a095-a2cc054b3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_class_codet5small\"\n",
    "ckpt = \"checkpoint-49000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bad0d2-b7de-4b19-ac46-e724eb58b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f073b3-ab12-4f1d-b547-b306d32fd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "class_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3eb868-7eef-433d-9bff-43d944f67a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718e70d-5372-457e-87bc-67fdce959b73",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_report = evaluate_pred_df(class_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80b751-8293-4d70-b2d9-f982e2480527",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in class_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489a7ad-028c-49b9-908a-eb6255abf044",
   "metadata": {
    "tags": []
   },
   "source": [
    "## list_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da0ca4-d518-48cb-ab65-8a37a14dbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_fixed_list_comp_codet5small\"\n",
    "ckpt = \"checkpoint-12000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442f059-4541-498a-9fd3-0431a2d23a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"list_comp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280bbf8-c804-4dbf-9771-aa419418b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25', is_exclude_same_io=True)\n",
    "list_comp_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a83a65-0310-46c2-9f3a-882cfd30e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp_pred_df = pd.read_csv(file_name)\n",
    "list_comp_pred_df = exclude_same_io(list_comp_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0c209-371d-4e8f-896d-e00911f40f69",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_comp_report = evaluate_pred_df(list_comp_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85faa6c4-3a58-4526-a140-593300e8ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in list_comp_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96ae7e-743e-4e24-860e-383f1452d24a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lookup_examples(list_comp_report, 1, 1, metric=\"diff_bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487ce7d-e26e-4714-8f72-8c7a04236797",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a22e6d-4f4c-40b2-8b6f-94bf287ac000",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_updated_docstring_codet5small\"\n",
    "ckpt = \"checkpoint-85500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc838a-c1ee-4cb2-804e-eca495b9fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"docstring\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52ec6f-6f8e-4691-945b-e59f7d2740cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "docstring_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc93304-2f4f-4254-b2e0-8e0a5052dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a923269-8f86-496f-b0f8-0b38c2c8ca16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docstring_report = evaluate_pred_df(docstring_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424914e8-ed36-49bb-ae5c-a306b501bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in docstring_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb428fc-892a-4a5f-907c-b19c918c7fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c715e6c-04c3-4af9-bd60-1d1f73df9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_codet5small\"\n",
    "ckpt = \"checkpoint-40500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae028a-ace6-43ae-a863-9d117437b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e6dc5-d3f2-4bd6-ad5a-eaec87916a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25', dropna=True)\n",
    "comment_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfd33b-dcd1-4bb2-9a72-af3246c5773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692d530-4d71-47c4-ad18-86652117f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why there is nan in preds, but just exclude it no matter what\n",
    "comment_pred_df = comment_pred_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630835e-8d2c-46f3-818b-482c8b942575",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "comment_report = evaluate_pred_df(comment_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf6b39-b015-4a37-bd6d-7f9851fe92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in comment_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab53ab9-b179-4b4b-80d6-17e0052b2beb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CodeT5 Individual Finetuned - Truncated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125fb258-4b35-45af-940a-0c8d9b009614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drwxr-xr-x 27 cting3 grads 4096 Aug  5 12:15 outlier_fixed_list_comp_codet5small\n",
    "# drwxr-xr-x 32 cting3 grads 4096 Aug  5 11:34 outlier_updated_docstring_codet5small\n",
    "# drwxr-xr-x 25 cting3 grads 4096 Jul 28 14:36 outlier_codet5small\n",
    "# drwxr-xr-x 32 cting3 grads 4096 Jul 28 14:18 outlier_casing_codet5small\n",
    "# drwxr-xr-x 32 cting3 grads 4096 Jul 24 15:47 outlier_class_codet5small\n",
    "\n",
    "# checkpoint-12000\n",
    "# checkpoint-85500\n",
    "# checkpoint-40500\n",
    "# checkpoint-27000\n",
    "# checkpoint-49000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505345b-f440-42cc-a7af-a45f3586e859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54384f96-d0fd-463b-9c58-46e54cc3f309",
   "metadata": {
    "tags": []
   },
   "source": [
    "## casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59f3988-442b-4ca3-b9a3-a9f564545095",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_casing_codet5small\"\n",
    "ckpt = \"checkpoint-27000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "657e84af-b269-4e0d-89d1-c6be955a18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_eval_set_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"casing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1be04b-dd18-4881-b660-046757ca8a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.9502663480841979,\n",
       " 'weighted_ngram': 0.9550267101422462,\n",
       " 'syntax_match': 0.9901381003399783,\n",
       " 'dataflow_match': 0.9544287595353866,\n",
       " 'code_bleu': 0.9624649795254523}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casing_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "casing_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32b15f32-8c9a-4dcd-b1c7-e7c0af288ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "casing_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "564b1356-6d30-4791-96dc-4a18623c985b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b2891a7a73442abc2c4a1ff4e63996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/cting3/miniconda3/envs/py_3_8/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/users/cting3/miniconda3/envs/py_3_8/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/users/cting3/miniconda3/envs/py_3_8/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    }
   ],
   "source": [
    "casing_report = evaluate_pred_df(casing_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cc54817-62cf-49d7-9817-663c1ec16960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.4724017901541522\n",
      "codebleu_above_90 : 0.8547986076578816\n",
      "diff_bleu_avg : 0.5964453082218742\n",
      "diff_bleu_perfect : 0.45997016409746394\n",
      "diff_bleu_above_90 : 0.4733963202386872\n",
      "parse_test_accuracy : 0.4669318746892093\n"
     ]
    }
   ],
   "source": [
    "for key, val in casing_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfaecc-0c4d-4377-866a-7f0b034f09be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af3a6872-a2ee-4e46-a0c4-f498198957e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_class_codet5small\"\n",
    "ckpt = \"checkpoint-49000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba0d51a3-9e77-433d-97b2-f8ef364c12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_eval_set_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94e72bb-2c19-42ca-bca2-f0d91c5455ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.7575970498680648,\n",
       " 'weighted_ngram': 0.784300360086646,\n",
       " 'syntax_match': 0.8638880792771787,\n",
       " 'dataflow_match': 0.8626689507992045,\n",
       " 'code_bleu': 0.8171136100077735}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "class_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "056d4afd-c85e-49e0-90ae-503fcc41cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b127db31-120e-4603-9c0e-dc5173493d1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c0ecc46ada4adc8c53e1d5f8208b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    }
   ],
   "source": [
    "class_report = evaluate_pred_df(class_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6061a23d-b3e7-4d0a-84e3-d184a24c753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.08478802992518704\n",
      "codebleu_above_90 : 0.31022443890274315\n",
      "diff_bleu_avg : 0.29821448465689815\n",
      "diff_bleu_perfect : 0.08528678304239401\n",
      "diff_bleu_above_90 : 0.10274314214463841\n",
      "parse_test_accuracy : 0.4967581047381546\n"
     ]
    }
   ],
   "source": [
    "for key, val in class_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd3c10-7cdf-47ce-a708-5c07c63d2983",
   "metadata": {
    "tags": []
   },
   "source": [
    "## list_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43060ccc-f088-4ad6-b79a-1a3646ffce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_fixed_list_comp_codet5small\"\n",
    "ckpt = \"checkpoint-12000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47c69770-9376-4150-b5ec-3d40f20f7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_eval_set_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"list_comp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eee3b597-5424-4974-b37d-6de751e82681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.9750531654811976,\n",
       " 'weighted_ngram': 0.9777989820886288,\n",
       " 'syntax_match': 0.9729383740338027,\n",
       " 'dataflow_match': 0.9273258171790089,\n",
       " 'code_bleu': 0.9632790846956594}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_comp_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25', is_exclude_same_io=True)\n",
    "list_comp_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3cbb85a-7b2c-48df-91da-5dcc8d6bfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp_pred_df = pd.read_csv(file_name)\n",
    "list_comp_pred_df = exclude_same_io(list_comp_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e43d4d3-a528-4989-81a1-2b068e0c7eaa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392d3794701f4b9e98a281097099c567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_comp_report = evaluate_pred_df(list_comp_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5093226e-4d12-4e77-8d3a-b70e6c73770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.21200980392156862\n",
      "codebleu_above_90 : 0.8970588235294118\n",
      "diff_bleu_avg : 0.38956310338746586\n",
      "diff_bleu_perfect : 0.21200980392156862\n",
      "diff_bleu_above_90 : 0.21446078431372548\n",
      "parse_test_accuracy : 0.32598039215686275\n"
     ]
    }
   ],
   "source": [
    "for key, val in list_comp_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4831c-4420-4034-9db3-7bee7ee56dcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4293e548-40fc-4f5f-8dc5-873125cab982",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_updated_docstring_codet5small\"\n",
    "ckpt = \"checkpoint-85500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4437a03f-e472-481a-9e5d-5fde72a0e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_eval_set_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"docstring\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63e2b9a8-3ea9-4c01-8718-7d44ccfa608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.421175022475854,\n",
       " 'weighted_ngram': 0.5404834582717518,\n",
       " 'syntax_match': 0.8606034809804484,\n",
       " 'dataflow_match': 0.8820790293490737,\n",
       " 'code_bleu': 0.676085247769282}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstring_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "docstring_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fda5f614-cd91-4eb0-9f13-9f1c9c4a31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d067a346-0ddd-45f0-a159-dcd82be21e2a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425ac46d9c1c45eaa8d3961f0d24da37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    }
   ],
   "source": [
    "docstring_report = evaluate_pred_df(docstring_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce599147-7346-4f3b-bbf1-4fc60c19cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.0\n",
      "codebleu_above_90 : 0.10355177588794397\n",
      "diff_bleu_avg : 0.015511289362665262\n",
      "diff_bleu_perfect : 0.0\n",
      "diff_bleu_above_90 : 0.0\n",
      "docstr_text_scores_avg : 0.0\n",
      "docstr_text_scores_perfect : 0.0\n",
      "docstr_text_scores_above_90 : 0.0\n",
      "parse_test_accuracy : 0.832416208104052\n"
     ]
    }
   ],
   "source": [
    "for key, val in docstring_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7024c01-ef5a-4aa4-be4f-ce02f8b853ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4ed71ab-5682-463a-b89e-f137a6f0f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"seq2seq_results\"\n",
    "model_name = \"outlier_codet5small\"\n",
    "ckpt = \"checkpoint-40500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c1fdd0e-289e-4f05-b4f8-ebb0ae81e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codet5_eval_set_preds.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23e9a397-6c5d-4265-a3ad-1dade6435419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.6448604000984216,\n",
       " 'weighted_ngram': 0.658919766640146,\n",
       " 'syntax_match': 0.8721128194792865,\n",
       " 'dataflow_match': 0.8420958814147669,\n",
       " 'code_bleu': 0.7544972169081553}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25', dropna=True)\n",
    "comment_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd22848c-f71d-4781-9b61-05cc91a4c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d86917c-934b-488a-8889-ed25d58c5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why there is nan in preds, but just exclude it no matter what\n",
    "comment_pred_df = comment_pred_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "174294af-f3d4-4767-b711-1d2aef022c13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af10c4b1b3f4686bda7912c4a8442b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    }
   ],
   "source": [
    "comment_report = evaluate_pred_df(comment_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4785136a-6ef3-4e4d-b4a4-d9ee3ca04c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.05958291956305859\n",
      "codebleu_above_90 : 0.2701092353525323\n",
      "diff_bleu_avg : 0.28453211512775645\n",
      "diff_bleu_perfect : 0.05114200595829196\n",
      "diff_bleu_above_90 : 0.09384309831181728\n",
      "comment_text_scores_avg : 0.17852446160161348\n",
      "comment_text_scores_perfect : 0.07944389275074479\n",
      "comment_text_scores_above_90 : 0.11519364448857994\n",
      "parse_test_accuracy : 0.5099304865938431\n"
     ]
    }
   ],
   "source": [
    "for key, val in comment_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874b43b-f7d8-4238-af45-60b9bf4b87a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Eval on test split from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891133a5-3134-4dd8-9ead-c9a73ed5e2c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Uncomment Parallel Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb10efc-1e36-4e04-8d7c-1c442d6523b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_outlier_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/no_outlier_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0036e88-85d5-4de7-bb9d-68cc81de9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1cf91-ab95-48dd-a226-e8d345bed8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357a66d-82b1-4a93-9138-3ba335083de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_pred_df = pd.read_csv(\"seq2seq_results/outlier_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edefd98-31d1-467f-9f98-b28c08dd8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = comment_pred_df[\"inputs\"] == comment_pred_df[\"labels\"]\n",
    "cleaned_comment_pred_df = comment_pred_df.drop(comment_pred_df[exact_match_bool].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a8ba3-0522-4ff9-b018-f6cbee27a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_codebleu(\"\", weights=\"0.25,0.25,0.25,0.25\", replaced_df=cleaned_comment_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c5403-4de8-4fcc-af78-a81ea8397373",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_pred_df = cleaned_comment_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b674c-a066-467d-b76b-eede739a46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_inputs = comment_pred_df[\"inputs\"].to_numpy()\n",
    "comment_labels = comment_pred_df[\"labels\"].to_numpy()\n",
    "comment_preds = comment_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4185e-912b-446f-bcd4-13c485a246fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "comment_code_scores = []\n",
    "comment_text_scores = []\n",
    "comment_diff_bleu_scores = []\n",
    "\n",
    "gold_comments = []\n",
    "pred_comments = []\n",
    "gold_comment_texts = []\n",
    "pred_comment_texts = []\n",
    "gold_comments_count = []\n",
    "pred_comments_count = []\n",
    "gold_has_comments_list = []\n",
    "pred_has_comments_list = []\n",
    "\n",
    "for idx in tqdm(range(comment_preds.shape[0])):\n",
    "    input_code = comment_inputs[idx]\n",
    "    gold = comment_labels[idx]\n",
    "    pred = comment_preds[idx]\n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    comment_code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    \n",
    "    gold_comment = comment(gold)\n",
    "    pred_comment = comment(pred)\n",
    "    gold_comment_text = \"\\n\".join(gold_comment)\n",
    "    pred_comment_text = \"\\n\".join(pred_comment)\n",
    "    gold_comment_count = len(gold_comment)\n",
    "    pred_comment_count = len(pred_comment)\n",
    "    gold_has_comment = len(gold_comment) > 0\n",
    "    pred_has_comment = len(pred_comment) > 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    comment_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        comment_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "    \n",
    "    comment_text_score = get_codebleu([[gold_comment_text]], [pred_comment_text], \"python\", '1,0,0,0')\n",
    "    \n",
    "    comment_code_scores += [comment_code_score]\n",
    "    comment_text_scores += [comment_text_score]\n",
    "    comment_diff_bleu_scores += [comment_diff_bleu_score]\n",
    "       \n",
    "    gold_comments += [gold_comment]\n",
    "    pred_comments += [pred_comment]\n",
    "    gold_comment_texts += [gold_comment_text]\n",
    "    pred_comment_texts += [pred_comment_text]\n",
    "    gold_comments_count += [gold_comment_count]\n",
    "    pred_comments_count += [pred_comment_count]\n",
    "    gold_has_comments_list += [gold_has_comment]\n",
    "    pred_has_comments_list += [pred_has_comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808cdcb-c92b-4c85-b8c6-02cba99e35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_bleu_scores = np.array([s[\"ngram\"] for s in comment_text_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529e210-04c5-4346-807a-d2297bebfe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_bleu_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e79f11-1095-49d9-9ab9-230efdb6d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Comment BLEU score on only comparing difference in prediction:\", np.mean(comment_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9824b0-4798-4220-924e-8d7354dd9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_bleu_scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f168f7-b81e-4c02-80ac-0c2ceb3c8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_bleu_scores[3236]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f0728-0ba9-4e4e-a86f-bc75d2204ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 188\n",
    "print_split_line(f\"{idx}-prediction\")\n",
    "print(comment_preds[idx])\n",
    "print_split_line(f\"{idx}-gold labels\")\n",
    "print(comment_labels[idx])\n",
    "print_split_line(f\"{idx}-score\")\n",
    "print(comment_bleu_scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a5a9d-aaaa-44c5-9918-6e77ec28492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_total = len(comment_preds)\n",
    "sum(pred_has_comments_list), sum(gold_has_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcb9b2-3053-4df1-85e4-990f8822ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(comment_total):\n",
    "    if comment_bleu_scores[idx] < 0.5 or comment_bleu_scores[idx] > 0.95:    \n",
    "        continue\n",
    "    \n",
    "    if not pred_has_comments_list[idx]:\n",
    "        continue\n",
    "    if not gold_has_comments_list[idx]:\n",
    "        continue\n",
    "        \n",
    "#     if \"copyright\" in pred_comment_texts[idx].lower():\n",
    "#         continue\n",
    "        \n",
    "#     if \"copyright\" in gold_comment_texts[idx].lower():\n",
    "#         continue\n",
    "        \n",
    "#     if \"license\" in pred_comment_texts[idx].lower():\n",
    "#         continue\n",
    "        \n",
    "#     if \"license\" in gold_comment_texts[idx].lower():\n",
    "#         continue\n",
    "        \n",
    "    \n",
    "        \n",
    "#     if \"\\n#\" in pred_comment_texts[idx].lower():\n",
    "#         continue\n",
    "        \n",
    "    # if \" #\" not in gold_comment_texts[idx].lower():\n",
    "    #     continue\n",
    "    \n",
    "    # if \" #\" in pred_comment_texts[idx].lower():\n",
    "    print_split_line(f\"{idx}-prediction\")\n",
    "    print(comment_preds[idx])\n",
    "    print_split_line(f\"{idx}-gold labels\")\n",
    "    print(comment_labels[idx])\n",
    "    print_split_line(f\"{idx}-score\")\n",
    "    print(comment_bleu_scores[idx])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27578397-ec84-420a-b964-dadaf1da6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of whether both do or do not have comments\")\n",
    "sum(np.array(pred_has_comments_list) == np.array(gold_has_comments_list)) / comment_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590f0d4-3da4-447d-8c97-9ba435cef63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of whether both have same comment counts\")\n",
    "sum(np.array(gold_comments_count) == np.array(pred_comments_count)) / comment_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bcc10-0c95-4865-acb2-e44d95db609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_and(comment_bleu_scores == 1, np.array(pred_has_comments_list), np.array(gold_has_comments_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5bb75-36b4-4995-8987-6e9b18354482",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(comment_bleu_scores == 1) / comment_total)\n",
    "print(\"Above 0.9 Comment BLEU Prediction Rate:\", sum(comment_bleu_scores >= 0.9) / comment_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c1efc-cbf3-48c8-a475-a496771e1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(np.logical_and(comment_bleu_scores == 1, np.array(pred_has_comments_list), np.array(gold_has_comments_list))) / comment_total)\n",
    "print(\"Above 0.9 Comment CodeBLEU Prediction Rate:\", sum(np.logical_and(comment_bleu_scores >= 0.9, np.array(pred_has_comments_list), np.array(gold_has_comments_list))) / comment_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572df29-ea42-4dcf-a916-bf917db9b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_split_line(\"prediction\")\n",
    "print(preds[3236])\n",
    "print_split_line(\"gold labels\")\n",
    "print(labels[3236])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de274092-1f1a-4004-9870-3c0ceeaf56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_codebleu([[labels[20]]], [preds[20]], \"python\", '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b571c7-1d8e-4166-b45b-e6b56b659bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Removed Class Parallel Corpus - with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7eaf4-5954-47dd-ac0a-9c672ab20c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_class_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_class_codet5small/codet5_preds.csv\",  '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4b396-5404-4474-9fc5-a26b4b85b8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_pred_df = pd.read_csv(\"seq2seq_results/outlier_class_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429400d-0434-47d7-92f7-5ebc49d7cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc4f5d-fcd9-4528-9918-b316df1d2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = class_pred_df[\"inputs\"] == class_pred_df[\"labels\"]\n",
    "cleaned_class_pred_df = class_pred_df.drop(class_pred_df[exact_match_bool].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3673682-bd00-4f07-833c-c7122d7e1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred_df = cleaned_class_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd893b-fe2c-43fa-8b3d-97f452d298fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_inputs = class_pred_df[\"inputs\"].to_numpy()\n",
    "class_labels = class_pred_df[\"labels\"].to_numpy()\n",
    "class_preds = class_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05f1a3-f240-4dbd-a0c2-59db0055a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "class_scores = []\n",
    "class_diff_bleu_scores = []\n",
    "for idx in tqdm(range(class_preds.shape[0])):\n",
    "    input_code = class_inputs[idx]\n",
    "    gold = class_labels[idx]\n",
    "    pred = class_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    class_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        class_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "    \n",
    "    class_diff_bleu_scores += [class_diff_bleu_score]\n",
    "    \n",
    "    score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    class_scores += [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0f058-2ce5-46b2-acd6-62921cd375f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Class BLEU score on only comparing difference in prediction:\", np.mean(class_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7945a-8668-4ba0-bd5d-32898ffebc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_total = class_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b0e0a-3f36-44e0-a6a4-4539bad67f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_code_bleus = np.array([s[\"code_bleu\"] for s in class_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf99ca-da29-49ec-bba8-299c39b3902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(class_code_bleus == 1) / class_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(class_code_bleus > 0.9) / class_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48869baf-3f1f-4d0b-86f1-8055bc73fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a perfect case\n",
    "print_split_line(\"input\")\n",
    "print(eval_dataset[\"train\"][\"no_class_content\"][63833])\n",
    "print_split_line(\"prediction\")\n",
    "print(class_preds[0])\n",
    "print_split_line(\"gold labels\")\n",
    "print(class_labels[0])\n",
    "print_split_line(\"score\")\n",
    "print(class_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e50e41-8226-456b-a5fe-3ec23adf49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_idx = 53092\n",
    "output_idx = 4293\n",
    "print_split_line(\"input\")\n",
    "print(eval_dataset[\"train\"][\"no_class_content\"][input_idx])\n",
    "print_split_line(\"prediction\")\n",
    "print(class_preds[output_idx])\n",
    "print_split_line(\"gold labels\")\n",
    "print(class_labels[output_idx])\n",
    "print_split_line(\"score\")\n",
    "print(class_scores[output_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b50a80-234f-4828-bfa7-1192036b4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[\"train\"][\"no_class_content\"][74459])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff02d4c-106f-4a22-86dd-c4faf474ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6d47f-a148-4f66-a494-036252cc24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_no_class_no_super_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_no_class_no_super_codet5small/codet5_preds.csv\",  '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f2add-c60a-4652-b8ee-cf1aedbb4908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_super_pred_df = pd.read_csv(\"seq2seq_results/outlier_no_class_no_super_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333081ad-af7d-4a36-89ae-b6e1bb8699f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_super_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da25e38-2a70-45ce-a9cf-1e5bd0befe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = class_super_pred_df[\"inputs\"] == class_super_pred_df[\"labels\"]\n",
    "cleaned_class_pred_df = class_super_pred_df.drop(class_super_pred_df[exact_match_bool].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8069f-336a-48c8-bd64-af3cf433462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_super_pred_df = cleaned_class_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed217f-53b4-4122-b702-5d33711aab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_super_inputs = class_super_pred_df[\"inputs\"].to_numpy()\n",
    "class_super_labels = class_super_pred_df[\"labels\"].to_numpy()\n",
    "class_super_preds = class_super_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8ac4b-f5a7-4206-b469-ebc331cb4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "class_super_scores = []\n",
    "class_super_diff_bleu_scores = []\n",
    "for idx in tqdm(range(class_super_preds.shape[0])):\n",
    "    input_code = class_super_inputs[idx]\n",
    "    gold = class_super_labels[idx]\n",
    "    pred = class_super_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    class_super_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        class_super_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "    \n",
    "    class_super_diff_bleu_scores += [class_super_diff_bleu_score]\n",
    "    \n",
    "    score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    class_super_scores += [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097e8ca-f00d-4ec3-893f-1ea4f4c50421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Class BLEU score on only comparing difference in prediction:\", np.mean(class_super_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376df9f-81a5-42bd-9b0e-1871d2af341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_super_total = class_super_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32a2c8-142f-4fee-b7c6-2cc08db908b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_super_code_bleus = np.array([s[\"code_bleu\"] for s in class_super_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dbc64-d936-4a13-884f-327d1e776d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(class_super_code_bleus == 1) / class_super_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(class_super_code_bleus > 0.9) / class_super_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fdfc4b-7c3a-4a6c-bf9c-b10108775b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(class_super_total):\n",
    "    if class_super_diff_bleu_scores[idx] >= 0.8:\n",
    "        # if \"oStart\" not in pred_docstr_texts[idx]: continue\n",
    "        print_split_line(f\"{idx}-prediction\")\n",
    "        print(class_super_preds[idx])\n",
    "        print_split_line(f\"{idx}-gold labels\")\n",
    "        print(class_super_labels[idx])\n",
    "        print_split_line(f\"{idx}-score\")\n",
    "        print(class_super_diff_bleu_scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd5cb5-044c-4d79-80fb-67952e5c352b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(class_super_total):\n",
    "    if class_super_diff_bleu_scores[idx] < 0.3 and class_super_diff_bleu_scores[idx] > 0.1:\n",
    "        print_split_line(f\"{idx}-input\")\n",
    "        print(class_super_inputs[idx])\n",
    "        print_split_line(f\"{idx}-prediction\")\n",
    "        print(class_super_preds[idx])\n",
    "        print_split_line(f\"{idx}-gold labels\")\n",
    "        print(class_super_labels[idx])\n",
    "        print_split_line(f\"{idx}-score\")\n",
    "        print(class_super_diff_bleu_scores[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ecd68-a41c-4403-baf2-6f7c4dff9ca0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Removed Doc String Parallel Corpus - with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a83418-e4e0-48ec-b3c6-c7508e893401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_docstring_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_docstring_codet5small/codet5_preds.csv\",  '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e1e56-3782-491c-96f7-a4031e53e642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docstr_pred_df = pd.read_csv(\"seq2seq_results/outlier_docstring_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4b674-ee7e-4672-855f-8cc499289ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_pred_df = docstr_pred_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344bf98-743b-40fa-b6ab-92d05c8738c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = docstr_pred_df[\"inputs\"] == docstr_pred_df[\"labels\"]\n",
    "cleaned_docstr_pred_df = docstr_pred_df.drop(docstr_pred_df[exact_match_bool].index)\n",
    "docstr_pred_df = cleaned_docstr_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da03fc-6ad5-4dc6-be2a-bac180bdc2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_inputs = docstr_pred_df[\"inputs\"].to_numpy()\n",
    "docstr_labels = docstr_pred_df[\"labels\"].to_numpy()\n",
    "docstr_preds = docstr_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd773a-3d36-4082-af22-eca3113ee588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_docstring(text):\n",
    "    regex_docstr = \"^\\s*\\'{3}([\\s\\S]*?)\\'{3}|^\\s*\\\"{3}([\\s\\S]*?)\\\"{3}\"\n",
    "    docstr_matches = re.findall(regex_docstr, text, re.M | re.S)\n",
    "    docstrs = []\n",
    "    for match in docstr_matches:\n",
    "        docstr_a, docstr_b = match\n",
    "        if docstr_a:\n",
    "            docstrs += [docstr_a]\n",
    "        else:\n",
    "            docstrs += [docstr_b]\n",
    "    return docstrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe1693-f846-449e-b61d-fb387782570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "gold_docstrs = []\n",
    "pred_docstrs = []\n",
    "gold_docstr_counts = []\n",
    "pred_docstr_counts = []\n",
    "gold_docstr_texts = []\n",
    "pred_docstr_texts = []\n",
    "gold_has_docstr_list = []\n",
    "pred_has_docstr_list = []\n",
    "\n",
    "docstr_code_scores = []\n",
    "docstr_text_scores = []\n",
    "docstr_diff_bleu_scores = []\n",
    "\n",
    "for idx in tqdm(range(docstr_preds.shape[0])):\n",
    "    input_code = docstr_inputs[idx]\n",
    "    gold = docstr_labels[idx]\n",
    "    pred = docstr_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    gold_docstr = get_docstring(gold)\n",
    "    pred_docstr = get_docstring(pred)\n",
    "    gold_docstr_text = \"\\n\".join(gold_docstr)\n",
    "    pred_docstr_text = \"\\n\".join(pred_docstr)\n",
    "    gold_docstr_count = len(gold_docstr)\n",
    "    pred_docstr_count = len(pred_docstr)\n",
    "    gold_has_docstr = len(gold_docstr) > 0\n",
    "    pred_has_docstr = len(pred_docstr) > 0\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    docstr_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        docstr_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "        \n",
    "    docstr_code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    docstr_text_score = get_codebleu([[gold_docstr_text]], [pred_docstr_text], \"python\", '1,0,0,0')\n",
    "    \n",
    "    docstr_code_scores += [docstr_code_score]\n",
    "    docstr_text_scores += [docstr_text_score]\n",
    "    docstr_diff_bleu_scores += [docstr_diff_bleu_score]\n",
    "       \n",
    "    gold_docstrs += [gold_docstr]\n",
    "    pred_docstrs += [pred_docstr]\n",
    "    gold_docstr_texts += [gold_docstr_text]\n",
    "    pred_docstr_texts += [pred_docstr_text]\n",
    "    gold_docstr_counts += [gold_docstr_count]\n",
    "    pred_docstr_counts += [pred_docstr_count]\n",
    "    gold_has_docstr_list += [gold_has_docstr]\n",
    "    pred_has_docstr_list += [pred_has_docstr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1c0d3-2bd1-46de-a676-ee4118ac0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_text_bleus = np.array([s[\"ngram\"] for s in docstr_text_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35344db-3345-43c3-89e2-cebb539198eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_code_bleus = np.array([s[\"code_bleu\"] for s in docstr_code_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3530b-6571-4390-b21d-8c6b8c9934fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_diff_bleu_scores = np.array(docstr_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f6655-95f6-44a5-aa75-d5762d11b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_text_bleus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f87bc1-81c7-4943-8af8-cc375339bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstr_total = docstr_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230a485-cdad-4e0c-ae36-5f6eeacf9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Docstring BLEU score on only comparing difference in prediction:\", np.mean(docstr_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a2f11-ae18-4079-84e7-ef4f0556c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(docstr_text_bleus == 1) / docstr_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(docstr_text_bleus > 0.9) / docstr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58f228-b8b3-4d0b-a012-a26447c5aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(docstr_diff_bleu_scores == 1) / docstr_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(docstr_diff_bleu_scores > 0.9) / docstr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b9e7b-87a0-43f1-9ca1-e21e88a37fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 67\n",
    "print_split_line(f\"{idx}-prediction\")\n",
    "print(docstr_preds[idx])\n",
    "print_split_line(f\"{idx}-gold labels\")\n",
    "print(docstr_labels[idx])\n",
    "print_split_line(f\"{idx}-score\")\n",
    "print(docstr_text_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b0b76-340d-41d7-8aff-43f7d555511e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(docstr_total):\n",
    "    if docstr_text_bleus[idx] >= 0.5:\n",
    "        if \"oStart\" not in pred_docstr_texts[idx]: continue\n",
    "        print_split_line(f\"{idx}-prediction\")\n",
    "        print(pred_docstr_texts[idx])\n",
    "        print_split_line(f\"{idx}-gold labels\")\n",
    "        print(gold_docstr_texts[idx])\n",
    "        print_split_line(f\"{idx}-score\")\n",
    "        print(docstr_text_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f31e4-138c-4e53-a474-569e03f2bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_updated_docstring_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_updated_docstring_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d663ac9-1ad2-4199-8248-53202bf8aa86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_docstr_pred_df = pd.read_csv(\"seq2seq_results/outlier_updated_docstring_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203a64e-4adf-49eb-9061-a7030b545db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = updated_docstr_pred_df[\"inputs\"] == updated_docstr_pred_df[\"labels\"]\n",
    "cleaned_docstr_pred_df = updated_docstr_pred_df.drop(updated_docstr_pred_df[exact_match_bool].index)\n",
    "updated_docstr_pred_df = cleaned_docstr_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e66e5-3e09-400a-a208-1e749d03ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_docstr_inputs = updated_docstr_pred_df[\"inputs\"].to_numpy()\n",
    "updated_docstr_labels = updated_docstr_pred_df[\"labels\"].to_numpy()\n",
    "updated_docstr_preds = updated_docstr_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342302ed-bd4d-491a-ba45-011fc67b0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "gold_docstrs = []\n",
    "pred_docstrs = []\n",
    "gold_docstr_counts = []\n",
    "pred_docstr_counts = []\n",
    "gold_docstr_texts = []\n",
    "pred_docstr_texts = []\n",
    "gold_has_docstr_list = []\n",
    "pred_has_docstr_list = []\n",
    "\n",
    "updated_docstr_code_scores = []\n",
    "updated_docstr_text_scores = []\n",
    "updated_docstr_diff_bleu_scores = []\n",
    "\n",
    "for idx in tqdm(range(docstr_preds.shape[0])):\n",
    "    input_code = docstr_inputs[idx]\n",
    "    gold = docstr_labels[idx]\n",
    "    pred = docstr_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    gold_docstr = get_docstring(gold)\n",
    "    pred_docstr = get_docstring(pred)\n",
    "    gold_docstr_text = \"\\n\".join(gold_docstr)\n",
    "    pred_docstr_text = \"\\n\".join(pred_docstr)\n",
    "    gold_docstr_count = len(gold_docstr)\n",
    "    pred_docstr_count = len(pred_docstr)\n",
    "    gold_has_docstr = len(gold_docstr) > 0\n",
    "    pred_has_docstr = len(pred_docstr) > 0\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    docstr_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        docstr_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "        \n",
    "    docstr_code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    docstr_text_score = get_codebleu([[gold_docstr_text]], [pred_docstr_text], \"python\", '1,0,0,0')\n",
    "    \n",
    "    updated_docstr_code_scores += [docstr_code_score]\n",
    "    updated_docstr_text_scores += [docstr_text_score]\n",
    "    updated_docstr_diff_bleu_scores += [docstr_diff_bleu_score]\n",
    "       \n",
    "    gold_docstrs += [gold_docstr]\n",
    "    pred_docstrs += [pred_docstr]\n",
    "    gold_docstr_texts += [gold_docstr_text]\n",
    "    pred_docstr_texts += [pred_docstr_text]\n",
    "    gold_docstr_counts += [gold_docstr_count]\n",
    "    pred_docstr_counts += [pred_docstr_count]\n",
    "    gold_has_docstr_list += [gold_has_docstr]\n",
    "    pred_has_docstr_list += [pred_has_docstr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26c14a-9849-4a57-aaa3-76dcf69dd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_docstr_text_bleus = np.array([s[\"ngram\"] for s in updated_docstr_text_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f46113-3d5c-4eb8-9f8b-a20b401c9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_docstr_code_bleus = np.array([s[\"code_bleu\"] for s in updated_docstr_code_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a77d7-74f3-4d63-9d95-65fc6235b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_docstr_text_bleus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280e852-468a-4434-aaaa-7281141f60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_docstr_diff_bleu_scores = np.array(updated_docstr_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd4310-55fd-40d1-a32e-d202dd2865bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Updated Docstring BLEU score on only comparing difference in prediction:\", np.mean(updated_docstr_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd82c3-64ad-456f-a0d0-5079c80a9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_docstr_total = updated_docstr_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4fbda-3016-4b90-8b8a-b36440ff60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(updated_docstr_text_bleus == 1) / updated_docstr_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(updated_docstr_text_bleus > 0.9) / updated_docstr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fafbc4-a20d-4384-bec3-6d73ec5c7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(updated_docstr_diff_bleu_scores == 1) / updated_docstr_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(updated_docstr_diff_bleu_scores > 0.9) / updated_docstr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc88c0-1250-40ad-8685-727b787aff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(docstr_total):\n",
    "    if updated_docstr_diff_bleu_scores[idx] >= 0.0 and updated_docstr_diff_bleu_scores[idx] < 0.1:\n",
    "        print_split_line(f\"{idx}-input\")\n",
    "        print(updated_docstr_inputs[idx])\n",
    "        print_split_line(f\"{idx}-prediction\")\n",
    "        print(updated_docstr_preds[idx])\n",
    "        print_split_line(f\"{idx}-gold labels\")\n",
    "        print(updated_docstr_labels[idx])\n",
    "        print_split_line(f\"{idx}-score\")\n",
    "        print(updated_docstr_diff_bleu_scores[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23023b11-6979-4340-98bb-22b11e81d8a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26d2b2-bede-423b-8122-75c29fb1b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_casing_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_casing_codet5small/codet5_preds.csv\",  '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170474c-fd38-4420-a716-6ea941ef5e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "casing_pred_df = pd.read_csv(\"seq2seq_results/outlier_casing_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdd6bd-73c6-4baa-87ea-281b9447d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = casing_pred_df[\"inputs\"] == casing_pred_df[\"labels\"]\n",
    "cleaned_casing_pred_df = casing_pred_df.drop(casing_pred_df[exact_match_bool].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785a428-52d6-4f76-a674-57f0b9ec538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_codebleu(\"\",  '0.25,0.25,0.25,0.25', replaced_df=cleaned_casing_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d61f8-51f0-4204-9fff-dfc2cf936dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "casing_pred_df = cleaned_casing_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3ce31-a379-427f-9180-81f2fabad05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "casing_inputs = casing_pred_df[\"inputs\"].to_numpy()\n",
    "casing_labels = casing_pred_df[\"labels\"].to_numpy()\n",
    "casing_preds = casing_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f45d1-2a22-4b26-8be8-6e6fd2e5c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "\n",
    "# input_casing = []\n",
    "# gold_casing = []\n",
    "# pred_casing = []\n",
    "\n",
    "# gold_docstrs = []\n",
    "# pred_docstrs = []\n",
    "# gold_docstr_counts = []\n",
    "# pred_docstr_counts = []\n",
    "# gold_docstr_texts = []\n",
    "# pred_docstr_texts = []\n",
    "# gold_has_docstr_list = []\n",
    "# pred_has_docstr_list = []\n",
    "\n",
    "casing_code_scores = []\n",
    "casing_diff_bleu_scores = []\n",
    "# docstr_text_scores = []\n",
    "\n",
    "for idx in tqdm(range(casing_preds.shape[0])):\n",
    "    input_code = casing_inputs[idx]\n",
    "    gold = casing_labels[idx]\n",
    "    pred = casing_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    casing_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        casing_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "    # gold_docstr = get_docstring(gold)\n",
    "    # pred_docstr = get_docstring(pred)\n",
    "    # gold_docstr_text = \"\\n\".join(gold_docstr)\n",
    "    # pred_docstr_text = \"\\n\".join(pred_docstr)\n",
    "    # gold_docstr_count = len(gold_docstr)\n",
    "    # pred_docstr_count = len(pred_docstr)\n",
    "    # gold_has_docstr = len(gold_docstr) > 0\n",
    "    # pred_has_docstr = len(pred_docstr) > 0\n",
    "    \n",
    "    \n",
    "    casing_code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    # docstr_text_score = get_codebleu([[gold_docstr_text]], [pred_docstr_text], \"python\", '1,0,0,0')\n",
    "    \n",
    "    casing_code_scores += [casing_code_score]\n",
    "    casing_diff_bleu_scores += [casing_diff_bleu_score]\n",
    "    # docstr_text_scores += [docstr_text_score]\n",
    "       \n",
    "    # gold_docstrs += [gold_docstr]\n",
    "    # pred_docstrs += [pred_docstr]\n",
    "    # gold_docstr_texts += [gold_docstr_text]\n",
    "    # pred_docstr_texts += [pred_docstr_text]\n",
    "    # gold_docstr_counts += [gold_docstr_count]\n",
    "    # pred_docstr_counts += [pred_docstr_count]\n",
    "    # gold_has_docstr_list += [gold_has_docstr]\n",
    "    # pred_has_docstr_list += [pred_has_docstr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21acd58-399a-410a-bc1c-24181f9183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Casing BLEU score on only comparing difference in prediction:\", np.mean(casing_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e76bd-1e1e-4999-a4cf-0b1c6ad51bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docstr_text_bleus = np.array([s[\"ngram\"] for s in docstr_text_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d0a4d-38d1-4c35-aa97-d66982b92140",
   "metadata": {},
   "outputs": [],
   "source": [
    "casing_code_bleus = np.array([s[\"code_bleu\"] for s in casing_code_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bdf31-72d4-49ac-be22-43c976791deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docstr_text_bleus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6cfab-5070-4c59-8d7d-bcb3bcf1a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "casing_total = casing_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c404bc-e032-43ae-96aa-0e818b976f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(casing_code_bleus == 1) / casing_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(casing_code_bleus > 0.9) / casing_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c04d94-a715-45d7-baa8-ebf217e80ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 531\n",
    "print_split_line(f\"{idx}-prediction\")\n",
    "print(casing_preds[idx])\n",
    "print_split_line(f\"{idx}-gold labels\")\n",
    "print(casing_labels[idx])\n",
    "print_split_line(f\"{idx}-score\")\n",
    "print(casing_code_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193350e-7d88-4dc3-a9bf-e2c0a7ff9197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f5faf-b57a-4e19-ada2-d1963300a94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(casing_total):\n",
    "    if casing_code_bleus[idx] < 0.6 and casing_code_bleus[idx] > 0.5:\n",
    "        print_split_line(f\"{idx}-input\")\n",
    "        print(casing_inputs[idx])\n",
    "        print_split_line(f\"{idx}-prediction\")\n",
    "        print(casing_preds[idx])\n",
    "        print_split_line(f\"{idx}-gold labels\")\n",
    "        print(casing_labels[idx])\n",
    "        print_split_line(f\"{idx}-score\")\n",
    "        print(casing_code_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9334e-5d66-4236-9fc5-721686ab7d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cd0dd-42a1-4af0-826e-4df8bcf6b487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc4d9f14-a0b9-466c-8416-515345d22ce5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb9d87-81c4-408c-9dd9-437fff036eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b922e-0c82-4dba-ae70-1b4589e5ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_comp_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_comp_codet5small/codet5_preds.csv\",  '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ff9f4-4fa5-4be7-9230-cdd6b4b67cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comp_pred_df = pd.read_csv(\"seq2seq_results/outlier_comp_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d452b-b991-41c3-81cf-fd404ce86324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = comp_pred_df[\"inputs\"] == comp_pred_df[\"labels\"]\n",
    "cleaned_comp_pred_df = comp_pred_df.drop(comp_pred_df[exact_match_bool].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af7d74-beb7-4cfa-82bf-7ca4c3ef8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_codebleu(\"\",  '0.25,0.25,0.25,0.25', replaced_df=cleaned_comp_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885482c-0ae9-4c4a-8da0-e25bbbdf946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_pred_df = cleaned_comp_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728d1be-fb49-444f-86d1-45b10dcfe719",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_inputs = comp_pred_df[\"inputs\"].to_numpy()\n",
    "comp_labels = comp_pred_df[\"labels\"].to_numpy()\n",
    "comp_preds = comp_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53155ce8-e73e-4e08-bed8-8b8fe017582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "\n",
    "# input_casing = []\n",
    "# gold_casing = []\n",
    "# pred_casing = []\n",
    "\n",
    "# gold_docstrs = []\n",
    "# pred_docstrs = []\n",
    "# gold_docstr_counts = []\n",
    "# pred_docstr_counts = []\n",
    "# gold_docstr_texts = []\n",
    "# pred_docstr_texts = []\n",
    "# gold_has_docstr_list = []\n",
    "# pred_has_docstr_list = []\n",
    "\n",
    "comp_code_scores = []\n",
    "comp_diff_bleu_scores = []\n",
    "# docstr_text_scores = []\n",
    "\n",
    "for idx in tqdm(range(comp_preds.shape[0])):\n",
    "    gold = comp_labels[idx]\n",
    "    pred = comp_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    input_code = comp_inputs[idx]\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    comp_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        comp_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "    # gold_docstr = get_docstring(gold)\n",
    "    # pred_docstr = get_docstring(pred)\n",
    "    # gold_docstr_text = \"\\n\".join(gold_docstr)\n",
    "    # pred_docstr_text = \"\\n\".join(pred_docstr)\n",
    "    # gold_docstr_count = len(gold_docstr)\n",
    "    # pred_docstr_count = len(pred_docstr)\n",
    "    # gold_has_docstr = len(gold_docstr) > 0\n",
    "    # pred_has_docstr = len(pred_docstr) > 0\n",
    "    \n",
    "    \n",
    "    comp_code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    # docstr_text_score = get_codebleu([[gold_docstr_text]], [pred_docstr_text], \"python\", '1,0,0,0')\n",
    "    \n",
    "    comp_code_scores += [comp_code_score]\n",
    "    comp_diff_bleu_scores += [comp_diff_bleu_score]\n",
    "    # docstr_text_scores += [docstr_text_score]\n",
    "       \n",
    "    # gold_docstrs += [gold_docstr]\n",
    "    # pred_docstrs += [pred_docstr]\n",
    "    # gold_docstr_texts += [gold_docstr_text]\n",
    "    # pred_docstr_texts += [pred_docstr_text]\n",
    "    # gold_docstr_counts += [gold_docstr_count]\n",
    "    # pred_docstr_counts += [pred_docstr_count]\n",
    "    # gold_has_docstr_list += [gold_has_docstr]\n",
    "    # pred_has_docstr_list += [pred_has_docstr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3172c7c-6af2-4905-96b5-203b4a380eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"List Comp BLEU score on only comparing difference in prediction:\", np.mean(comp_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62613567-62ca-410e-95ad-d73e74e5e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docstr_text_bleus = np.array([s[\"ngram\"] for s in docstr_text_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688269d-429f-434f-a9b7-9d0c49191be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_code_bleus = np.array([s[\"code_bleu\"] for s in comp_code_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09916d5-f5bb-49f6-9080-e6a4b772fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docstr_text_bleus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47c01e-bffd-4b93-9a42-51280b2591f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_total = comp_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36635d16-f945-4f2a-a10b-ee9092d4c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(comp_code_bleus == 1) / comp_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(comp_code_bleus > 0.9) / comp_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a57ce-1fb9-45ad-9b55-787395747321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = 760\n",
    "# print_split_line(f\"{idx}-prediction\")\n",
    "# print(docstr_preds[idx])\n",
    "# print_split_line(f\"{idx}-gold labels\")\n",
    "# print(docstr_labels[idx])\n",
    "# print_split_line(f\"{idx}-score\")\n",
    "# print(docstr_text_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b21777-fc2e-4442-8768-9713e85277df",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_code_bleus.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc4005-f71e-4cf0-8372-3a310d150a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b62447-bfbb-43dd-8522-974946cf5ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(comp_total):\n",
    "    if comp_code_bleus[idx] == 1: continue\n",
    "    if comp_code_bleus[idx] >= 0.7: continue\n",
    "    if comp_code_bleus[idx] < 0.4: continue\n",
    "    print_split_line(f\"{idx}-input\")\n",
    "    print(comp_inputs[idx])\n",
    "    print_split_line(f\"{idx}-prediction\")\n",
    "    print(comp_preds[idx])\n",
    "    print_split_line(f\"{idx}-gold labels\")\n",
    "    print(comp_labels[idx])\n",
    "    print_split_line(f\"{idx}-score\")\n",
    "    print(comp_code_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cafcfb-b718-4f0e-9f08-fb614084e1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c732ac-fbb8-4123-95dd-37a5d1e5f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_fixed_list_comp_codet5small\n",
    "evaluate_codebleu(\"seq2seq_results/outlier_fixed_list_comp_codet5small/codet5_preds.csv\",  '0.25,0.25,0.25,0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca923b4-162a-405e-9ad8-2782901b0f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_comp_pred_df = pd.read_csv(\"seq2seq_results/outlier_fixed_list_comp_codet5small/codet5_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb014f9-5088-44b4-8a11-f4e1fec39733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding those input exactly same as the output\n",
    "exact_match_bool = fixed_comp_pred_df[\"inputs\"] == fixed_comp_pred_df[\"labels\"]\n",
    "cleaned_comp_pred_df = fixed_comp_pred_df.drop(fixed_comp_pred_df[exact_match_bool].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d8598-0760-4b06-b7fc-d27a35a398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_codebleu(\"\",  '0.25,0.25,0.25,0.25', replaced_df=cleaned_comp_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba32bd-b760-4151-9748-72e2eda40c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_comp_pred_df = cleaned_comp_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be7ed7-2f32-42f4-897f-fa72e190d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_comp_inputs = fixed_comp_pred_df[\"inputs\"].to_numpy()\n",
    "fixed_comp_labels = fixed_comp_pred_df[\"labels\"].to_numpy()\n",
    "fixed_comp_preds = fixed_comp_pred_df[\"preds\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf3a39-8383-4cad-b824-ff5ff4be2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unit score\n",
    "\n",
    "# input_casing = []\n",
    "# gold_casing = []\n",
    "# pred_casing = []\n",
    "\n",
    "# gold_docstrs = []\n",
    "# pred_docstrs = []\n",
    "# gold_docstr_counts = []\n",
    "# pred_docstr_counts = []\n",
    "# gold_docstr_texts = []\n",
    "# pred_docstr_texts = []\n",
    "# gold_has_docstr_list = []\n",
    "# pred_has_docstr_list = []\n",
    "\n",
    "fixed_comp_code_scores = []\n",
    "fixed_comp_diff_bleu_scores = []\n",
    "# docstr_text_scores = []\n",
    "\n",
    "for idx in tqdm(range(fixed_comp_preds.shape[0])):\n",
    "    gold = fixed_comp_labels[idx]\n",
    "    pred = fixed_comp_preds[idx]\n",
    "    \n",
    "    refs = [\n",
    "        [gold]\n",
    "    ]\n",
    "    hyp = [pred]\n",
    "    \n",
    "    input_code = fixed_comp_inputs[idx]\n",
    "    \n",
    "    gold_diff_str = get_diff_str(input_code, gold)\n",
    "    pred_diff_str = get_diff_str(input_code, pred)\n",
    "    \n",
    "    fixed_comp_diff_bleu_score = 0\n",
    "    if len(pred_diff_str.split()) > 0:\n",
    "        fixed_comp_diff_bleu_score = sentence_bleu([gold_diff_str.split()], pred_diff_str.split(), auto_reweigh=True)\n",
    "    # gold_docstr = get_docstring(gold)\n",
    "    # pred_docstr = get_docstring(pred)\n",
    "    # gold_docstr_text = \"\\n\".join(gold_docstr)\n",
    "    # pred_docstr_text = \"\\n\".join(pred_docstr)\n",
    "    # gold_docstr_count = len(gold_docstr)\n",
    "    # pred_docstr_count = len(pred_docstr)\n",
    "    # gold_has_docstr = len(gold_docstr) > 0\n",
    "    # pred_has_docstr = len(pred_docstr) > 0\n",
    "    \n",
    "    \n",
    "    fixed_comp_code_score = get_codebleu(refs, hyp, \"python\", '0.25,0.25,0.25,0.25')\n",
    "    # docstr_text_score = get_codebleu([[gold_docstr_text]], [pred_docstr_text], \"python\", '1,0,0,0')\n",
    "    \n",
    "    fixed_comp_code_scores += [fixed_comp_code_score]\n",
    "    fixed_comp_diff_bleu_scores += [fixed_comp_diff_bleu_score]\n",
    "    # docstr_text_scores += [docstr_text_score]\n",
    "       \n",
    "    # gold_docstrs += [gold_docstr]\n",
    "    # pred_docstrs += [pred_docstr]\n",
    "    # gold_docstr_texts += [gold_docstr_text]\n",
    "    # pred_docstr_texts += [pred_docstr_text]\n",
    "    # gold_docstr_counts += [gold_docstr_count]\n",
    "    # pred_docstr_counts += [pred_docstr_count]\n",
    "    # gold_has_docstr_list += [gold_has_docstr]\n",
    "    # pred_has_docstr_list += [pred_has_docstr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a6f18-58be-4c87-b421-bb148fa3586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_comp_diff_bleu_scores = np.array(fixed_comp_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf2bfa-782d-4748-904b-ed7838a14748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"List Comp BLEU score on only comparing difference in prediction:\", np.mean(fixed_comp_diff_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401a2f4-3c80-484f-8665-9f915bd279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docstr_text_bleus = np.array([s[\"ngram\"] for s in docstr_text_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a8e71-dcec-4ca0-a6a1-432a2cd45f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_comp_code_bleus = np.array([s[\"code_bleu\"] for s in fixed_comp_code_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c7dca-3ef2-448e-be6e-226506953108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docstr_text_bleus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f67e80-2c0b-47d1-879c-9308a8303bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_comp_total = fixed_comp_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab0574-de72-4798-84b6-a3ddcfb6f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(fixed_comp_code_bleus == 1) / fixed_comp_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(fixed_comp_code_bleus > 0.9) / fixed_comp_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d16677-ed8a-43bb-988a-2b1a315d9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perfect Prediction Rate:\", sum(fixed_comp_diff_bleu_scores == 1) / fixed_comp_total)\n",
    "print(\"Above 0.9 CodeBLEU Prediction Rate:\", sum(fixed_comp_diff_bleu_scores > 0.9) / fixed_comp_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595820b-6ae0-44a8-bdee-e5da67395e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = 760\n",
    "# print_split_line(f\"{idx}-prediction\")\n",
    "# print(docstr_preds[idx])\n",
    "# print_split_line(f\"{idx}-gold labels\")\n",
    "# print(docstr_labels[idx])\n",
    "# print_split_line(f\"{idx}-score\")\n",
    "# print(docstr_text_bleus[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff3932-f5a6-467c-a738-33784613652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_comp_code_bleus.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff669baa-daca-4600-bc63-61c2e347e671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c576b9-81a1-460f-a044-bb3186956a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(fixed_comp_total):\n",
    "    if fixed_comp_diff_bleu_scores[idx] != 1: continue\n",
    "    if fixed_comp_diff_bleu_scores[idx] <= 0.9: continue\n",
    "    # if fixed_comp_code_bleus[idx] < 0.2: continue\n",
    "    print_split_line(f\"{idx}-input\")\n",
    "    print(fixed_comp_inputs[idx])\n",
    "    print_split_line(f\"{idx}-prediction\")\n",
    "    print(fixed_comp_preds[idx])\n",
    "    print_split_line(f\"{idx}-gold labels\")\n",
    "    print(fixed_comp_labels[idx])\n",
    "    print_split_line(f\"{idx}-score\")\n",
    "    print(fixed_comp_diff_bleu_scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f5b98-a99b-4cf6-849f-0d1b1e8151da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471426d-0e35-4c31-a1fe-0d40e63b085c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d198ce8e-bd0a-4377-a47b-d70a18466749",
   "metadata": {},
   "source": [
    "# Codex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845a56-a9cc-4cb5-b01a-b22df843dd86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## list comp - post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d59c6a4-8a0c-4d30-979b-dbf694268336",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/data/ken/data/code\"\n",
    "model_name = \".\"\n",
    "ckpt = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "060b07a2-1c27-4434-a181-402d57b6a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"codex_output_postprocessed.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"list_comp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "814cb814-5f1c-4452-9c90-9041b64c16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.8944182364863242,\n",
       " 'weighted_ngram': 0.8976747447630965,\n",
       " 'syntax_match': 0.8812310617313291,\n",
       " 'dataflow_match': 0.7832284157996021,\n",
       " 'code_bleu': 0.8641381146950879}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_comp_codex_codebleu = evaluate_codebleu(file_name,  '0.25,0.25,0.25,0.25')\n",
    "list_comp_codex_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47df08ed-6c56-4951-9767-7f59cdada305",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp_codex_pred_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "682258d3-631f-49c5-87bf-548d5b06ba34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd65bb9ee14d48b48be72c443e45438e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "list_comp_codex_report = evaluate_pred_df(list_comp_codex_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9954b5a-e930-4ce8-ab5d-997d2b63e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.0021141649048625794\n",
      "codebleu_above_90 : 0.5095137420718816\n",
      "diff_bleu_avg : 0.16295339387378543\n",
      "diff_bleu_perfect : 0.048625792811839326\n",
      "diff_bleu_above_90 : 0.0507399577167019\n",
      "parse_test_accuracy : 0.5665961945031712\n"
     ]
    }
   ],
   "source": [
    "for key, val in list_comp_codex_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5908955-7d65-4821-923c-e876d2ba28eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================1-INPUT=====================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from tensorpack.RL import HistoryFramePlayer\n",
      "__all__ = ['HistoryPlayerWithVar']\n",
      "\n",
      "class HistoryPlayerWithVar(HistoryFramePlayer):\n",
      "\n",
      "    def current_state(self):\n",
      "        assert (len(self.history) != 0)\n",
      "        assert (len(self.history[0]) == 2), 'state needs to be like [img, vars]'\n",
      "        diff_len = (self.history.maxlen - len(self.history))\n",
      "        zeros = [\u001b[31m]\n",
      "        for \u001b[0mk in range(diff_len)\u001b[31m:\n",
      "            zeros.append(np.zeros_like(self.history[0][0]))\u001b[0m\n",
      "        for k in self.history:\n",
      "            zeros.append(k[0])\n",
      "        img = np.concatenate(zeros, axis=2)\n",
      "        gvar = self.history[(- 1)][1]\n",
      "        return (img, gvar)\n",
      "\n",
      "\n",
      "=====================1-PREDICTION=====================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from tensorpack.RL import HistoryFramePlayer\n",
      "__all__ = ['HistoryPlayerWithVar']\n",
      "\n",
      "class HistoryPlayerWithVar(HistoryFramePlayer):\n",
      "\n",
      "    def current_state(self):\n",
      "        assert (len(self.history) != 0)\n",
      "        assert (len(self.history[0]) == 2), 'state needs to be like [img, vars]'\n",
      "        diff_len = (self.history.maxlen - len(self.history))\n",
      "        zeros = [\u001b[31mnp.zeros_li\u001b[0mk\u001b[31me(self.history[0][0]) for k\u001b[0m in range(diff_len)\u001b[31m]\n",
      "        img = np.concatenate([\u001b[0mk[0] for k in self.history\u001b[31m] + \u001b[0mzeros, axis=2)\n",
      "        gvar = self.history[(- 1)][1]\n",
      "        return (img, gvar)\n",
      "\u001b[31m\n",
      "Original code:\n",
      "\n",
      "import numpy as np\n",
      "from tensorpack.RL import HistoryFramePlayer\n",
      "__all__ = ['HistoryPlayerWithVar']\n",
      "\n",
      "class HistoryPlayerWithVar(HistoryFramePlayer):\n",
      "\n",
      "    def current\u001b[0m\n",
      "\n",
      "=====================1-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from tensorpack.RL import HistoryFramePlayer\n",
      "__all__ = ['HistoryPlayerWithVar']\n",
      "\n",
      "class HistoryPlayerWithVar(HistoryFramePlayer):\n",
      "\n",
      "    def current_state(self):\n",
      "        assert (len(self.history) != 0)\n",
      "        assert (len(self.history[0]) == 2), 'state needs to be like [img, vars]'\n",
      "        diff_len = (self.history.maxlen - len(self.history))\n",
      "        zeros = [\u001b[31mnp.zeros_li\u001b[0mk\u001b[31me(self.history[0][0]) for k\u001b[0m in range(diff_len)\u001b[31m]\u001b[0m\n",
      "        for k in self.history:\n",
      "            zeros.append(k[0])\n",
      "        img = np.concatenate(zeros, axis=2)\n",
      "        gvar = self.history[(- 1)][1]\n",
      "        return (img, gvar)\n",
      "\n",
      "\n",
      "=====================1-DIFF_BLEU=====================\n",
      "\n",
      "3.5841972478868185e-155\n",
      "\n",
      "=====================3-INPUT=====================\n",
      "\n",
      "\n",
      "from .entity import Entity\n",
      "from .service import Service\n",
      "\n",
      "class EscalationPolicy(Entity):\n",
      "    'PagerDuty escalation policy entity.'\n",
      "    STR_OUTPUT_FIELDS = ('id', 'name')\n",
      "    TRANSLATE_QUERY_PARAM = ('name',)\n",
      "\n",
      "    def services(self):\n",
      "        'Fetch all instances of services for this EP.'\n",
      "        ids = []\u001b[31m\n",
      "       \u001b[0m for ref in self['services']\u001b[31m:\n",
      "            ids.append(ref['id'\u001b[0m]\u001b[31m)\u001b[0m\n",
      "        return [Service.fetch(id) for id in ids]\n",
      "\n",
      "    def update(self, *args, **kwargs):\n",
      "        'Update this escalation policy.'\n",
      "        raise NotImplemented\n",
      "\n",
      "\n",
      "=====================3-PREDICTION=====================\n",
      "\n",
      "\n",
      "from .entity import Entity\n",
      "from .service import Service\n",
      "\n",
      "class EscalationPolicy(Entity):\n",
      "    'PagerDuty escalation policy entity.'\n",
      "    STR_OUTPUT_FIELDS = ('id', 'name')\n",
      "    TRANSLATE_QUERY_PARAM = ('name',)\n",
      "\n",
      "    def services(self):\n",
      "        'Fetch all instances of services for this EP.'\n",
      "        ids = [\u001b[31mref['id'\u001b[0m] for ref in self['services']]\n",
      "        return [Service.fetch(id) for id in ids]\n",
      "\n",
      "    def update(self, *args, **kwargs):\n",
      "        'Update this escalation policy.'\n",
      "        raise NotImplemented\n",
      "\u001b[31m\n",
      "Original\u001b[0m\n",
      "\n",
      "=====================3-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "from .entity import Entity\n",
      "from .service import Service\n",
      "\n",
      "class EscalationPolicy(Entity):\n",
      "    'PagerDuty escalation policy entity.'\n",
      "    STR_OUTPUT_FIELDS = ('id', 'name')\n",
      "    TRANSLATE_QUERY_PARAM = ('name',)\n",
      "\n",
      "    def services(self):\n",
      "        'Fetch all instances of services for this EP.'\n",
      "        ids = [\u001b[31mref['id'\u001b[0m] for ref in self['services']]\n",
      "        return [Service.fetch(id) for id in ids]\n",
      "\n",
      "    def update(self, *args, **kwargs):\n",
      "        'Update this escalation policy.'\n",
      "        raise NotImplemented\n",
      "\n",
      "\n",
      "=====================3-DIFF_BLEU=====================\n",
      "\n",
      "1.9501786259064824e-103\n",
      "\n",
      "=====================4-INPUT=====================\n",
      "\n",
      "\n",
      "\n",
      "class Solution():\n",
      "\n",
      "    def reorganizeString(self, S: str) -> str:\n",
      "        from collections import Counter\n",
      "        res = []\n",
      "        sMap = Counter(S)\n",
      "        cArr = [\u001b[31m]\n",
      "       \u001b[0m for k in sMap.keys()\u001b[31m:\n",
      "            cArr.append([k, sMap[k]])\u001b[0m\n",
      "        if (not len(cArr)):\n",
      "            return ''\n",
      "        while (len(cArr) >= 2):\n",
      "            cArr.sort(key=(lambda x: x[1]), reverse=True)\n",
      "            if (cArr[1][1] >= 1):\n",
      "                cArr[0][1] -= 1\n",
      "                cArr[1][1] -= 1\n",
      "                res.extend([cArr[0][0], cArr[1][0]])\n",
      "            elif (cArr[0][1] >= 1):\n",
      "                res.append(cArr[0][0])\n",
      "                cArr[0][1] -= 1\n",
      "                break\n",
      "            else:\n",
      "                break\n",
      "        if (cArr[0][1] > 0):\n",
      "            return ''\n",
      "        return ''.join(res)\n",
      "\n",
      "\n",
      "=====================4-PREDICTION=====================\n",
      "\n",
      "\n",
      "class Solution():\n",
      "\n",
      "    def reorganizeString(self, S: str) -> str:\n",
      "        from collections import Counter\n",
      "        res = []\n",
      "        sMap = Counter(S)\n",
      "        cArr = []\n",
      "        for k in sMap.keys():\n",
      "            cArr.append([k, sMap[k]])\n",
      "        if (not len(cArr)):\n",
      "            return ''\n",
      "        while (len(cArr) >= 2):\n",
      "            cArr.sort(key=(lambda x: x[1]), reverse=True)\n",
      "            if (cArr[1][1] >= 1):\n",
      "                cArr[0][1] -= 1\n",
      "                cArr[1][1] -= 1\n",
      "                res.extend([cArr[0][0], cArr[1][0]])\n",
      "            elif (cArr[0][1] >= 1):\n",
      "                res.append(cArr[0][0])\n",
      "                cArr[0][1] -= 1\n",
      "                break\n",
      "            else:\n",
      "                break\n",
      "        if (cArr[0][1] > 0):\n",
      "   \n",
      "\n",
      "=====================4-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "\n",
      "class Solution():\n",
      "\n",
      "    def reorganizeString(self, S: str) -> str:\n",
      "        from collections import Counter\n",
      "        res = []\n",
      "        sMap = Counter(S)\n",
      "        cArr = [\u001b[31m[k, sMap[k]]\u001b[0m for k in sMap.keys()\u001b[31m]\u001b[0m\n",
      "        if (not len(cArr)):\n",
      "            return ''\n",
      "        while (len(cArr) >= 2):\n",
      "            cArr.sort(key=(lambda x: x[1]), reverse=True)\n",
      "            if (cArr[1][1] >= 1):\n",
      "                cArr[0][1] -= 1\n",
      "                cArr[1][1] -= 1\n",
      "                res.extend([cArr[0][0], cArr[1][0]])\n",
      "            elif (cArr[0][1] >= 1):\n",
      "                res.append(cArr[0][0])\n",
      "                cArr[0][1] -= 1\n",
      "                break\n",
      "            else:\n",
      "                break\n",
      "        if (cArr[0][1] > 0):\n",
      "            return ''\n",
      "        return ''.join(res)\n",
      "\n",
      "\n",
      "=====================4-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================5-INPUT=====================\n",
      "\n",
      "\n",
      "import os\n",
      "import unittest\n",
      "from google.cloud import datacatalog\n",
      "datacatalog_client = datacatalog.DataCatalogClient()\n",
      "\n",
      "class CleanupResultsTest(unittest.TestCase):\n",
      "\n",
      "    def test_entries_should_not_exist_after_cleanup(self):\n",
      "        query = 'system=greenplum'\n",
      "        scope = datacatalog.SearchCatalogRequest.Scope()\n",
      "        scope.include_project_ids.append(os.environ['GREENPLUM2DC_DATACATALOG_PROJECT_ID'])\n",
      "        request = datacatalog.SearchCatalogRequest()\n",
      "        request.scope = scope\n",
      "        request.query = query\n",
      "        request.page_size = 1000\n",
      "        search_results = []\n",
      "        f\u001b[31mor result in datacatalog_client.\u001b[0msearch_\u001b[31mcatalog(request):\n",
      "            search_results.append(result)\n",
      "        self.assertEqual(len(search_\u001b[0mresults), 0)\n",
      "\n",
      "\n",
      "=====================5-PREDICTION=====================\n",
      "\n",
      "\n",
      "import os\n",
      "import unittest\n",
      "from google.cloud import datacatalog\n",
      "datacatalog_client = datacatalog.DataCatalogClient()\n",
      "\n",
      "class CleanupResultsTest(unittest.TestCase):\n",
      "\n",
      "    def test_entries_should_not_exist_after_cleanup(self):\n",
      "        query = 'system=greenplum'\n",
      "        scope = datacatalog.SearchCatalogRequest.Scope()\n",
      "        scope.include_project_ids.append(os.environ['GREENPLUM2DC_DATACATALOG_PROJECT_ID'])\n",
      "        request = datacatalog.SearchCatalogRequest()\n",
      "        request.scope = scope\n",
      "        request.query = query\n",
      "        request.page_size = 1000\n",
      "        search_results = [\u001b[31mresult for result in datacatalog_client.search_catalog(request)\u001b[0m]\n",
      "        \u001b[31msel\u001b[0mf\u001b[31m.assertEqual(len(\u001b[0msearch_results), 0)\n",
      "\u001b[31m\n",
      "Original\u001b[0m\n",
      "\n",
      "=====================5-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import os\n",
      "import unittest\n",
      "from google.cloud import datacatalog\n",
      "datacatalog_client = datacatalog.DataCatalogClient()\n",
      "\n",
      "class CleanupResultsTest(unittest.TestCase):\n",
      "\n",
      "    def test_entries_should_not_exist_after_cleanup(self):\n",
      "        query = 'system=greenplum'\n",
      "        scope = datacatalog.SearchCatalogRequest.Scope()\n",
      "        scope.include_project_ids.append(os.environ['GREENPLUM2DC_DATACATALOG_PROJECT_ID'])\n",
      "        request = datacatalog.SearchCatalogRequest()\n",
      "        request.scope = scope\n",
      "        request.query = query\n",
      "        request.page_size = 1000\n",
      "        search_results = [\u001b[31mresult for result in datacatalog_client.search_catalog(request)\u001b[0m]\n",
      "        \u001b[31msel\u001b[0mf\u001b[31m.assertEqual(len(\u001b[0msearch_results), 0)\n",
      "\n",
      "\n",
      "=====================5-DIFF_BLEU=====================\n",
      "\n",
      "1.9501786259064824e-103\n",
      "\n",
      "=====================6-INPUT=====================\n",
      "\n",
      "\n",
      "from distutils.core import setup\n",
      "from distutils.extension import Extension\n",
      "from Cython.Distutils import build_ext\n",
      "from distutils import log\n",
      "import os\n",
      "from distutils.command.clean import clean as _clean\n",
      "\n",
      "class clean(_clean):\n",
      "    '\\n    Subclass clean so it removes all the Cython generated C files.\\n    '\n",
      "\n",
      "    def run(self):\n",
      "        _clean.run(self)\n",
      "        for ext in self.distribution.ext_modules:\n",
      "            cy_sources = []\u001b[31m\n",
      "            for s in ext.sources:\n",
      "                if s.endswith('.pyx'):\n",
      "                    cy_sources.append(s)\u001b[0m\n",
      "            for cy_source in cy_sources:\n",
      "                c_source = (cy_source[:(- 3)] + 'c')\n",
      "                if os.path.exists(c_source):\n",
      "                    log.info('removing %s', c_source)\n",
      "                    os.remove(c_source)\n",
      "                so_built = (cy_source[:(- 3)] + 'so')\n",
      "                if os.path.exists(so_built):\n",
      "                    log.info('removing %s', so_built)\n",
      "                    os.remove(so_built)\n",
      "ext_modules = [Extension('bvg', ['bvg.pyx'], include_dirs=['../include/'], library_dirs=['../'], libraries=['bvg'])]\n",
      "setup(name='python interface for libbvg', version='2.0.0', author='David F. Gleich and Wei-Yen Day', cmdclass={'build_ext': build_ext, 'clean': clean}, ext_modules=ext_modules)\n",
      "\n",
      "\n",
      "=====================6-PREDICTION=====================\n",
      "\n",
      "\n",
      "from distutils.core import setup\n",
      "from distutils.extension import Extension\n",
      "from Cython.Distutils import build_ext\n",
      "from distutils import log\n",
      "import os\n",
      "from distutils.command.clean import clean as _clean\n",
      "\n",
      "class clean(_clean):\n",
      "    '\\n    Subclass clean so it removes all the Cython generated C files.\\n    '\n",
      "\n",
      "    def run(self):\n",
      "        _clean.run(self)\n",
      "        for ext in self.distribution.ext_modules:\n",
      "            cy_sources = []\n",
      "            for s in ext.sources:\n",
      "                if s.endswith('.pyx'):\n",
      "                    cy_sources.append(s)\n",
      "            for cy_source in cy_sources:\n",
      "                c_source = (cy_source[:(- 3)] + 'c')\n",
      "                if os.path.exists(c_source):\n",
      "                    log.info('removing %s', c_source)\n",
      "                    os.remove(c_source)\n",
      "                so_built = (cy_source[:(- 3)] + 'so')\n",
      "                if os.path.exists(so_built):\n",
      "                 \n",
      "\n",
      "=====================6-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "from distutils.core import setup\n",
      "from distutils.extension import Extension\n",
      "from Cython.Distutils import build_ext\n",
      "from distutils import log\n",
      "import os\n",
      "from distutils.command.clean import clean as _clean\n",
      "\n",
      "class clean(_clean):\n",
      "    '\\n    Subclass clean so it removes all the Cython generated C files.\\n    '\n",
      "\n",
      "    def run(self):\n",
      "        _clean.run(self)\n",
      "        for ext in self.distribution.ext_modules:\n",
      "            cy_sources = [\u001b[31ms for s in ext.sources if s.endswith('.pyx')\u001b[0m]\n",
      "            for cy_source in cy_sources:\n",
      "                c_source = (cy_source[:(- 3)] + 'c')\n",
      "                if os.path.exists(c_source):\n",
      "                    log.info('removing %s', c_source)\n",
      "                    os.remove(c_source)\n",
      "                so_built = (cy_source[:(- 3)] + 'so')\n",
      "                if os.path.exists(so_built):\n",
      "                    log.info('removing %s', so_built)\n",
      "                    os.remove(so_built)\n",
      "ext_modules = [Extension('bvg', ['bvg.pyx'], include_dirs=['../include/'], library_dirs=['../'], libraries=['bvg'])]\n",
      "setup(name='python interface for libbvg', version='2.0.0', author='David F. Gleich and Wei-Yen Day', cmdclass={'build_ext': build_ext, 'clean': clean}, ext_modules=ext_modules)\n",
      "\n",
      "\n",
      "=====================6-DIFF_BLEU=====================\n",
      "\n",
      "0\n",
      "\n",
      "=====================8-INPUT=====================\n",
      "\n",
      "\n",
      "import logging\n",
      "from XmlParser import XmlParser\n",
      "from Component import Component\n",
      "\n",
      "class ComponentParser(XmlParser):\n",
      "    _root_tag = 'components'\n",
      "    _parse_type = 'component'\n",
      "\n",
      "    def __init__(self):\n",
      "        self._components = {}\n",
      "\n",
      "    @property\n",
      "    def components(self):\n",
      "        return self._components\n",
      "\n",
      "    def _parse(self, xmlmodel):\n",
      "        name = xmlmodel.get('name')\n",
      "        if (not name):\n",
      "            logging.warn('Ignoring component definition with no name')\n",
      "        else:\n",
      "            attributes = xmlmodel.attrib\n",
      "            categories = [category.get('name') for category in xmlmodel.findall('category')]\n",
      "            self._components[name] = Component(categories, name, attributes)\n",
      "\n",
      "    def resolve(self, categories):\n",
      "        for (name, component) in self._components.items():\n",
      "            for category_name in component.category_names:\n",
      "                category_link = [\u001b[31m]\n",
      "                for \u001b[0mcategory_link \u001b[31min categories:\n",
      "                   \u001b[0m if (category_link.catid == category_name)\u001b[31m:\n",
      "                        category_link.append(category_link)\u001b[0m\n",
      "                if category_link:\n",
      "                    category = category_link[0]\n",
      "                    component.set_category_link(category_name, category)\n",
      "                    category.set_component(component)\n",
      "                else:\n",
      "                    logging.warn(\"Component '{0}' references undefined category '{1}'\".format(name, category_name))\n",
      "\n",
      "\n",
      "=====================8-PREDICTION=====================\n",
      "\n",
      "\n",
      "import logging\n",
      "from XmlParser import XmlParser\n",
      "from Component import Component\n",
      "\n",
      "class ComponentParser(XmlParser):\n",
      "    _root_tag = 'components'\n",
      "    _parse_type = 'component'\n",
      "\n",
      "    def __init__(self):\n",
      "        self._components = {}\n",
      "\n",
      "    @property\n",
      "    def components(self):\n",
      "        return self._components\n",
      "\n",
      "    def _parse(self, xmlmodel):\n",
      "        name = xmlmodel.get('name')\n",
      "        if (not name):\n",
      "            logging.warn('Ignoring component definition with no name')\n",
      "        else:\n",
      "            attributes = xmlmodel.attrib\n",
      "            categories = [category.get('name') for category in xmlmodel.findall('category')]\n",
      "            self._components[name] = Component(categories, name, attributes)\n",
      "\n",
      "    def resolve(self, categories):\n",
      "        for (name, component) in self._components.items():\n",
      "            for category_name in component.category_names:\n",
      "                category_link = [category_link \u001b[31mfor category_link in categories if \u001b[0mcategory_link.catid == category_name\u001b[31m]\u001b[0m\n",
      "                if category_link:\n",
      "                    category = category_link[0]\n",
      "                    component.set_category_link(category_name, category)\n",
      "                    category.set_component(component)\n",
      "                else:\n",
      "                    logging.warn(\"Component '{0}' references undefined category '{1}'\".format(name, category_name))\n",
      "\u001b[31m\n",
      "Original\u001b[0m\n",
      "\n",
      "=====================8-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import logging\n",
      "from XmlParser import XmlParser\n",
      "from Component import Component\n",
      "\n",
      "class ComponentParser(XmlParser):\n",
      "    _root_tag = 'components'\n",
      "    _parse_type = 'component'\n",
      "\n",
      "    def __init__(self):\n",
      "        self._components = {}\n",
      "\n",
      "    @property\n",
      "    def components(self):\n",
      "        return self._components\n",
      "\n",
      "    def _parse(self, xmlmodel):\n",
      "        name = xmlmodel.get('name')\n",
      "        if (not name):\n",
      "            logging.warn('Ignoring component definition with no name')\n",
      "        else:\n",
      "            attributes = xmlmodel.attrib\n",
      "            categories = [category.get('name') for category in xmlmodel.findall('category')]\n",
      "            self._components[name] = Component(categories, name, attributes)\n",
      "\n",
      "    def resolve(self, categories):\n",
      "        for (name, component) in self._components.items():\n",
      "            for category_name in component.category_names:\n",
      "                category_link = [category_link \u001b[31mfor category_link in categories\u001b[0m if (category_link.catid == category_name)\u001b[31m]\u001b[0m\n",
      "                if category_link:\n",
      "                    category = category_link[0]\n",
      "                    component.set_category_link(category_name, category)\n",
      "                    category.set_component(component)\n",
      "                else:\n",
      "                    logging.warn(\"Component '{0}' references undefined category '{1}'\".format(name, category_name))\n",
      "\n",
      "\n",
      "=====================8-DIFF_BLEU=====================\n",
      "\n",
      "8.38826642100846e-155\n",
      "\n",
      "=====================9-INPUT=====================\n",
      "\n",
      "\n",
      "import re\n",
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "class listBerita():\n",
      "\n",
      "    def __init__(self, link):\n",
      "        self.link = link\n",
      "\n",
      "    def daftarBerita(self):\n",
      "        page = urllib.request.urlopen(self.link)\n",
      "        soup = BeautifulSoup(page, 'html.parser')\n",
      "        artikel = soup.find_all('div', class_='article__title')\n",
      "        dataList = {}\n",
      "        counter = 0\n",
      "        for element in artikel:\n",
      "            dataList[counter] = {}\n",
      "            dataList[counter]['judul'] = element.a.get_text()\n",
      "            dataList[counter]['link'] = element.a['href']\n",
      "            counter = (counter + 1)\n",
      "        beritanotVideo = [\u001b[31m]\n",
      "       \u001b[0m for (data, info) in dataList.items()\u001b[31m:\n",
      "            if (not ('\u001b[0mgaleri' in info['link']))\u001b[31m:\n",
      "                if (not info['judul'].lower().startswith(('video', 'vlog'))):\n",
      "                    beritanotVideo.append(data)\u001b[0m\n",
      "        dataListBaru = {}\n",
      "        for x in beritanotVideo:\n",
      "            dataListBaru[x] = {}\n",
      "            dataListBaru[x]['judul'] = dataList[x]['judul']\n",
      "            dataListBaru[x]['link'] = dataList[x]['link']\n",
      "        return dataListBaru\n",
      "\n",
      "\n",
      "=====================9-PREDICTION=====================\n",
      "\n",
      "\n",
      "import re\n",
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "class listBerita():\n",
      "\n",
      "    def __init__(self, link):\n",
      "        self.link = link\n",
      "\n",
      "    def daftarBerita(self):\n",
      "        page = urllib.request.urlopen(self.link)\n",
      "        soup = BeautifulSoup(page, 'html.parser')\n",
      "        artikel = soup.find_all('div', class_='article__title')\n",
      "        dataList = {}\n",
      "        counter = 0\n",
      "        for element in artikel:\n",
      "            dataList[counter] = {}\n",
      "            dataList[counter]['judul'] = element.a.get_text()\n",
      "            dataList[counter]['link'] = element.a['href']\n",
      "            counter = (counter + 1)\n",
      "        beritanotVideo = [\u001b[31mdata\u001b[0m for (data, info) in dataList.items() if (not ('galeri' in info['link']))\u001b[31m and\u001b[0m (not info['judul'].lower().startswith(('video', 'vlog')))\u001b[31m]\u001b[0m\n",
      "        dataListBaru = {}\n",
      "        for x in beritanotVideo:\n",
      "            dataListBaru[x] = {}\n",
      "            dataListBaru[x]['judul'] = dataList[x]['judul']\n",
      "            dataListBaru[x]['link'] = dataList[x]['link']\n",
      "        return dataListBaru\n",
      "\u001b[31m\n",
      "Original\u001b[0m\n",
      "\n",
      "=====================9-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "import re\n",
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "class listBerita():\n",
      "\n",
      "    def __init__(self, link):\n",
      "        self.link = link\n",
      "\n",
      "    def daftarBerita(self):\n",
      "        page = urllib.request.urlopen(self.link)\n",
      "        soup = BeautifulSoup(page, 'html.parser')\n",
      "        artikel = soup.find_all('div', class_='article__title')\n",
      "        dataList = {}\n",
      "        counter = 0\n",
      "        for element in artikel:\n",
      "            dataList[counter] = {}\n",
      "            dataList[counter]['judul'] = element.a.get_text()\n",
      "            dataList[counter]['link'] = element.a['href']\n",
      "            counter = (counter + 1)\n",
      "        beritanotVideo = [\u001b[31mdata\u001b[0m for (data, info) in dataList.items()\u001b[31m if (not info['judul'].lower().startswith(('video', 'vlo\u001b[0mg\u001b[31m'))) if (not ('g\u001b[0maleri' in info['link']))\u001b[31m]\u001b[0m\n",
      "        dataListBaru = {}\n",
      "        for x in beritanotVideo:\n",
      "            dataListBaru[x] = {}\n",
      "            dataListBaru[x]['judul'] = dataList[x]['judul']\n",
      "            dataListBaru[x]['link'] = dataList[x]['link']\n",
      "        return dataListBaru\n",
      "\n",
      "\n",
      "=====================9-DIFF_BLEU=====================\n",
      "\n",
      "6.416038883891965e-155\n",
      "\n",
      "=====================11-INPUT=====================\n",
      "\n",
      "\n",
      "\n",
      "class Solution(object):\n",
      "\n",
      "    def validTree(self, n, edges):\n",
      "        '\\n        O(N)\\n        O(N)\\n        :type n: int\\n        :type edges: List[List[int]]\\n        :rtype: bool\\n        '\n",
      "        group = []\u001b[31m\n",
      "        for i in range(n):\n",
      "            group.append(i)\u001b[0m\n",
      "        for (e1, e2) in edges:\n",
      "            root1 = self.find(e1, group)\n",
      "            root2 = self.find(e2, group)\n",
      "            if (root1 == root2):\n",
      "                return False\n",
      "            else:\n",
      "                group[root2] = root1\n",
      "        return (len(edges) == (n - 1))\n",
      "\n",
      "    def find(self, e, group):\n",
      "        if (e == group[e]):\n",
      "            return e\n",
      "        else:\n",
      "            return self.find(group[e], group)\n",
      "\n",
      "\n",
      "=====================11-PREDICTION=====================\n",
      "\n",
      "\n",
      "class Solution(object):\n",
      "\n",
      "    def validTree(self, n, edges):\n",
      "        '\\n        O(N)\\n        O(N)\\n        :type n: int\\n        :type edges: List[List[int]]\\n        :rtype: bool\\n        '\n",
      "        group = [\u001b[31mi for i in range(n)\u001b[0m]\n",
      "        for (e1, e2) in edges:\n",
      "            root1 = self.find(e1, group)\n",
      "            root2 = self.find(e2, group)\n",
      "            if (root1 == root2):\n",
      "                return False\n",
      "            else:\n",
      "                group[root2] = root1\n",
      "        return (len(edges) == (n - 1))\n",
      "\n",
      "    def find(self, e, group):\n",
      "        if (e == group[e]):\n",
      "            return e\n",
      "        else:\n",
      "            return self.find(group[e], group)\n",
      "\u001b[31m\n",
      "Original\u001b[0m\n",
      "\n",
      "=====================11-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "\n",
      "class Solution(object):\n",
      "\n",
      "    def validTree(self, n, edges):\n",
      "        '\\n        O(N)\\n        O(N)\\n        :type n: int\\n        :type edges: List[List[int]]\\n        :rtype: bool\\n        '\n",
      "        group = [\u001b[31mi for i in range(n)\u001b[0m]\n",
      "        for (e1, e2) in edges:\n",
      "            root1 = self.find(e1, group)\n",
      "            root2 = self.find(e2, group)\n",
      "            if (root1 == root2):\n",
      "                return False\n",
      "            else:\n",
      "                group[root2] = root1\n",
      "        return (len(edges) == (n - 1))\n",
      "\n",
      "    def find(self, e, group):\n",
      "        if (e == group[e]):\n",
      "            return e\n",
      "        else:\n",
      "            return self.find(group[e], group)\n",
      "\n",
      "\n",
      "=====================11-DIFF_BLEU=====================\n",
      "\n",
      "1.9501786259064824e-103\n",
      "\n",
      "=====================14-INPUT=====================\n",
      "\n",
      "\n",
      "__author__ = 'Chris Mitchell'\n",
      "import os, unittest, hashlib\n",
      "import pythomics.genomics.parsers as parser\n",
      "\n",
      "class Test_VCF_Iterator(unittest.TestCase):\n",
      "\n",
      "    def setUp(self):\n",
      "        base_dir = os.path.split(__file__)[0]\n",
      "        data_dir = os.path.join(base_dir, 'fixtures')\n",
      "        self.handle = os.path.join(data_dir, 'valid-4.0.vcf')\n",
      "\n",
      "    def test_vcf_iterator(self):\n",
      "        out = ''\n",
      "        f = parser.VCFIterator(self.handle)\n",
      "        assert isinstance(f, parser.VCFIterator)\n",
      "        entries = []\u001b[31m\n",
      "        for row in f:\n",
      "            entries.append(str(row))\u001b[0m\n",
      "        out = '\\n'.join(entries)\n",
      "        digest = hashlib.sha224(out.encode('utf-8')).hexdigest()\n",
      "        self.assertEqual('013ed0dc8b1a1de19d1a2997e95b40b9070c6b1d32f931f568816a0a', digest, 'VCF Iterator Failure')\n",
      "\n",
      "    def test_vcf_zygosity(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf_variants(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf_filters(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf_alleles(self):\n",
      "        pass\n",
      "suite = unittest.TestLoader().loadTestsFromTestCase(Test_VCF_Iterator)\n",
      "unittest.TextTestRunner(verbosity=2).run(suite)\n",
      "\n",
      "\n",
      "=====================14-PREDICTION=====================\n",
      "\n",
      "\n",
      "__author__ = 'Chris Mitchell'\n",
      "import os, unittest, hashlib\n",
      "import pythomics.genomics.parsers as parser\n",
      "\n",
      "class Test_VCF_Iterator(unittest.TestCase):\n",
      "\n",
      "    def setUp(self):\n",
      "        base_dir = os.path.split(__file__)[0]\n",
      "        data_dir = os.path.join(base_dir, 'fixtures')\n",
      "        self.handle = os.path.join(data_dir, 'valid-4.0.vcf')\n",
      "\n",
      "    def test_vcf_iterator(self):\n",
      "        out = ''\n",
      "        f = parser.VCFIterator(self.handle)\n",
      "        assert isinstance(f, parser.VCFIterator)\n",
      "        entries = [\u001b[31mstr(row) for row in f\u001b[0m]\n",
      "        out = '\\n'.join(entries)\n",
      "        digest = hashlib.sha224(out.encode('utf-8')).hexdigest()\n",
      "        self.assertEqual('013ed0dc8b1a1de19d1a2997e95b40b9070c6b1d32f931f568816a0a', digest, 'VCF Iterator Failure')\n",
      "\n",
      "    def test_vcf_zygosity(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf\n",
      "\n",
      "=====================14-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "__author__ = 'Chris Mitchell'\n",
      "import os, unittest, hashlib\n",
      "import pythomics.genomics.parsers as parser\n",
      "\n",
      "class Test_VCF_Iterator(unittest.TestCase):\n",
      "\n",
      "    def setUp(self):\n",
      "        base_dir = os.path.split(__file__)[0]\n",
      "        data_dir = os.path.join(base_dir, 'fixtures')\n",
      "        self.handle = os.path.join(data_dir, 'valid-4.0.vcf')\n",
      "\n",
      "    def test_vcf_iterator(self):\n",
      "        out = ''\n",
      "        f = parser.VCFIterator(self.handle)\n",
      "        assert isinstance(f, parser.VCFIterator)\n",
      "        entries = [\u001b[31mstr(row) for row in f\u001b[0m]\n",
      "        out = '\\n'.join(entries)\n",
      "        digest = hashlib.sha224(out.encode('utf-8')).hexdigest()\n",
      "        self.assertEqual('013ed0dc8b1a1de19d1a2997e95b40b9070c6b1d32f931f568816a0a', digest, 'VCF Iterator Failure')\n",
      "\n",
      "    def test_vcf_zygosity(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf_variants(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf_filters(self):\n",
      "        pass\n",
      "\n",
      "    def test_vcf_alleles(self):\n",
      "        pass\n",
      "suite = unittest.TestLoader().loadTestsFromTestCase(Test_VCF_Iterator)\n",
      "unittest.TextTestRunner(verbosity=2).run(suite)\n",
      "\n",
      "\n",
      "=====================14-DIFF_BLEU=====================\n",
      "\n",
      "1.9501786259064824e-103\n",
      "\n",
      "=====================15-INPUT=====================\n",
      "\n",
      "\n",
      "'Module for the PathJoinSubstitution substitution.'\n",
      "import os\n",
      "from typing import Iterable\n",
      "from typing import Text\n",
      "from ..launch_context import LaunchContext\n",
      "from ..some_substitutions_type import SomeSubstitutionsType\n",
      "from ..substitution import Substitution\n",
      "\n",
      "class PathJoinSubstitution(Substitution):\n",
      "    'Substitution that join paths, in a platform independent way.'\n",
      "\n",
      "    def __init__(self, substitutions: Iterable[SomeSubstitutionsType]) -> None:\n",
      "        'Create a PathJoinSubstitution.'\n",
      "        from ..utilities import normalize_to_list_of_substitutions\n",
      "        self.__substitutions = normalize_to_list_of_substitutions(substitutions)\n",
      "\n",
      "    @property\n",
      "    def substitutions(self) -> Iterable[Substitution]:\n",
      "        'Getter for variable_name.'\n",
      "        return self.__substitutions\n",
      "\n",
      "    def describe(self) -> Text:\n",
      "        'Return a description of this substitution as a string.'\n",
      "        return \"LocalVar('{}')\".format(' + '.join([s.describe() for s in self.substitutions]))\n",
      "\n",
      "    def perform(self, context: LaunchContext) -> Text:\n",
      "        'Perform the substitution by retrieving the local variable.'\n",
      "        performed_substitutions = [\u001b[31m]\n",
      "        for sub in self.__substitutions:\n",
      "            performed_substitutions.append(\u001b[0msub.perform(context)\u001b[31m)\u001b[0m\n",
      "        return os.path.join(*performed_substitutions)\n",
      "\n",
      "\n",
      "=====================15-PREDICTION=====================\n",
      "\n",
      "\n",
      "'Module for the PathJoinSubstitution substitution.'\n",
      "import os\n",
      "from typing import Iterable\n",
      "from typing import Text\n",
      "from ..launch_context import LaunchContext\n",
      "from ..some_substitutions_type import SomeSubstitutionsType\n",
      "from ..substitution import Substitution\n",
      "\n",
      "class PathJoinSubstitution(Substitution):\n",
      "    'Substitution that join paths, in a platform independent way.'\n",
      "\n",
      "    def __init__(self, substitutions: Iterable[SomeSubstitutionsType]) -> None:\n",
      "        'Create a PathJoinSubstitution.'\n",
      "        from ..utilities import normalize_to_list_of_substitutions\n",
      "        self.__substitutions = normalize_to_list_of_substitutions(substitutions)\n",
      "\n",
      "    @property\n",
      "    def substitutions(self) -> Iterable[Substitution]:\n",
      "        'Getter for variable_name.'\n",
      "        return self.__substitutions\n",
      "\n",
      "    def describe(self) -> Text:\n",
      "        'Return a description of this substitution as a string.'\n",
      "        return \"LocalVar('{}')\".format(' + '.join([s.describe() for s in self.substitutions]))\n",
      "\n",
      "    def perform(self, context: LaunchContext) -> Text:\n",
      "        'Perform the substitution by retrieving the local variable.'\n",
      "        \u001b[31mreturn os.path.\u001b[0m\n",
      "\n",
      "=====================15-GOLD LABELS=====================\n",
      "\n",
      "\n",
      "'Module for the PathJoinSubstitution substitution.'\n",
      "import os\n",
      "from typing import Iterable\n",
      "from typing import Text\n",
      "from ..launch_context import LaunchContext\n",
      "from ..some_substitutions_type import SomeSubstitutionsType\n",
      "from ..substitution import Substitution\n",
      "\n",
      "class PathJoinSubstitution(Substitution):\n",
      "    'Substitution that join paths, in a platform independent way.'\n",
      "\n",
      "    def __init__(self, substitutions: Iterable[SomeSubstitutionsType]) -> None:\n",
      "        'Create a PathJoinSubstitution.'\n",
      "        from ..utilities import normalize_to_list_of_substitutions\n",
      "        self.__substitutions = normalize_to_list_of_substitutions(substitutions)\n",
      "\n",
      "    @property\n",
      "    def substitutions(self) -> Iterable[Substitution]:\n",
      "        'Getter for variable_name.'\n",
      "        return self.__substitutions\n",
      "\n",
      "    def describe(self) -> Text:\n",
      "        'Return a description of this substitution as a string.'\n",
      "        return \"LocalVar('{}')\".format(' + '.join([s.describe() for s in self.substitutions]))\n",
      "\n",
      "    def perform(self, context: LaunchContext) -> Text:\n",
      "        'Perform the substitution by retrieving the local variable.'\n",
      "        performed_substitutions = [sub.perform(context)\u001b[31m for sub in self.__substitutions]\u001b[0m\n",
      "        return os.path.join(*performed_substitutions)\n",
      "\n",
      "\n",
      "=====================15-DIFF_BLEU=====================\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "lookup_examples(list_comp_codex_report, 0.01, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6f2ca-1dd3-48de-add3-22d6a8f578d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## decorator - post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c27f17b0-e040-4b9b-8c80-be648804dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/data/ken/data/code\"\n",
    "model_name = \".\"\n",
    "ckpt = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee820a62-b3aa-4ae1-ad7f-b2c4d35283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_csvfile = \"decorator.output_post_process.csv\"\n",
    "file_name = f\"{folder}/{model_name}/{ckpt}/{pred_csvfile}\"\n",
    "target_feats = [\"decorator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7939aa1-5cea-4dd1-aed9-857923cb2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorator_codex_pred_df = pd.read_csv(file_name)\n",
    "decorator_codex_pred_df = get_valid_pred_df(decorator_codex_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786e1549-24ba-4192-8dda-4e7febd7081d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram': 0.634677744038495,\n",
       " 'weighted_ngram': 0.6514251474979365,\n",
       " 'syntax_match': 0.7381001301280734,\n",
       " 'dataflow_match': 0.6642056330325403,\n",
       " 'code_bleu': 0.6721021636742612}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decorator_codex_codebleu = evaluate_codebleu(file_name, replaced_df=decorator_codex_pred_df, weights='0.25,0.25,0.25,0.25')\n",
    "decorator_codex_codebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339057ef-288c-4f7d-82aa-7b276e6150f8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d77e741c7c74a7baad8e31d8d0188a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ken/miniconda3/envs/code/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    }
   ],
   "source": [
    "decorator_codex_report = evaluate_pred_df(decorator_codex_pred_df, target_feats, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2269992-980e-4f10-92ce-b70dafbbc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.005780346820809248\n",
      "codebleu_above_90 : 0.08670520231213873\n",
      "diff_bleu_avg : 0.01865318485074729\n",
      "diff_bleu_perfect : 0.017341040462427744\n",
      "diff_bleu_above_90 : 0.017341040462427744\n",
      "parse_test_accuracy : 0.3583815028901734\n"
     ]
    }
   ],
   "source": [
    "for key, val in decorator_codex_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd99295f-46f1-4f76-85e5-507314791019",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79a4e0db9a142169a81493ba12f4016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    }
   ],
   "source": [
    "decorator_codex_report = evaluate_pred_df(decorator_codex_pred_df, target_feats, clean_diff=True, is_nl=True, parse_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bc4298-67cf-4f83-9b72-ecef0d687f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebleu_perfect : 0.005780346820809248\n",
      "codebleu_above_90 : 0.08670520231213873\n",
      "diff_bleu_avg : 0.06637365209541911\n",
      "diff_bleu_perfect : 0.057803468208092484\n",
      "diff_bleu_above_90 : 0.057803468208092484\n",
      "parse_test_accuracy : 0.3583815028901734\n"
     ]
    }
   ],
   "source": [
    "for key, val in decorator_codex_report.items():\n",
    "    if type(val) != list and len(val.shape) == 0:\n",
    "        print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a5faaa-2224-42ca-afc9-17859ab74d7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================9-INPUT=====================\n",
      "\n",
      "import os\n",
      "import pytest\n",
      "EXAMINATORS = ['bamboo.buildKey', 'BUILD_ID', 'BUILD_NUMBER', 'BUILDKITE', 'CI', 'CIRCLECI', 'CONTINUOUS_INTEGRATION', 'GITHUB_ACTIONS', 'HUDSON_URL', 'JENKINS_URL', 'TEAMCITY_VERSION', 'TRAVIS']\n",
      "\n",
      "def pytest_runtest_makereport(item):\n",
      "    \u001b[31m\"\"\"\u001b[0mFailing test cases are not a problem anymore.\u001b[31m\"\"\"\u001b[0m\n",
      "    outcome = (yield)\n",
      "    rep = outcome.get_result()\n",
      "    examinators = EXAMINATORS\n",
      "    for examinator in item.config.getini('vw_examinators').split('\\n'):\n",
      "        examinators.append(examinator.strip())\n",
      "    if any((os.environ.get(gaze, False) for gaze in examinators)):\n",
      "        rep.outcome = 'passed'\n",
      "\n",
      "def pytest_addoption(parser):\n",
      "    parser.addini('vw_examinators', 'List of additional VW examinators.')\n",
      "\n",
      "=====================9-PREDICTION=====================\n",
      "\n",
      "import os\n",
      "import pytest\n",
      "EXAMINATORS = ['bamboo.buildKey', 'BUILD_ID', 'BUILD_NUMBER', 'BUILDKITE', 'CI', 'CIRCLECI', 'CONTINUOUS_INTEGRATION', 'GITHUB_ACTIONS', 'HUDSON_URL', 'JENKINS_URL', 'TEAMCITY_VERSION', 'TRAVIS']\n",
      "\u001b[31m\n",
      "@pytest.hookimpl(hookwrapper=True)\u001b[0m\n",
      "def pytest_runtest_makereport(item):\n",
      "    \"\"\"Failing test cases are not a problem anymore.\"\"\"\n",
      "    outcome = (yield)\n",
      "    rep = outcome.get_result()\n",
      "    examinators = EXAMINATORS\n",
      "    for examinator in item.config.getini('vw_examinators').split('\\n\n",
      "\n",
      "=====================9-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport os\n",
      "import pytest\n",
      "EXAMINATORS = ['bamboo.buildKey', 'BUILD_ID', 'BUILD_NUMBER', 'BUILDKITE', 'CI', 'CIRCLECI', 'CONTINUOUS_INTEGRATION', 'GITHUB_ACTIONS', 'HUDSON_URL', 'JENKINS_URL', 'TEAMCITY_VERSION', 'TRAVIS']\n",
      "\n",
      "\u001b[31m@pytest.hookimpl(hookwrapper=True)\n",
      "\u001b[0mdef pytest_runtest_makereport(item):\n",
      "    \u001b[31m'\u001b[0mFailing test cases are not a problem anymore.\u001b[31m'\u001b[0m\n",
      "    outcome = (yield)\n",
      "    rep = outcome.get_result()\n",
      "    examinators = EXAMINATORS\n",
      "    for examinator in item.config.getini('vw_examinators').split('\\n'):\n",
      "        examinators.append(examinator.strip())\n",
      "    if any((os.environ.get(gaze, False) for gaze in examinators)):\n",
      "        rep.outcome = 'passed'\n",
      "\n",
      "def pytest_addoption(parser):\n",
      "    parser.addini('vw_examinators', 'List of additional VW examinators.')\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================9-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================16-INPUT=====================\n",
      "\n",
      "\u001b[31m\"\"\"\n",
      "\u001b[0mSupport for GEOS prepared geometry operations.\u001b[31m\n",
      "\"\"\"\u001b[0m\n",
      "from shapely.geos import lgeos\n",
      "from shapely.impl import DefaultImplementation, delegated\n",
      "\n",
      "class PreparedGeometry(object):\n",
      "    \u001b[31m\"\"\"\n",
      "\u001b[0m    A geometry prepared for efficient comparison to a set of other geometries.\u001b[31m\n",
      "    \n",
      "\u001b[0m    Example:\u001b[31m\n",
      "      \n",
      "\u001b[0m      >>> from shapely.geometry import Point, Polygon\u001b[31m\n",
      "\u001b[0m      >>> triangle = Polygon(((0.0, 0.0), (1.0, 1.0), (1.0, -1.0)))\u001b[31m\n",
      "\u001b[0m      >>> p = prep(triangle)\u001b[31m\n",
      "\u001b[0m      >>> p.intersects(Point(0.5, 0.5))\u001b[31m\n",
      "\u001b[0m      True\u001b[31m\n",
      "    \"\"\"\u001b[0m\n",
      "    impl = DefaultImplementation\n",
      "\n",
      "    def __init__(self, context):\n",
      "        self.context = context\n",
      "        self.__geom__ = lgeos.GEOSPrepare(self.context._geom)\n",
      "\n",
      "    def __del__(self):\n",
      "        if self.__geom__ is not None:\n",
      "            try:\n",
      "                lgeos.GEOSPreparedGeom_destroy(self.__geom__)\n",
      "            except AttributeError:\n",
      "                pass\n",
      "        self.__geom__ = None\n",
      "        self.context = None\n",
      "\n",
      "    def _geom(self):\n",
      "        return self.__geom__\n",
      "\n",
      "    def intersects(self, other):\n",
      "        return bool(self.impl['prepared_intersects'](self, other))\n",
      "\n",
      "    def contains(self, other):\n",
      "        return bool(self.impl['prepared_contains'](self, other))\n",
      "\n",
      "    def contains_properly(self, other):\n",
      "        return bool(self.impl['prepared_contains_properly'](self, other))\n",
      "\n",
      "    def covers(self, other):\n",
      "        return bool(self.impl['prepared_covers'](self, other))\n",
      "\n",
      "def prep(ob):\n",
      "    \u001b[31m\"\"\"\u001b[0mCreates and returns a prepared geometric object.\u001b[31m\"\"\"\u001b[0m\n",
      "    return PreparedGeometry(ob)\n",
      "\n",
      "=====================16-PREDICTION=====================\n",
      "\n",
      "\"\"\"\n",
      "Support for GEOS prepared geometry operations.\n",
      "\"\"\"\n",
      "from shapely.geos import lgeos\n",
      "from shapely.impl import DefaultImplementation, delegated\n",
      "\n",
      "class PreparedGeometry(object):\n",
      "    \"\"\"\n",
      "    A geometry prepared for efficient comparison to a set of other geometries.\n",
      "    \n",
      "    Example:\n",
      "      \n",
      "      >>> from shapely.geometry import Point, Polygon\n",
      "      >>> triangle = Polygon(((0.0, 0.0), (1.0, 1.0), (1.0, -1.0)))\n",
      "      >>> p = prep(triangle)\n",
      "      >>> p.intersects(Point(0.5, 0.5))\n",
      "      True\n",
      "    \"\"\"\n",
      "    impl = DefaultImplementation\n",
      "\n",
      "    def __init__(self, context):\n",
      "        self.context = context\n",
      "        self.__geom__ = lgeos.GEOSPrepare(self.context._geom)\n",
      "\n",
      "    def __del__(self):\n",
      "        if self.__geom__ is not None:\n",
      "            try:\n",
      "                lgeos.GEOSPreparedGeom_destroy(self.__geom__)\n",
      "            except AttributeError:\n",
      "                pass\n",
      "        self.__geom__ = None\n",
      "        self.context = None\n",
      "\n",
      "    def _geom(self):\n",
      "        return self.__geom__\n",
      "\n",
      "    \u001b[31m@delegated\n",
      "    \u001b[0mdef intersects(self, other):\n",
      "        return bool(self.impl['prepared_intersects'](self, other))\n",
      "\n",
      "    \u001b[31m@delegated\n",
      "    \u001b[0mdef contains(self, other):\n",
      "        return bool(self.impl['prepared_contains'](self, other))\n",
      "\u001b[31m\n",
      "    @delegated\u001b[0m\n",
      "    def contains_properly(self, other):\n",
      "        return bool(self.impl['prepared_contains_properly'](self, other))\n",
      "\n",
      "   \n",
      "\n",
      "=====================16-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "'\\n\u001b[0mSupport for GEOS prepared geometry operations.\u001b[31m\\n'\u001b[0m\n",
      "from shapely.geos import lgeos\n",
      "from shapely.impl import DefaultImplementation, delegated\n",
      "\n",
      "class PreparedGeometry(object):\n",
      "    \u001b[31m'\\n\u001b[0m    A geometry prepared for efficient comparison to a set of other geometries.\u001b[31m\\n    \\n\u001b[0m    Example:\u001b[31m\\n      \\n\u001b[0m      >>> from shapely.geometry import Point, Polygon\u001b[31m\\n\u001b[0m      >>> triangle = Polygon(((0.0, 0.0), (1.0, 1.0), (1.0, -1.0)))\u001b[31m\\n\u001b[0m      >>> p = prep(triangle)\u001b[31m\\n\u001b[0m      >>> p.intersects(Point(0.5, 0.5))\u001b[31m\\n\u001b[0m      True\u001b[31m\\n    '\u001b[0m\n",
      "    impl = DefaultImplementation\n",
      "\n",
      "    def __init__(self, context):\n",
      "        self.context = context\n",
      "        self.__geom__ = lgeos.GEOSPrepare(self.context._geom)\n",
      "\n",
      "    def __del__(self):\n",
      "        if \u001b[31m(\u001b[0mself.__geom__ is not None\u001b[31m)\u001b[0m:\n",
      "            try:\n",
      "                lgeos.GEOSPreparedGeom_destroy(self.__geom__)\n",
      "            except AttributeError:\n",
      "                pass\n",
      "        self.__geom__ = None\n",
      "        self.context = None\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef _geom(self):\n",
      "        return self.__geom__\n",
      "\u001b[31m\n",
      "    @delegated\u001b[0m\n",
      "    def intersects(self, other):\n",
      "        return bool(self.impl['prepared_intersects'](self, other))\n",
      "\n",
      "    \u001b[31m@delegated\n",
      "    \u001b[0mdef contains(self, other):\n",
      "        return bool(self.impl['prepared_contains'](self, other))\n",
      "\u001b[31m\n",
      "    @delegated\u001b[0m\n",
      "    def contains_properly(self, other):\n",
      "        return bool(self.impl['prepared_contains_properly'](self, other))\n",
      "\n",
      "    \u001b[31m@delegated\n",
      "    \u001b[0mdef covers(self, other):\n",
      "        return bool(self.impl['prepared_covers'](self, other))\n",
      "\n",
      "def prep(ob):\n",
      "    \u001b[31m'\u001b[0mCreates and returns a prepared geometric object.\u001b[31m'\u001b[0m\n",
      "    return PreparedGeometry(ob)\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================16-DIFF_BLEU=====================\n",
      "\n",
      "0.513417119032592\n",
      "\n",
      "=====================36-INPUT=====================\n",
      "\n",
      "import click\n",
      "from arrow.cli import pass_context, json_loads\n",
      "from arrow.decorators import custom_exception, dict_output\n",
      "\n",
      "def cli(ctx):\n",
      "    \u001b[31m\"\"\"\u001b[0mGet the search tools available\u001b[31m\n",
      "\n",
      "\u001b[0mOutput:\u001b[31m\n",
      "\n",
      "\u001b[0m    dictionary containing the search tools and their metadata.\u001b[31m\n",
      "\u001b[0m          For example::\u001b[31m\n",
      "\n",
      "\u001b[0m            {\u001b[31m\n",
      "\u001b[0m                \"sequence_search_tools\": {\u001b[31m\n",
      "\u001b[0m                    \"blat_prot\": {\u001b[31m\n",
      "\u001b[0m                        \"name\": \"Blat protein\",\u001b[31m\n",
      "\u001b[0m                        \"search_class\": \"org.bbop.apollo.sequence.search.blat.BlatCommandLineProteinToNucleotide\",\u001b[31m\n",
      "\u001b[0m                        \"params\": \"\",\u001b[31m\n",
      "\u001b[0m                        \"search_exe\": \"/usr/local/bin/blat\"\u001b[31m\n",
      "\u001b[0m                    },\u001b[31m\n",
      "\u001b[0m                    \"blat_nuc\": {\u001b[31m\n",
      "\u001b[0m                        \"name\": \"Blat nucleotide\",\u001b[31m\n",
      "\u001b[0m                        \"search_class\": \"org.bbop.apollo.sequence.search.blat.BlatCommandLineNucleotideToNucleotide\",\u001b[31m\n",
      "\u001b[0m                        \"params\": \"\",\u001b[31m\n",
      "\u001b[0m                        \"search_exe\": \"/usr/local/bin/blat\"\u001b[31m\n",
      "\u001b[0m                    }\u001b[31m\n",
      "\u001b[0m                }\u001b[31m\n",
      "\u001b[0m            }\u001b[31m\n",
      "    \"\"\"\u001b[0m\n",
      "    return ctx.gi.annotations.get_search_tools()\n",
      "\n",
      "=====================36-PREDICTION=====================\n",
      "\n",
      "import click\n",
      "from arrow.cli import pass_context, json_loads\n",
      "from arrow.decorators import custom_exception, dict_output\n",
      "\u001b[31m\n",
      "@click.command('get_search_tools')\n",
      "@pass_context\n",
      "@custom_exception\n",
      "@dict_output\u001b[0m\n",
      "def cli(ctx):\n",
      "    \"\"\"Get the search tools available\n",
      "\n",
      "Output:\n",
      "\n",
      "    dictionary containing the search tools and their metadata.\n",
      "          For example::\n",
      "\n",
      "            {\n",
      "                \"sequence_search_tools\": {\n",
      "                    \"blat_prot\": {\n",
      "                        \"name\": \"Blat protein\",\n",
      "                        \"search_class\": \"org.bbop.apollo.sequence.search.blat.BlatCommandLineProteinToNucleotide\",\n",
      "                        \"params\": \"\",\n",
      "                        \"search_exe\": \"/usr/local/bin/blat\"\n",
      "                    },\n",
      "                    \"blat_nuc\": {\n",
      "                        \"name\": \"Blat nucleotide\",\n",
      "                        \"search_class\": \"org.bbop.apollo.sequence.search.blat.BlatCommandLineNucleotideToNucleotide\",\n",
      "                        \"params\": \"\",\n",
      "                        \"search_exe\": \"/usr/local/bin/blat\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "    \"\"\"\n",
      "    return ctx.gi.annotations.get_search_tools()\u001b[31m\n",
      "\n",
      "\n",
      "Original\u001b[0m\n",
      "\n",
      "=====================36-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport click\n",
      "from arrow.cli import pass_context, json_loads\n",
      "from arrow.decorators import custom_exception, dict_output\n",
      "\n",
      "\u001b[31m@click.command('get_search_tools')\n",
      "@pass_context\n",
      "@custom_exception\n",
      "@dict_output\n",
      "\u001b[0mdef cli(ctx):\n",
      "    \u001b[31m'\u001b[0mGet the search tools available\u001b[31m\\n\\n\u001b[0mOutput:\u001b[31m\\n\\n\u001b[0m    dictionary containing the search tools and their metadata.\u001b[31m\\n\u001b[0m          For example::\u001b[31m\\n\\n\u001b[0m            {\u001b[31m\\n\u001b[0m                \"sequence_search_tools\": {\u001b[31m\\n\u001b[0m                    \"blat_prot\": {\u001b[31m\\n\u001b[0m                        \"name\": \"Blat protein\",\u001b[31m\\n\u001b[0m                        \"search_class\": \"org.bbop.apollo.sequence.search.blat.BlatCommandLineProteinToNucleotide\",\u001b[31m\\n\u001b[0m                        \"params\": \"\",\u001b[31m\\n\u001b[0m                        \"search_exe\": \"/usr/local/bin/blat\"\u001b[31m\\n\u001b[0m                    },\u001b[31m\\n\u001b[0m                    \"blat_nuc\": {\u001b[31m\\n\u001b[0m                        \"name\": \"Blat nucleotide\",\u001b[31m\\n\u001b[0m                        \"search_class\": \"org.bbop.apollo.sequence.search.blat.BlatCommandLineNucleotideToNucleotide\",\u001b[31m\\n\u001b[0m                        \"params\": \"\",\u001b[31m\\n\u001b[0m                        \"search_exe\": \"/usr/local/bin/blat\"\u001b[31m\\n\u001b[0m                    }\u001b[31m\\n\u001b[0m                }\u001b[31m\\n\u001b[0m            }\u001b[31m\\n    '\u001b[0m\n",
      "    return ctx.gi.annotations.get_search_tools()\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================36-DIFF_BLEU=====================\n",
      "\n",
      "0.668740304976422\n",
      "\n",
      "=====================42-INPUT=====================\n",
      "\n",
      "import datetime\n",
      "from collections import OrderedDict\n",
      "\n",
      "class Story:\n",
      "    \u001b[31m\"\"\"\n",
      "\u001b[0m    Represent a story we worked on\u001b[31m\n",
      "    \"\"\"\u001b[0m\n",
      "\n",
      "    def __init__(self, story):\n",
      "        self.story = story\n",
      "\n",
      "    def serialize(self):\n",
      "        return OrderedDict([('seamus_id', self.seamus_id), ('timestamp', self.timestamp), ('duration', self.duration), ('contributors', self.contributors), ('contribution', self.contribution)])\n",
      "\n",
      "    def timestamp(self):\n",
      "        seconds = (float(self.story['Timestamp']) - 25569) * 86400.0\n",
      "        dt = datetime.datetime.utcfromtimestamp(seconds)\n",
      "        return dt.replace(microsecond=0)\n",
      "\n",
      "    def seamus_id(self):\n",
      "        return str(int(float(self.story['Seamus ID'])))\n",
      "\n",
      "    def contribution(self):\n",
      "        return str(self.story['What did you do?'])\n",
      "\n",
      "    def contributors(self):\n",
      "        return str(self.story['Who worked on it?'])\n",
      "\n",
      "    def duration(self):\n",
      "        value = self.story['How long did it take? (hours)']\n",
      "        if value:\n",
      "            return abs(float(value))\n",
      "        else:\n",
      "            return float(0)\n",
      "\n",
      "=====================42-PREDICTION=====================\n",
      "\n",
      "import datetime\n",
      "from collections import OrderedDict\n",
      "\n",
      "class Story:\n",
      "    \"\"\"\n",
      "    Represent a story we worked on\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, story):\n",
      "        self.story = story\n",
      "\n",
      "    def serialize(self):\n",
      "        return OrderedDict([('seamus_id', self.seamus_id), ('timestamp', self.timestamp), ('duration', self.duration), ('contributors', self.contributors), ('contribution', self.contribution)])\n",
      "\u001b[31m\n",
      "    @property\u001b[0m\n",
      "    def timestamp(self):\n",
      "        seconds = (float(self.story['Timestamp']) - 25569) * 86400.0\n",
      "        dt = datetime.datetime.utcfromtimestamp(seconds)\n",
      "        return dt.replace(microsecond=0)\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef seamus_id(self):\n",
      "        return str(int(float(self.story['Seamus ID'])))\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef contribution(self):\n",
      "        return str(self.story['What did you do?'])\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef contributors(self):\n",
      "        return str(self.story['Who worked on it?'])\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef duration(self):\n",
      "        value = self.story['How long did it take? (hours)']\n",
      "        if value:\n",
      "            return abs(float(value))\n",
      "       \n",
      "\n",
      "=====================42-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport datetime\n",
      "from collections import OrderedDict\n",
      "\n",
      "class Story\u001b[31m()\u001b[0m:\n",
      "    \u001b[31m'\\n\u001b[0m    Represent a story we worked on\u001b[31m\\n    '\u001b[0m\n",
      "\n",
      "    def __init__(self, story):\n",
      "        self.story = story\n",
      "\n",
      "    def serialize(self):\n",
      "        return OrderedDict([('seamus_id', self.seamus_id), ('timestamp', self.timestamp), ('duration', self.duration), ('contributors', self.contributors), ('contribution', self.contribution)])\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef timestamp(self):\n",
      "        seconds = \u001b[31m(\u001b[0m(float(self.story['Timestamp']) - 25569) * 86400.0\u001b[31m)\u001b[0m\n",
      "        dt = datetime.datetime.utcfromtimestamp(seconds)\n",
      "        return dt.replace(microsecond=0)\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef seamus_id(self):\n",
      "        return str(int(float(self.story['Seamus ID'])))\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef contribution(self):\n",
      "        return str(self.story['What did you do?'])\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef contributors(self):\n",
      "        return str(self.story['Who worked on it?'])\n",
      "\n",
      "    \u001b[31m@property\n",
      "    \u001b[0mdef duration(self):\n",
      "        value = self.story['How long did it take? (hours)']\n",
      "        if value:\n",
      "            return abs(float(value))\n",
      "        else:\n",
      "            return float(0)\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================42-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================43-INPUT=====================\n",
      "\n",
      "import tuned.monitors\n",
      "import os\n",
      "\n",
      "class DiskMonitor(tuned.monitors.Monitor):\n",
      "    _supported_vendors = ['ATA', 'SCSI']\n",
      "\n",
      "    def _init_available_devices(cls):\n",
      "        block_devices = os.listdir('/sys/block')\n",
      "        available = set(filter(cls._is_device_supported, block_devices))\n",
      "        cls._available_devices = available\n",
      "        for d in available:\n",
      "            cls._load[d] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "    def _is_device_supported(cls, device):\n",
      "        vendor_file = '/sys/block/%s/device/vendor' % device\n",
      "        try:\n",
      "            vendor = open(vendor_file).read().strip()\n",
      "        except IOError:\n",
      "            return False\n",
      "        return \u001b[31mvendor in cls._supported_vendors\n",
      "\u001b[0m\n",
      "    def update(cls):\n",
      "        for device in cls._updating_devices:\n",
      "            cls._update_disk(device)\n",
      "\n",
      "    def _update_disk(cls, dev):\n",
      "        with open('/sys/block/' + dev + '/stat') as statfile:\n",
      "            cls._load[dev] = list(map(int, statfile.read().split()))\n",
      "\n",
      "=====================43-PREDICTION=====================\n",
      "\n",
      "import tuned.monitors\n",
      "import os\n",
      "\n",
      "class DiskMonitor(tuned.monitors.Monitor):\n",
      "    _supported_vendors = ['ATA', 'SCSI']\n",
      "\n",
      "    \u001b[31m@classmethod\n",
      "    \u001b[0mdef _init_available_devices(cls):\n",
      "        block_devices = os.listdir('/sys/block')\n",
      "        available = set(filter(cls._is_device_supported, block_devices))\n",
      "        cls._available_devices = available\n",
      "        for d in available:\n",
      "            cls._load[d] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\u001b[31m\n",
      "    @classmethod\u001b[0m\n",
      "    def _is_device_supported(cls, device):\n",
      "        vendor_file = '/sys/block/%s/device/vendor' % device\n",
      "        try:\n",
      "            vendor = open(vendor_file).read().strip()\n",
      "        except IOError:\n",
      "            return False\n",
      "        return vendor in cls._supported_vendors\n",
      "\n",
      "    \u001b[31m@classmethod\n",
      "    \u001b[0mdef update(cls):\n",
      "        for device in cls._updating_devices:\n",
      "            cls._update_disk(device)\n",
      "\n",
      "    \u001b[31m@classmethod\n",
      "    \u001b[0mdef _update_disk(cls, dev):\n",
      "        with open('/sys/block/' + dev + '/stat') as statfile:\n",
      "            cls._load[dev] = list(map(int,\n",
      "\n",
      "=====================43-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport tuned.monitors\n",
      "import os\n",
      "\n",
      "class DiskMonitor(tuned.monitors.Monitor):\n",
      "    _supported_vendors = ['ATA', 'SCSI']\n",
      "\n",
      "    \u001b[31m@classmethod\n",
      "    \u001b[0mdef _init_available_devices(cls):\n",
      "        block_devices = os.listdir('/sys/block')\n",
      "        available = set(filter(cls._is_device_supported, block_devices))\n",
      "        cls._available_devices = available\n",
      "        for d in available:\n",
      "            cls._load[d] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "    \u001b[31m@classmethod\n",
      "    \u001b[0mdef _is_device_supported(cls, device):\n",
      "        vendor_file = \u001b[31m(\u001b[0m'/sys/block/%s/device/vendor' % device\u001b[31m)\u001b[0m\n",
      "        try:\n",
      "            vendor = open(vendor_file).read().strip()\n",
      "        except IOError:\n",
      "            return False\n",
      "        return \u001b[31m(vendor in cls._supported_vendors)\n",
      "\n",
      "    @classmethod\u001b[0m\n",
      "    def update(cls):\n",
      "        for device in cls._updating_devices:\n",
      "            cls._update_disk(device)\n",
      "\n",
      "    \u001b[31m@classmethod\n",
      "    \u001b[0mdef _update_disk(cls, dev):\n",
      "        with open\u001b[31m((\u001b[0m('/sys/block/' + dev\u001b[31m)\u001b[0m + '/stat')\u001b[31m)\u001b[0m as statfile:\n",
      "            cls._load[dev] = list(map(int, statfile.read().split()))\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================43-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================44-INPUT=====================\n",
      "\n",
      "import os\n",
      "from tempfile import mkdtemp\n",
      "from contextlib import contextmanager\n",
      "from shutil import rmtree\n",
      "import mock\n",
      "import pytest\n",
      "\n",
      "def murlopen():\n",
      "    with mock.patch('hashin.urlopen') as patch:\n",
      "        \u001b[31myield patch\n",
      "\u001b[0m\n",
      "def mock_get_parser():\n",
      "    with mock.patch('hashin.get_parser') as patch:\n",
      "        \u001b[31myield patch\n",
      "\u001b[0m\n",
      "def mock_sys():\n",
      "    with mock.patch('hashin.sys') as patch:\n",
      "        \u001b[31myield patch\n",
      "\u001b[0m\n",
      "def mock_run():\n",
      "    with mock.patch('hashin.run') as patch:\n",
      "        \u001b[31myield patch\n",
      "\n",
      "def tmpfile():\n",
      "\n",
      "    \u001b[0m@contextmanager\n",
      "    def inner(name='requirements.txt'):\n",
      "        dir_ = mkdtemp('hashintest')\n",
      "        try:\n",
      "            yield os.path.join(dir_, name)\n",
      "        finally:\n",
      "            rmtree(dir_)\n",
      "    return inner\n",
      "\n",
      "=====================44-PREDICTION=====================\n",
      "\n",
      "import os\n",
      "from tempfile import mkdtemp\n",
      "from contextlib import contextmanager\n",
      "from shutil import rmtree\n",
      "import mock\n",
      "import pytest\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef murlopen():\n",
      "    with mock.patch('hashin.urlopen') as patch:\n",
      "        yield patch\n",
      "\u001b[31m\n",
      "@pytest.fixture\u001b[0m\n",
      "def mock_get_parser():\n",
      "    with mock.patch('hashin.get_parser') as patch:\n",
      "        yield patch\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef mock_sys():\n",
      "    with mock.patch('hashin.sys') as patch:\n",
      "        yield patch\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef mock_run():\n",
      "    with mock.patch('hashin.run') as patch:\n",
      "        yield patch\n",
      "\u001b[31m\n",
      "@pytest.fixture\u001b[0m\n",
      "def tmpfile():\n",
      "\n",
      "    @contextmanager\n",
      "    def inner(name='requirements.txt'):\n",
      "        dir_ = mkdtemp('hashintest')\n",
      "        try:\n",
      "            yield os.path.join(dir_, name)\n",
      "        finally:\n",
      "            rmtree(dir_)\n",
      "    return\n",
      "\n",
      "=====================44-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport os\n",
      "from tempfile import mkdtemp\n",
      "from contextlib import contextmanager\n",
      "from shutil import rmtree\n",
      "import mock\n",
      "import pytest\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef murlopen():\n",
      "    with mock.patch('hashin.urlopen') as patch:\n",
      "        \u001b[31m(yield patch)\n",
      "\n",
      "@pytest.fixture\u001b[0m\n",
      "def mock_get_parser():\n",
      "    with mock.patch('hashin.get_parser') as patch:\n",
      "        \u001b[31m(yield patch)\n",
      "\n",
      "@pytest.fixture\u001b[0m\n",
      "def mock_sys():\n",
      "    with mock.patch('hashin.sys') as patch:\n",
      "        \u001b[31m(yield patch)\n",
      "\n",
      "@pytest.fixture\u001b[0m\n",
      "def mock_run():\n",
      "    with mock.patch('hashin.run') as patch:\n",
      "        \u001b[31m(yield patch)\n",
      "\n",
      "\u001b[0m@\u001b[31mpytest.fixture\n",
      "def tmpfile():\n",
      "\n",
      "    @\u001b[0mcontextmanager\n",
      "    def inner(name='requirements.txt'):\n",
      "        dir_ = mkdtemp('hashintest')\n",
      "        try:\n",
      "            \u001b[31m(\u001b[0myield os.path.join(dir_, name)\u001b[31m)\u001b[0m\n",
      "        finally:\n",
      "            rmtree(dir_)\n",
      "    return inner\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================44-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================55-INPUT=====================\n",
      "\n",
      "import json\n",
      "from Crypto.PublicKey import RSA\n",
      "from decimal import Decimal\n",
      "\n",
      "class Util:\n",
      "\n",
      "    def construct_key_from_data(rsa_data):\n",
      "        k = json.loads(rsa_data['whole'])\n",
      "        key = RSA.construct((long(k['n']), long(k['e']), long(k['d']), long(k['p']), long(k['q']), long(k['u'])))\n",
      "        return key\n",
      "\n",
      "    def construct_pubkey_from_data(rsa_data):\n",
      "        key = RSA.construct((long(rsa_data['n']), long(rsa_data['e'])))\n",
      "        return key\n",
      "\n",
      "    def create_future_transaction(btc, prevtx, outputs, amount_available, receiver_address, locktime):\n",
      "        inputs = []\n",
      "        for tx in prevtx:\n",
      "            inputs.append({'txid': tx['txid'], 'vout': tx['vout']})\n",
      "        cash_back = amount_available\n",
      "        for (oracle, fee) in outputs.iteritems():\n",
      "            cash_back -= Decimal(fee)\n",
      "        outputs[receiver_address] = cash_back\n",
      "        vout = {}\n",
      "        for (address, value) in outputs.iteritems():\n",
      "            vout[address] = float(value)\n",
      "        transaction = btc.create_raw_transaction(inputs, vout)\n",
      "        return transaction\n",
      "\n",
      "=====================55-PREDICTION=====================\n",
      "\n",
      "import json\n",
      "from Crypto.PublicKey import RSA\n",
      "from decimal import Decimal\n",
      "\n",
      "class Util:\n",
      "\u001b[31m\n",
      "    @staticmethod\u001b[0m\n",
      "    def construct_key_from_data(rsa_data):\n",
      "        k = json.loads(rsa_data['whole'])\n",
      "        key = RSA.construct((long(k['n']), long(k['e']), long(k['d']), long(k['p']), long(k['q']), long(k['u'])))\n",
      "        return key\n",
      "\n",
      "    \u001b[31m@staticmethod\n",
      "    \u001b[0mdef construct_pubkey_from_data(rsa_data):\n",
      "        key = RSA.construct((long(rsa_data['n']), long(rsa_data['e'])))\n",
      "        return key\n",
      "\n",
      "    \u001b[31m@staticmethod\n",
      "    \u001b[0mdef create_future_transaction(btc, prevtx, outputs, amount_available, receiver_address, locktime):\n",
      "        inputs = []\n",
      "        for tx in prevtx:\n",
      "            inputs.append({'txid': tx['txid'], 'vout': tx['vout']})\n",
      "        cash_back = amount_available\n",
      "        for (oracle, fee) in outputs.iteritems():\n",
      "            cash_back -= Decimal(fee)\n",
      "        outputs[receiver_address] = cash_back\n",
      "        vout = {}\n",
      "        for (address, value) in outputs.iteritems():\n",
      "            vout[address] = float(value\n",
      "\n",
      "=====================55-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport json\n",
      "from Crypto.PublicKey import RSA\n",
      "from decimal import Decimal\n",
      "\n",
      "class Util\u001b[31m()\u001b[0m:\n",
      "\u001b[31m\n",
      "    @staticmethod\u001b[0m\n",
      "    def construct_key_from_data(rsa_data):\n",
      "        k = json.loads(rsa_data['whole'])\n",
      "        key = RSA.construct((long(k['n']), long(k['e']), long(k['d']), long(k['p']), long(k['q']), long(k['u'])))\n",
      "        return key\n",
      "\n",
      "    \u001b[31m@staticmethod\n",
      "    \u001b[0mdef construct_pubkey_from_data(rsa_data):\n",
      "        key = RSA.construct((long(rsa_data['n']), long(rsa_data['e'])))\n",
      "        return key\n",
      "\n",
      "    \u001b[31m@staticmethod\n",
      "    \u001b[0mdef create_future_transaction(btc, prevtx, outputs, amount_available, receiver_address, locktime):\n",
      "        inputs = []\n",
      "        for tx in prevtx:\n",
      "            inputs.append({'txid': tx['txid'], 'vout': tx['vout']})\n",
      "        cash_back = amount_available\n",
      "        for (oracle, fee) in outputs.iteritems():\n",
      "            cash_back -= Decimal(fee)\n",
      "        outputs[receiver_address] = cash_back\n",
      "        vout = {}\n",
      "        for (address, value) in outputs.iteritems():\n",
      "            vout[address] = float(value)\n",
      "        transaction = btc.create_raw_transaction(inputs, vout)\n",
      "        return transaction\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================55-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================80-INPUT=====================\n",
      "\n",
      "from .core_types import Taker, combination\n",
      "from .cmd_stream import CmdStream\n",
      "\n",
      "class Path(Taker):\n",
      "\n",
      "    def __init__(self, *args):\n",
      "        super(Path, self).__init__()\n",
      "        self.cmd_stream = CmdStream()\n",
      "        self.take(*args)\n",
      "\n",
      "def path_at_path(child, parent):\n",
      "    parent.cmd_stream.take(child.cmd_stream)\n",
      "\n",
      "def path_at_cmd_stream(path, cmd_stream):\n",
      "    cmd_stream.take(path.cmd_stream)\n",
      "\n",
      "=====================80-PREDICTION=====================\n",
      "\n",
      "from .core_types import Taker, combination\n",
      "from .cmd_stream import CmdStream\n",
      "\n",
      "class Path(Taker):\n",
      "\n",
      "    def __init__(self, *args):\n",
      "        super(Path, self).__init__()\n",
      "        self.cmd_stream = CmdStream()\n",
      "        self.take(*args)\n",
      "\n",
      "\u001b[31m@combination(Path, Path)\n",
      "\u001b[0mdef path_at_path(child, parent):\n",
      "    parent.cmd_stream.take(child.cmd_stream)\n",
      "\n",
      "\u001b[31m@combination(Path, CmdStream)\n",
      "\u001b[0mdef path_at_cmd_stream(path, cmd_stream):\n",
      "    cmd_stream.take(path.cmd_stream)\u001b[31m\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "=====================80-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mfrom .core_types import Taker, combination\n",
      "from .cmd_stream import CmdStream\n",
      "\n",
      "class Path(Taker):\n",
      "\n",
      "    def __init__(self, *args):\n",
      "        super(Path, self).__init__()\n",
      "        self.cmd_stream = CmdStream()\n",
      "        self.take(*args)\n",
      "\n",
      "\u001b[31m@combination(Path, Path)\n",
      "\u001b[0mdef path_at_path(child, parent):\n",
      "    parent.cmd_stream.take(child.cmd_stream)\n",
      "\n",
      "\u001b[31m@combination(Path, CmdStream)\n",
      "\u001b[0mdef path_at_cmd_stream(path, cmd_stream):\n",
      "    cmd_stream.take(path.cmd_stream)\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================80-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================86-INPUT=====================\n",
      "\n",
      "from __future__ import absolute_import\n",
      "import os\n",
      "from launchpad import settings\n",
      "import tornado.web\n",
      "\n",
      "class MainHandler(tornado.web.RequestHandler):\n",
      "    \u001b[31m\"\"\"\u001b[0m Return files from app and node_modules otherwise return index.html\u001b[31m\"\"\"\u001b[0m\n",
      "\n",
      "    def initialize(self):\n",
      "        self.mimetypes = settings.CONF_OPTIONS['mimetypes']\n",
      "        self.default_extension = settings.CONF_OPTIONS['defaultextension']\n",
      "        self.dir_base = 'www/'\n",
      "\n",
      "    def get_file_path(path):\n",
      "        path_segment = path.split('?', 1)[0]\n",
      "        return os.path.join(settings.doc_dir, path_segment)\n",
      "\n",
      "    def get(self, path):\n",
      "        filetype = path.split('.')[-1]\n",
      "        if filetype not in self.mimetypes:\n",
      "            path = path + '.' + self.default_extension\n",
      "            filetype = self.default_extension\n",
      "        try:\n",
      "            with open(self.get_file_path(self.dir_base + path), 'rb') as request_page:\n",
      "                self.write(request_page.read())\n",
      "                self.set_status(200)\n",
      "                if filetype in list(self.mimetypes.keys()):\n",
      "                    self.set_header('Content-Type', self.mimetypes[filetype])\n",
      "        except:\n",
      "            with open(self.get_file_path(self.dir_base + 'index.html'), 'rb') as request_page:\n",
      "                self.write(request_page.read())\n",
      "                self.set_status(200)\n",
      "\n",
      "    def default(self, path):\n",
      "        self.set_status(405)\n",
      "        self.write({'error': {'title': 'Method Not Allowed'}})\n",
      "\n",
      "=====================86-PREDICTION=====================\n",
      "\n",
      "from __future__ import absolute_import\n",
      "import os\n",
      "from launchpad import settings\n",
      "import tornado.web\n",
      "\n",
      "class MainHandler(tornado.web.RequestHandler):\n",
      "    \"\"\" Return files from app and node_modules otherwise return index.html\"\"\"\n",
      "\n",
      "    def initialize(self):\n",
      "        self.mimetypes = settings.CONF_OPTIONS['mimetypes']\n",
      "        self.default_extension = settings.CONF_OPTIONS['defaultextension']\n",
      "        self.dir_base = 'www/'\n",
      "\n",
      "    \u001b[31m@staticmethod\n",
      "    \u001b[0mdef get_file_path(path):\n",
      "        path_segment = path.split('?', 1)[0]\n",
      "        return os.path.join(settings.doc_dir, path_segment)\n",
      "\n",
      "    def get(self, path):\n",
      "        filetype = path.split('.')[-1]\n",
      "        if filetype not in self.mimetypes:\n",
      "            path = path + '.' + self.default_extension\n",
      "            filetype = self.default_extension\n",
      "        try:\n",
      "            with open(self.get_file_path(self.dir_base + path), 'rb') as request_page:\n",
      "                self.write(request_page.read())\n",
      "                self.set_status(200)\n",
      "                if filetype in list(self.mimetypes.keys()):\n",
      "                    self.set_header('Content-Type', self.mimetypes[filetype])\n",
      "        except:\n",
      "            with open(self.get_file_path(self.dir_base + 'index.html'), 'rb') as request_page:\n",
      "                self.write(request_page.read())\n",
      "                self.set_status(200)\n",
      "\n",
      "    def default(self, path):\n",
      "        self.set_status(405)\n",
      "        self.write({'error': {'title': 'Method Not Allowed'}}\n",
      "\n",
      "=====================86-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mfrom __future__ import absolute_import\n",
      "import os\n",
      "from launchpad import settings\n",
      "import tornado.web\n",
      "\n",
      "class MainHandler(tornado.web.RequestHandler):\n",
      "    \u001b[31m'\u001b[0m Return files from app and node_modules otherwise return index.html\u001b[31m'\u001b[0m\n",
      "\n",
      "    def initialize(self):\n",
      "        self.mimetypes = settings.CONF_OPTIONS['mimetypes']\n",
      "        self.default_extension = settings.CONF_OPTIONS['defaultextension']\n",
      "        self.dir_base = 'www/'\n",
      "\n",
      "    \u001b[31m@staticmethod\n",
      "    \u001b[0mdef get_file_path(path):\n",
      "        path_segment = path.split('?', 1)[0]\n",
      "        return os.path.join(settings.doc_dir, path_segment)\n",
      "\n",
      "    def get(self, path):\n",
      "        filetype = path.split('.')[\u001b[31m(\u001b[0m-\u001b[31m \u001b[0m1\u001b[31m)\u001b[0m]\n",
      "        if \u001b[31m(\u001b[0mfiletype not in self.mimetypes\u001b[31m)\u001b[0m:\n",
      "            path = \u001b[31m((\u001b[0mpath + '.'\u001b[31m)\u001b[0m + self.default_extension\u001b[31m)\u001b[0m\n",
      "            filetype = self.default_extension\n",
      "        try:\n",
      "            with open(self.get_file_path(\u001b[31m(\u001b[0mself.dir_base + path\u001b[31m)\u001b[0m), 'rb') as request_page:\n",
      "                self.write(request_page.read())\n",
      "                self.set_status(200)\n",
      "                if \u001b[31m(\u001b[0mfiletype in list(self.mimetypes.keys(\u001b[31m)\u001b[0m)):\n",
      "                    self.set_header('Content-Type', self.mimetypes[filetype])\n",
      "        except:\n",
      "            with open(self.get_file_path(\u001b[31m(\u001b[0mself.dir_base + 'index.html'\u001b[31m)\u001b[0m), 'rb') as request_page:\n",
      "                self.write(request_page.read())\n",
      "                self.set_status(200)\n",
      "\n",
      "    def default(self, path):\n",
      "        self.set_status(405)\n",
      "        self.write({'error': {'title': 'Method Not Allowed'}})\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================86-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================89-INPUT=====================\n",
      "\n",
      "import logging\n",
      "import requests\n",
      "from tenacity import before_log, retry, stop_after_attempt\n",
      "\n",
      "class MarketDataClient(object):\n",
      "    logger = logging.getLogger(__name__)\n",
      "    base_url = 'http://market-data:8000'\n",
      "\n",
      "    def _make_request(self, url):\n",
      "        response = requests.get(f'{self.base_url}/{url}', headers={'content-type': 'application/json'})\n",
      "        return response.json()\n",
      "\n",
      "    def all_prices(self):\n",
      "        return self._make_request('prices')\n",
      "\n",
      "    def price(self, code):\n",
      "        return self._make_request(f'prices/{code}')\n",
      "\n",
      "=====================89-PREDICTION=====================\n",
      "\n",
      "import logging\n",
      "import requests\n",
      "from tenacity import before_log, retry, stop_after_attempt\n",
      "\n",
      "class MarketDataClient(object):\n",
      "    logger = logging.getLogger(__name__)\n",
      "    base_url = 'http://market-data:8000'\n",
      "\n",
      "    \u001b[31m@retry(stop=stop_after_attempt(3), before=before_log(logger, logging.DEBUG))\n",
      "    \u001b[0mdef _make_request(self, url):\n",
      "        response = requests.get(f'{self.base_url}/{url}', headers={'content-type': 'application/json'})\n",
      "        return response.json()\n",
      "\n",
      "    def all_prices(self):\n",
      "        return self._make_request('prices')\n",
      "\n",
      "    def price(self, code):\n",
      "\n",
      "=====================89-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mimport logging\n",
      "import requests\n",
      "from tenacity import before_log, retry, stop_after_attempt\n",
      "\n",
      "class MarketDataClient(object):\n",
      "    logger = logging.getLogger(__name__)\n",
      "    base_url = 'http://market-data:8000'\n",
      "\n",
      "    def _make_request(self, url):\n",
      "        response = requests.get(f'{self.base_url}/{url}', headers={'content-type': 'application/json'})\n",
      "        return response.json()\n",
      "\n",
      "    \u001b[31m@retry(stop=stop_after_attempt(3), before=before_log(logger, logging.DEBUG))\n",
      "    \u001b[0mdef all_prices(self):\n",
      "        return self._make_request('prices')\n",
      "\n",
      "    def price(self, code):\n",
      "        return self._make_request(f'prices/{code}')\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================89-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================146-INPUT=====================\n",
      "\n",
      "\"\u001b[31m\"\"\n",
      "\u001b[0mpytest configuration for GridSync's GUI tests.\u001b[31m\n",
      "\"\"\u001b[0m\"\n",
      "from unittest.mock import MagicMock\n",
      "import pytest\n",
      "from qtpy.QtWidgets import QApplication\n",
      "from twisted.python.filepath import FilePath\n",
      "from gridsync.gui import Gui\n",
      "from gridsync.preferences import Preferences\n",
      "app = QApplication([])\n",
      "\n",
      "def preferences_config_file(tmpdir_factory) -> FilePath:\n",
      "    \u001b[31m\"\"\"\n",
      "\u001b[0m    The value of this fixture is a path suitable for use as a preferences\u001b[31m\n",
      "\u001b[0m    configuration file.  It does not exist initially.\u001b[31m\n",
      "    \"\"\"\u001b[0m\n",
      "    return FilePath(str(tmpdir_factory.mktemp('config'))).child('preferences.ini')\n",
      "\n",
      "def preferences(preferences_config_file: str) -> Preferences:\n",
      "    \u001b[31m\"\"\"\n",
      "\u001b[0m    The value of this fixture is a preferences object which tests can set\u001b[31m\n",
      "    preferences in and get preferences from.\n",
      "    \"\"\"\u001b[0m\n",
      "    return Preferences(preferences_config_file)\n",
      "\n",
      "def gui(preferences: Preferences) -> Gui:\n",
      "    \u001b[31m\"\"\"\n",
      "\u001b[0m    The value of this fixture is a GUI object which tests can add widgets to\u001b[31m\n",
      "\u001b[0m    and do other GUI-y things to.\u001b[31m\n",
      "    \"\"\"\u001b[0m\n",
      "    return Gui(MagicMock(), preferences=preferences)\n",
      "\n",
      "=====================146-PREDICTION=====================\n",
      "\n",
      "\"\"\"\n",
      "pytest configuration for GridSync's GUI tests.\n",
      "\"\"\"\n",
      "from unittest.mock import MagicMock\n",
      "import pytest\n",
      "from qtpy.QtWidgets import QApplication\n",
      "from twisted.python.filepath import FilePath\n",
      "from gridsync.gui import Gui\n",
      "from gridsync.preferences import Preferences\n",
      "app = QApplication([])\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef preferences_config_file(tmpdir_factory) -> FilePath:\n",
      "    \"\"\"\n",
      "    The value of this fixture is a path suitable for use as a preferences\n",
      "    configuration file.  It does not exist initially.\n",
      "    \"\"\"\n",
      "    return FilePath(str(tmpdir_factory.mktemp('config'))).child('preferences.ini')\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef preferences(preferences_config_file: str) -> Preferences:\n",
      "    \"\"\"\n",
      "    The value of this fixture is a preferences object which tests can set\n",
      "    preferences in and get preferences from.\n",
      "    \"\"\"\n",
      "    return Preferences(preferences_config_file)\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef gui(preferences:\n",
      "\n",
      "=====================146-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0m\"\u001b[31m\\n\u001b[0mpytest configuration for GridSync's GUI tests.\u001b[31m\\n\u001b[0m\"\n",
      "from unittest.mock import MagicMock\n",
      "import pytest\n",
      "from qtpy.QtWidgets import QApplication\n",
      "from twisted.python.filepath import FilePath\n",
      "from gridsync.gui import Gui\n",
      "from gridsync.preferences import Preferences\n",
      "app = QApplication([])\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef preferences_config_file(tmpdir_factory) -> FilePath:\n",
      "    \u001b[31m'\\n\u001b[0m    The value of this fixture is a path suitable for use as a preferences\u001b[31m\\n\u001b[0m    configuration file.  It does not exist initially.\u001b[31m\\n    '\u001b[0m\n",
      "    return FilePath(str(tmpdir_factory.mktemp('config'))).child('preferences.ini')\n",
      "\n",
      "\u001b[31m@pytest.fixture\n",
      "\u001b[0mdef preferences(preferences_config_file: str) -> Preferences:\n",
      "    \u001b[31m'\\n\u001b[0m    The value of this fixture is a preferences object which tests can set\u001b[31m\\n    preferences in and get preferences from.\\n    '\u001b[0m\n",
      "    return Preferences(preferences_config_file)\n",
      "\u001b[31m\n",
      "@pytest.fixture\u001b[0m\n",
      "def gui(preferences: Preferences) -> Gui:\n",
      "    \u001b[31m'\\n\u001b[0m    The value of this fixture is a GUI object which tests can add widgets to\u001b[31m\\n\u001b[0m    and do other GUI-y things to.\u001b[31m\\n    '\u001b[0m\n",
      "    return Gui(MagicMock(), preferences=preferences)\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================146-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n",
      "\n",
      "=====================150-INPUT=====================\n",
      "\n",
      "from .base_renderer import *\n",
      "import os.path\n",
      "__file__ = os.path.normpath(os.path.abspath(__file__))\n",
      "__path__ = os.path.dirname(__file__)\n",
      "\n",
      "@renderer\n",
      "class OrgRenderer(CommandlineRenderer):\n",
      "\n",
      "    def __init__(self):\n",
      "        super(OrgRenderer, self).__init__(executable='ruby', args=['-rubygems', os.path.join(__path__, 'bin/org.rb')])\n",
      "\n",
      "    \u001b[31mdef is_enabled(\u001b[0mcls, filename, syntax):\n",
      "        return filename.endswith('.org')\n",
      "\n",
      "=====================150-PREDICTION=====================\n",
      "\n",
      "from .base_renderer import *\n",
      "import os.path\n",
      "__file__ = os.path.normpath(os.path.abspath(__file__))\n",
      "__path__ = os.path.dirname(__file__)\n",
      "\n",
      "@renderer\n",
      "class OrgRenderer(CommandlineRenderer):\n",
      "\n",
      "    def __init__(self):\n",
      "        super(OrgRenderer, self).__init__(executable='ruby', args=['-rubygems', os.path.join(__path__, 'bin/org.rb')])\n",
      "\n",
      "    \u001b[31m@\u001b[0mcl\u001b[31massmethod\n",
      "    def is_enabled(cls,\u001b[0m\n",
      "\n",
      "=====================150-GOLD LABELS=====================\n",
      "\n",
      "\u001b[31m\n",
      "\u001b[0mfrom .base_renderer import *\n",
      "import os.path\n",
      "__file__ = os.path.normpath(os.path.abspath(__file__))\n",
      "__path__ = os.path.dirname(__file__)\n",
      "\n",
      "@renderer\n",
      "class OrgRenderer(CommandlineRenderer):\n",
      "\n",
      "    def __init__(self):\n",
      "        super(OrgRenderer, self).__init__(executable='ruby', args=['-rubygems', os.path.join(__path__, 'bin/org.rb')])\n",
      "\n",
      "    \u001b[31m@\u001b[0mcl\u001b[31massmethod\n",
      "    def is_enabled(cl\u001b[0ms, filename, syntax):\n",
      "        return filename.endswith('.org')\u001b[31m\n",
      "\u001b[0m\n",
      "\n",
      "=====================150-DIFF_BLEU=====================\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "lookup_examples(decorator_codex_report, 1, 0.5, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56995c94-b60c-4a62-ae5c-89e29ff4b837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@mock.patch socket.gethostbyname_ex ip',\n",
       " 'None : None : @mock.patch socket.gethostbyname_ex ')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decorator_codex_report[\"pred_diffs\"][0], decorator_codex_report[\"gold_diffs\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
